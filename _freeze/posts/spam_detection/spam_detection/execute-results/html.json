{
  "hash": "31c515974f4b1bfd2986c4c9ec5841b3",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Spam Detection\"\nimage: \"spam_detection.jpg\"\ndescription: \"NLP\"\nauthor: \"Simone Brazzi\"\ndate: \"2024-09-05\"\ndraft: false\ncategories:\n  - code\n  - NLP\n  - Natural Language, Processing\n  - Python, R\ntoc: true\ntoc-title: \"Table of Contents\"\nother-links:\n  - icon: file-pdf\n    text: LDA Paper\n    href: https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf\ntoc-depth: 3\nnumber-sections: true\nnumber-depth: 3\nembed-resources: true\nanchor-sections: true\nsmooth-scroll: true\nhighlight-style: monokai\ncode-line-numbers: true\ncode-copy: true\ncode-link: true\n\nexecute:\n  eval: false\n  warnings: false\n  freeze: true\n  cache: true\n---\n\n\n\n# Introduction\n\nTasks:\n\n-   Train a model to classify spam.\n-   Find the main topic in the spam emails.\n-   Calculate semantic distance of the spam topics.\n-   Extract the ORG from ham emilas.\n\n# Import\n\n## R libraries\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse, verbose = FALSE)\nlibrary(ggplot2)\nlibrary(gt)\nlibrary(reticulate)\nlibrary(plotly)\n\n# renv::use_python(\"/Users/simonebrazzi/venv/blog/bin/python3\")\n```\n:::\n\n\n\n## Python packages\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport spacy\nimport nltk\nimport string\nimport gensim\nimport gensim.corpora as corpora\nimport gensim.downloader\n\nfrom nltk.corpus import stopwords\nfrom gensim.models import Word2Vec\nfrom scipy.spatial.distance import cosine\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\nfrom collections import Counter\n# glove vector\nglove_vector = gensim.downloader.load(\"glove-wiki-gigaword-300\")\n# nltk stopwords\nnltk.download('stopwords')\nstop_words = stopwords.words('english')\n```\n:::\n\n\n\n## Config class\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nclass Config():\n  def __init__(self):\n    \n    \"\"\"\n    CLass initialize function.\n    \"\"\"\n    \n    self.path=\"/Users/simonebrazzi/R/blog/posts/spam_detection/spam_dataset.csv\"\n    self.random_state=42\n  \n  def get_lemmas(self, doc):\n    \n    \"\"\"\n    List comprehension to get lemmas. It performs:\\n\n      1. Lowercase.\\n\n      2. Stop words removal\\n.\n      3. Whitespaces removal.\\n\n      4. Remove the word 'subject'.\\n\n      5. Digits removal.\\n\n      6. Get only words with length >= 5.\\n\n    \"\"\"\n    \n    lemmas = [\n      [\n        t.lemma_ for t in d \n        if (text := t.text.lower()) not in punctuation \n        and text not in stop_words \n        and not t.is_space \n        and text != \"subject\" \n        and not t.is_digit\n        and len(text) >=5\n        ]\n        for d in doc\n        ]\n    return lemmas\n  \n  def get_entities(self, doc):\n    \n    \"\"\"\n    List comprehension to get the lemmas which have entity type 'ORG'.\n    \"\"\"\n    \n    entities = [\n    [\n      t.lemma_ for t in d \n      if (text := t.text.lower()) not in punctuation \n      and text not in stop_words \n      and not t.is_space \n      and text != \"subject\" \n      and not t.is_digit\n      and len(text) >=5\n      and t.ent_type_ == \"ORG\"\n      ]\n      for d in doc\n      ]\n    return entities\n  \n  def get_sklearn_mlp(self, activation, solver, max_iter, hidden_layer_sizes, tol):\n    \n    \"\"\"\n    It initialize the sklearn MLPClassifier.\n    \"\"\"\n    \n    mlp = mlp = MLPClassifier(\n      activation=activation,\n      solver=solver,\n      max_iter=max_iter,\n      hidden_layer_sizes=hidden_layer_sizes,\n      tol=tol,\n      verbose=True\n      )\n    return mlp\n  \n  def get_lda(self, corpus, id2word, num_topics, passes, workers, random_state):\n    \n    \"\"\"\n    Initialize LDA.\n    \"\"\"\n    \n    lda = gensim.models.LdaMulticore(\n      corpus=corpus,\n      id2word=id2word,\n      num_topics=num_topics,\n      passes=passes,\n      workers=workers,\n      random_state=self.random_state\n      )\n    return lda\n  \nconfig = Config()\n```\n:::\n\n\n\n# Dataset\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndf = pd.read_csv(config.path, index_col=0)\n```\n:::\n\n\n\nTo further perform analysis also in R, here we assign the df to a variable using `reticulate` library.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(reticulate)\ndf <- py$df\n```\n:::\n\n\n\n## EDA\n\nFirst things first, Exploratory Data Analysis. Considering we are going to perform a classification, is interesting to check if our dataset is unbalanced.\n\n\n\n::: {#tbl-eda .cell tbl-cap='Absolute and relative frequencies'}\n\n```{.r .cell-code}\ndf_g <- df %>% \n  summarise(\n    freq_abs = n(),\n    freq_rel = n() / nrow(df),\n    .by = label\n    )\n\ndf_g %>% \n  gt() %>% \n  fmt_auto() %>% \n  cols_width(\n    label ~ pct(20),\n    freq_abs ~ pct(35),\n    freq_rel ~ pct(35)\n    ) %>% \n  cols_align(\n    align = \"center\",\n    columns = c(freq_abs, freq_rel)\n  ) %>% \n  tab_header(\n    title = \"Label frequency\",\n    subtitle = \"Absolute and relative frequencies\"\n  ) %>% \n  cols_label(\n    label = \"Label\",\n    freq_abs = \"Absolute frequency\",\n    freq_rel = \"Relative frequency\"\n  ) %>% \n  tab_options(\n    table.width = pct(100)\n    )\n```\n:::\n\n\n\nIt is useful also a visual cue.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ng <- ggplot(df_g) +\n  geom_col(aes(x = freq_abs, y = label, fill = label)) +\n  scale_fill_brewer(palette = \"Set1\", direction = -1) +\n  theme_minimal() +\n  ggtitle(\"Absolute frequency barchart\") +\n  xlab(\"Absolute frequency\") +\n  ylab(\"Label\") +\n  labs(fill = \"Label\")\nggplotly(g)\n```\n:::\n\n\n\nThe dataset is not balanced, so it could be relevant when training the model for classification. Depending on the model performance, we know what we could investigate first.\n\n# Preprocessing\n\nIn case of text, the preprocessing is fundamental: the computer does not understand the semantic or grammatical meaning of words. In case of text preprocessing, we follow the following steps:\n\n1.  Lowercasing.\n2.  Punctuation removal.\n3.  Lemmatization.\n4.  Tokenization.\n5.  Stopwords removal.\n\nUsing `SpaCy`, we can applied these steps. The method `nlp.pipe` improves the performance and returns a generator. It yields a `Doc` objects, not a list. To use it as a list, it has to be defined as such. To speed up the process, is it possible to enable the multi process method in `nlp.pipe`. But, what does the variable `nlp` stand for? It load a spaCy model: we are going to use the `en_core_web_lg`.\n\n-   **Language**: EN; english.\n-   **Type**: CORE; vocabulary, syntax, entities, vectors\n-   **Genre**: WEB; written text (blogs, news, comments).\n-   **Size**: LG; large (560 mB).\n\nCheck this [link](https://spacy.io/models/en) for the documentation about this model.\n\nI had chosen this model, even if it is the biggest, to get the full potential of it.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# load en_core_web_lg\nnlp = spacy.load(\"en_core_web_lg\")\ndoc = list(nlp.pipe(df.text, n_process=4))\n```\n:::\n\n\n\nThe specific preprocessing in this case should check these steps:\n\n1.  Remove punctuation.\n2.  Remove stop words.\n3.  Remove spaces.\n4.  Remove *\"subject\"* token.\n5.  Lemmatization.\n\nTo improve code performance, these are the most noticible points:\n\n-   When iterating over a collection of unique elements, **`set()`** performs better then `list()`. The underlying hash table structure allows for swift traversal. This is particularly noticible when the df dimension increase.\n-   **List comprehension**, which performs better then for loops and are much more readable in some context.\n-   The **walrus operator** `:=`. It is a syntax which lets assign variables in the middle of expressions. It avoids redundant calculations and improves readability.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\npunctuation = set(string.punctuation)\nstop_words = set(stop_words)\n\nlemmas = config.get_lemmas(doc)\nentities = config.get_entities(doc)\n\n# assignment to a df column\ndf[\"lemmas\"] = lemmas\ndf[\"entities\"] = entities\n```\n:::\n\n\n\n# Tasks\n\n## Classification\n\nThe text is already preprocessed as list of lemmas. For the classification task, it is necessary to convert it as a string.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndf[\"feature\"] = df.lemmas.apply(lambda x : \" \".join(x))\n```\n:::\n\n\n\n### Features\n\nAs said, the machine does not understand human readable text. It has to be transformed. The best approach is to **vectorize** it with `TfidfVectorizer()`. It is a tool for converting text into a matrix of TF-IDF features. The **T**erm**F**requency-**I**nverse**D**ocument**F**requency is a statistical method. It is a measure of importance of a word in a document, part of a corpus, adjusted for the frequency in the corpus. The model vectorize a word by multiplying the word **T**erm **F**requency\n\n$$\nTF = \\frac{word\\ frequency\\ in\\ document}{total\\ words\\ in\\ document}\n$$ with the **I**nverse **D**ocument **F**requency\n\n$$\nIDF = log(\\frac{total\\ number\\ documents}{documents\\ containing\\ the\\ word})\n$$ The final result is\n\n$$\nTF-IDF = TF * IDF\n$$\n\nThe resulting **score** represents the importance of a word. It dependes on the word frequency both in a specific document and in the corpus.\n\n::: {.callout-note}\nAn example can be useful. If a word t appears 20 times in a document of 100 words, we have\n\n$$\nTF = \\frac{20}{100}=0.2\n$$\n\nIf there are 10.000 documents in the corpus and 100 documents contains the term t\n\n$$\nIDF = log(\\frac{10000}{100})=2\n$$\n\nThis means the score is\n\n$$\nTF-IDF=0.2*2=0.4\n$$\n:::\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nvectorizer = TfidfVectorizer()\nX = vectorizer.fit_transform(df['feature'])\ny = df.label\n```\n:::\n\n\n\n## Split\n\nNot much to say about this: a best practice which let evaluate the performance of our model on new data.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nxtrain, xtest, ytrain, ytest = train_test_split(X, y, test_size=.2, random_state=42)\n```\n:::\n\n\n\n### Model\n\nThe model is the `MLPClassifier()`. It is a Multi Perceptron Layer Classifier.\n\n![MLPClassifier](mlpc.png)\n\nIt is an Artificial Neural Network used for classification. It consists of **multiple layers of nodes**, called **perceptrons**. For further reading, see the [documentation](https://scikit-learn.org/stable/modules/neural_networks_supervised.html).\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nmlp = config.get_sklearn_mlp(\n  activation=\"logistic\",\n  solver=\"adam\",\n  max_iter=100,\n  hidden_layer_sizes=(100,),\n  tol=.005\n  )\n```\n:::\n\n\n\n### Fit\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nmlp.fit(xtrain, ytrain)\n```\n:::\n\n\n\n### Predict\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nypred = mlp.predict(xtest)\n```\n:::\n\n\n\n### Classification report\n\nConsidering we are doing a classificatoin, one method to evaluate the performance is the classification report. It summarize the performance of the model comparing **true** and **predicted** labels, showing not only the metrics (precision, recall and F1-score) but also the support.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ncr = classification_report(\n  ytest,\n  ypred,\n  target_names=[\"spam\", \"ham\"],\n  digits=4,\n  output_dict=True\n  )\ndf_cr = pd.DataFrame.from_dict(cr).reset_index()\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(reticulate)\n\ndf_cr <- py$df_cr %>% dplyr::rename(names = index)\ncols <- df_cr %>% colnames()\ndf_cr %>% \n  pivot_longer(\n    cols = -names,\n    names_to = \"metrics\",\n    values_to = \"values\"\n  ) %>% \n  pivot_wider(\n    names_from = names,\n    values_from = values\n  ) %>% \n  gt() %>%\n  tab_header(\n    title = \"Confusion Matrix\",\n    subtitle = \"Sklearn MLPClassifier\"\n  ) %>% \n  fmt_number(\n    columns = c(\"precision\", \"recall\", \"f1-score\", \"support\"),\n    decimals = 2,\n    drop_trailing_zeros = TRUE,\n    drop_trailing_dec_mark = FALSE\n  ) %>% \n  cols_align(\n    align = \"center\",\n    columns = c(\"precision\", \"recall\", \"f1-score\", \"support\")\n  ) %>% \n  cols_align(\n    align = \"left\",\n    columns = metrics\n  ) %>% \n  cols_label(\n    metrics = \"Metrics\",\n    precision = \"Precision\",\n    recall = \"Recall\",\n    `f1-score` = \"F1-Score\",\n    support = \"Support\"\n  )\n```\n:::\n\n\n\nEven if the model is not fitted for an unbalanced dataset, it is not affecting the performance. Precision and Recall are high, so much that is could seems to be overfitted.\n\n## Topic Modeling for spam content\n\nTopic modeling in nlp can count on the **L**atent **D**irilicht **M**odel. It is a generative model used to get the topics which occur in a set of documents.\n\nThe LDA model has:\n\n-   Input: a **corpus** of text documents, preprocessed as tokenized and cleaned words. We have this in the *lemmas* column.\n-   Output: a **distribution** of topics for each document and one of words for each topic.\n\nFor further reading, you can find the paper in the Table of Contents or at this [link](https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf).\n\n### Dataset\n\nFilter data to have a spam dataframe and create a variable with the lemmas column to work with.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nspam = df[df.label == \"spam\"]\nx = spam.lemmas\n```\n:::\n\n\n\n### Create corpus\n\nLDA algortihm needs the corpus as a **bag of word**.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nid2word = corpora.Dictionary(x)\ncorpus = [id2word.doc2bow(t) for t in x]\n```\n:::\n\n\n\n### Model\n\nThe model will return a user defined number of topics. For each of it, it will return a user defined number of words ad the probability of each of them.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nlda = config.get_lda(\n  corpus=corpus,\n  id2word=id2word,\n  num_topics=10,\n  passes=10, # number of times the algorithm see the corpus\n  workers=4, # parellalize\n  random_state=42\n  )\ntopic_words = lda.show_topics(num_topics=10, num_words=5, formatted=False)\n\n# Iterate over topic_words to extract the data\ndata = []\ndata = [\n    (topic, w, p)\n    for topic, words in topic_words\n    for w, p in words\n    ]\ntopics_df = pd.DataFrame(data, columns=['topic', 'word', 'proba'])\n```\n:::\n\n::: {#tbl-topics .cell tbl-cap='Topics id, words and probabilities'}\n\n```{.r .cell-code}\npy$topics_df %>% \n  gt() %>% \n  tab_header(\n    title = \"Words and probabilities by topics\"\n  ) %>% \n  fmt_auto() %>% \n  cols_width(\n    topic ~ pct(33),\n    word ~ pct(33),\n    proba ~ pct(33)\n    ) %>% \n  cols_align(\n    align = \"center\",\n    columns = c(topic, word, proba)\n  ) %>% \n  cols_label(\n    topic = \"Topic\",\n    word = \"Word\",\n    proba = \"Probability\"\n  ) %>% \n  tab_options(\n    table.width = pct(100)\n  )\n```\n:::\n\n\n\n## Semantic distance between topics\n\nThe semantic distance requires a documents made of strings. Using a `dict`, we can extract the topics and the words. The dict has the topics as keys and the words as a list of words. The documents can be created using the `.join()`.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ntopics_dict = {}\nfor topics, words in topic_words:\n  topics_dict[topics] = [w[0] for w in words]\n\ndocuments = [\" \".join(words) for words in topics_dict.values()]\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nvectorizer = TfidfVectorizer()\ntfidf_matrix = vectorizer.fit_transform(documents)\ncosine_sim_matrix = cosine_similarity(tfidf_matrix, dense_output=True)\ntopics = list(topics_dict.keys())\ncosine_sim_df = pd.DataFrame(cosine_sim_matrix, index=topics, columns=topics)\n```\n:::\n\n::: {#tbl-cos-sim .cell tbl-cap='Cosine similarity matrix'}\n\n```{.r .cell-code}\npy$cosine_sim_df %>% \n  gt() %>% \n  fmt_auto() %>% \n  cols_align(\n    align = \"center\",\n    columns = c(`0`, `1`, `2`, `3`, `4`, `5`, `6`, `7`, `8`, `9`)\n  ) %>% \n  cols_label(\n    `0` = \"Topic 0\",\n    `1` = \"Topic 1\",\n    `2` = \"Topic 2\",\n    `3` = \"Topic 3\",\n    `4` = \"Topic 4\",\n    `5` = \"Topic 5\",\n    `6` = \"Topic 6\",\n    `7` = \"Topic 7\",\n    `8` = \"Topic 8\",\n    `9` = \"Topic 9\",\n  ) %>% \n  tab_options(\n    table.width = pct(100)\n    )\n```\n:::\n\n\n\n## Organization of \"HAM\" mails\n\n### Create \"HAM\" df\n\n### Get ham lemmas which have ORG entity\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nham = df[df.label == \"ham\"]\nx = ham.entities\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom collections import Counter\n\n# Flatten the list of lists and create a Counter object\nd = Counter([i for e in x for i in e])\n\nfreq_df = pd.DataFrame(d.items(), columns=[\"word\", \"freq\"])\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"wordcloud\")\nlibrary(\"RColorBrewer\")\n\nset.seed(42)\nword_freqs_df <- py$freq_df\n\nwordcloud(\n  words = word_freqs_df$word,\n  freq = word_freqs_df$freq,\n  min.freq = 1,\n  max.words = 100, # nrow(word_freqs_df),\n  random.order = FALSE,\n  rot.per = .3,\n  colors = brewer.pal(n = 8, name = \"Dark2\")\n  )\n```\n:::\n\n::: {#tbl-word-freq .cell tbl-cap='Top 10 words by frequency'}\n\n```{.r .cell-code}\nword_freqs_df %>% \n  arrange(desc(freq)) %>% \n  head(10) %>% \n  gt() %>%                                        \n  tab_header(\n    title = \"Top 10 ham words \",\n    subtitle = \"by frequency\"\n  ) %>% \n  fmt_auto() %>% \n  cols_width(\n    word ~ pct(50),\n    freq ~ pct(50)\n    ) %>% \n  cols_align(\n    align = \"center\",\n    columns = c(word, freq)\n  ) %>% \n  cols_label(\n    word = \"Word\",\n    freq = \"Frequency\"\n  ) %>% \n  tab_options(\n    table.width = pct(100)\n  )\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}