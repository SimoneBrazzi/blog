[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Nice to meet you! My name is Simone Brazzi. I am 34 years old guy from Bologna, Italy.\nI decided to open this blog as a personal journal (what a new idea! Who ever thought about this?) of my data journey. This also means it is an opportunity to display my portfolio and shares my projects.\nAt the date of writing, I am a self made data professional. I am currently working as a Data Analyst/Engineer for UniCredit Group.\nI have too many hobbies, but I will try to summarize them in a sparse order:\n\nTraining: my 2024 goal is to be able to run at least an half marathon.\nStudy: I am studying data science to further improve my knowledge in the data field. I would like to do a Master Degree in the field, but it is difficult to combine the private life with an international MSc, at least for now.\nGaming, movies and manga: of course! I mean, it is part of being a nerd, right?! For cliche purpose, I love Quentin Tarantino blabbering, Nolan craziness, even though my favorite genre is horror. My favorites directors are Jordan Pelee and Ari Aster. I am currently reading Berserker, My Hero Academia, Jujutsu Kaisen, One Piece, Chainsaw Man and Kagurabachi.\nCooking, I think it is in my vein being italian (joking!) and considering my mother is a chef.\n\nFor now, I think I have annoyed you enough: stay tuned for future posts!"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Toxic Comment Filter\n\n\n\n\n\n\ncode\n\n\nDeep Learning\n\n\nPython, R\n\n\n\nBiLSTM model for multi label classification\n\n\n\n\n\nAug 12, 2024\n\n\nSimone Brazzi\n\n\n\n\n\n\n\n\n\n\n\n\nEurostat Homicide Data\n\n\n\n\n\n\ncode\n\n\nshiny\n\n\nR\n\n\n\nA primer on Shiny to analyze gender differences in homicide rates\n\n\n\n\n\nDec 29, 2023\n\n\nSimone Brazzi\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/eurostat_homicide_rate/eurostat_homicide_rate.html",
    "href": "posts/eurostat_homicide_rate/eurostat_homicide_rate.html",
    "title": "Eurostat Homicide Data",
    "section": "",
    "text": "Hi there and welcome to my first project for the blog! The topic is a sad one, but I would like to explain why I decided to start with this. I was trying Shiny for different dashboards, but I wasn’t satisfied to learn using the classic examples. Unfortunately, italian crime news suffocated the public debate with a case of homicide, in which the victim is a young women. The public debate was focusing so baaaaaadly on the concept of femicide and the data, that I decided to clear the situation with a simple dashboard.\nFirst of all, at this link you can find the dashboard published using shinyapps.io. Also at this link you can find the Github repo. As you can see, even if it is the main branch, there are some details which are not uber perfect, but that don’t interfere with the code.\nNow lets jump into the detail of how to create a Shiny dashboard!"
  },
  {
    "objectID": "posts/eurostat_homicide_rate/eurostat_homicide_rate.html#global.r",
    "href": "posts/eurostat_homicide_rate/eurostat_homicide_rate.html#global.r",
    "title": "Eurostat Homicide Data",
    "section": "global.R",
    "text": "global.R\nAs said, this file is our usual R Script file. First thing first, we import our libraries:\n\n\nCode\n# wrangling\nlibrary(\"tidyverse\")\nlibrary(\"readr\")\nlibrary(\"stringr\")\nlibrary(\"dplyr\")\nlibrary(\"magrittr\")\nlibrary(\"forcats\")\nlibrary(\"lubridate\")\nlibrary(\"writexl\")\nlibrary(\"eurostat\")\n# plotting and dashboarding\nlibrary(\"shiny\")\nlibrary(\"shinythemes\")\nlibrary(\"ggplot2\")\nlibrary(\"plotly\")\nlibrary(\"scales\")\nlibrary(\"RColorBrewer\")\nlibrary(\"waiter\")\n# connecting and other\nlibrary(\"rsconnect\")\nlibrary(\"markdown\")\n\n\nLots of packages! The division is merely to remember how everything is managed and because I have OCD for this type of things.\nI want to focus on some packages:\n\ntidyverse, we all know it. As you can see, I also imported lots of single packages which compose the tidyverse, because I was getting errors of missing methods.\neurostat, which lets data flows from the eurostat website to my dashboard. This also lets the dashboard automatically updates when new data is available.\nscales, to nicely scaling my x and y axis.\nRColorBrewer, because I wanted to have a colorblind safe dashboard, even tough I am not.\nwaiter, for nice waiting images while the dashboard is loading.\n\nNow we can focus on the data importing and wrangling. For this, the eurostat library does the job. Lets focus on the crim_hom_vrel dataset.\n\n\nCode\n# search in eurostat db\nhomicide &lt;- search_eurostat(\"homicide\")\n\n# import data to variable\ncrim_hom_vrel &lt;- get_eurostat(\"crim_hom_vrel\", time_format = \"date\")\n\n# convert all observations to understandable data\ncrim_hom_vrel &lt;- label_eurostat(crim_hom_vrel)\n\n# label_eurostat_vars(crim_hom_vrel)\n\n# order data by country and date for time series purpose\ncrim_hom_vrel &lt;- crim_hom_vrel %&gt;% \n  arrange(geo, time)\n\ncrim_hom_vrel_grouped &lt;- crim_hom_vrel %&gt;% \n  dplyr::group_by(geo, time, sex, pers_cat, unit) %&gt;% \n  dplyr::summarise(values_grouped = sum(values), .groups = \"drop\") %&gt;% \n  filter(unit == \"Number\") %&gt;% \n  arrange(geo, time, sex)\n\n\nEverything pretty simple. I would like to highlight something about the dplyr::group_by and dplyr::summarise. As you can see, after having grouped and summarized, I need to drop the groups with the method .groups = \"drop\". With dplyr v.1.1.0 we can do the same with the help of the .by method in summarise.\n\n\nCode\ncrim_hom_vrel_grouped &lt;- crim_hom_vrel %&gt;% \n  dplyr::summarise(\n    values_grouped = sum(values),\n    .by = c(geo, time, sex, pers_cat, unit)\n    ) %&gt;% \n  filter(unit == \"Number\") %&gt;% \n  arrange(geo, time, sex)\n\n\nCopying from the dplyr website the differences between .by and group_by() are:\n\n\n\n.by\ngroup_by()\n\n\n\n\nGrouping only affects a single verb\nGrouping is persistent across multiple verbs\n\n\nSelects variables with tidy-select\nComputes expressions with data-masking\n\n\nSummaries use existing order of group keys\nSummaries sort group keys in ascending order\n\n\n\nLast part is all about colors.\n\n\nCode\n# brewer.pal(11, \"RdYlBu\")\npalette &lt;- c(\"#A50026\", \"#D73027\", \"#F46D43\", \"#FDAE61\", \"#FEE090\", \"#FFFFBF\", \"#E0F3F8\", \"#ABD9E9\", \"#74ADD1\", \"#4575B4\", \"#313695\")\n\npalette_crim_hom_vrel_grouped &lt;- rep(\n  palette,\n  length.out = crim_hom_vrel_grouped$geo %&gt;% str_unique() %&gt;% length()\n  )\n\n\nHere I defined the palette using ColorBrewer. Using rep I replicated the 11 colours for the length of the unique geo values."
  },
  {
    "objectID": "posts/eurostat_homicide_rate/eurostat_homicide_rate.html#ui.r",
    "href": "posts/eurostat_homicide_rate/eurostat_homicide_rate.html#ui.r",
    "title": "Eurostat Homicide Data",
    "section": "ui.R",
    "text": "ui.R"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Simone Brazzi",
    "section": "",
    "text": "I am a (data) nerd, with lot of passion and some mistake along the way. Data analyst by day, aspiring data wizard by night! I love using data to tell stories and drive business decisions. But I’m not content with stopping there. My passion for data and desire to expand my skillset has led me on a quest to become a data scientist. I’m a fearless problem solver with an insatiable curiosity, and I’m always seeking new challenges and opportunities to learn and grow. Let’s make some magic with data! When I don’t stare tolines of code or spreadsheet, I like to read and play video game or spend quality time with my dog."
  },
  {
    "objectID": "posts/toxic_comment_filter/toxic_comment_filter.html",
    "href": "posts/toxic_comment_filter/toxic_comment_filter.html",
    "title": "Toxic Comment Filter",
    "section": "",
    "text": "Costruire un modello in grado di filtrare i commenti degli utenti in base al grado di dannosità del linguaggio.\nPreprocessare il testo eliminando l’insieme di token che non danno contributo significativo a livello semantico.\nTrasformare il corpus testuale in sequenze.\nCostruire un modello di Deep Learning comprendente dei layer ricorrenti per un task di classificazione multilabel.\n\nIn prediction time, il modello deve ritornare un vettore contenente un 1 o uno 0 in corrispondenza di ogni label presente nel dataset (toxic, severe_toxic, obscene, threat, insult, identity_hate). In questo modo, un commento non dannoso sarà classificato da un vettore di soli 0 [0,0,0,0,0,0]. Al contrario, un commento pericoloso presenterà almeno un 1 tra le 6 labels."
  },
  {
    "objectID": "posts/toxic_comment_filter/toxic_comment_filter.html#import-r-libraries",
    "href": "posts/toxic_comment_filter/toxic_comment_filter.html#import-r-libraries",
    "title": "Toxic Comment Filter",
    "section": "2.1 Import R libraries",
    "text": "2.1 Import R libraries\nImport R libraries. These will be used for both the rendering of the document and data analysis. The reason is I prefer ggplot2 over matplotlib. I will also use colorblind safe palettes.\n\n\nCode\nlibrary(tidyverse, verbose = FALSE)\nlibrary(tidymodels, verbose = FALSE)\nlibrary(reticulate)\nlibrary(ggplot2)\nlibrary(plotly)\nlibrary(RColorBrewer)\nlibrary(bslib)\nlibrary(Metrics)\n\nreticulate::use_virtualenv(\"r-tf\")"
  },
  {
    "objectID": "posts/toxic_comment_filter/toxic_comment_filter.html#import-python-packages",
    "href": "posts/toxic_comment_filter/toxic_comment_filter.html#import-python-packages",
    "title": "Toxic Comment Filter",
    "section": "2.2 Import Python packages",
    "text": "2.2 Import Python packages\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport keras\nimport keras_nlp\n\nfrom keras.backend import clear_session\nfrom keras.models import Model, load_model\nfrom keras.layers import TextVectorization, Input, Dense, Embedding, Dropout, GlobalAveragePooling1D, LSTM, Bidirectional, GlobalMaxPool1D, Flatten, Attention\nfrom keras.metrics import Precision, Recall, AUC, SensitivityAtSpecificity, SpecificityAtSensitivity, F1Score\n\n\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.metrics import multilabel_confusion_matrix, classification_report, ConfusionMatrixDisplay, precision_recall_curve, f1_score, recall_score, roc_auc_score\n\n\nCreate a Config class to store all the useful parameters for the model and for the project."
  },
  {
    "objectID": "posts/toxic_comment_filter/toxic_comment_filter.html#class-config",
    "href": "posts/toxic_comment_filter/toxic_comment_filter.html#class-config",
    "title": "Toxic Comment Filter",
    "section": "2.3 Class Config",
    "text": "2.3 Class Config\nI created a class with all the basic configuration of the model, to improve the readability.\n\n\nCode\nclass Config():\n    def __init__(self):\n        self.url = \"https://s3.eu-west-3.amazonaws.com/profession.ai/datasets/Filter_Toxic_Comments_dataset.csv\"\n        self.max_tokens = 20000\n        self.output_sequence_length = 911 # check the analysis done to establish this value\n        self.embedding_dim = 128\n        self.batch_size = 32\n        self.epochs = 100\n        self.temp_split = 0.3\n        self.test_split = 0.5\n        self.random_state = 42\n        self.total_samples = 159571 # total train samples\n        self.train_samples = 111699\n        self.val_samples = 23936\n        self.features = 'comment_text'\n        self.labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n        self.new_labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate', \"clean\"]\n        self.label_mapping = {label: i for i, label in enumerate(self.labels)}\n        self.new_label_mapping = {label: i for i, label in enumerate(self.labels)}\n        self.path = \"/Users/simonebrazzi/R/blog/posts/toxic_comment_filter/history/f1score/\"\n        self.model =  self.path + \"model_f1.keras\"\n        self.checkpoint = self.path + \"checkpoint.lstm_model_f1.keras\"\n        self.history = self.path + \"lstm_model_f1.xlsx\"\n        \n        self.metrics = [\n            Precision(name='precision'),\n            Recall(name='recall'),\n            AUC(name='auc', multi_label=True, num_labels=len(self.labels)),\n            F1Score(name=\"f1\", average=\"macro\")\n            \n        ]\n    def get_early_stopping(self):\n        early_stopping = keras.callbacks.EarlyStopping(\n            monitor=\"val_f1\", # \"val_recall\",\n            min_delta=0.2,\n            patience=10,\n            verbose=0,\n            mode=\"max\",\n            restore_best_weights=True,\n            start_from_epoch=3\n        )\n        return early_stopping\n\n    def get_model_checkpoint(self, filepath):\n        model_checkpoint = keras.callbacks.ModelCheckpoint(\n            filepath=filepath,\n            monitor=\"val_f1\", # \"val_recall\",\n            verbose=0,\n            save_best_only=True,\n            save_weights_only=False,\n            mode=\"max\",\n            save_freq=\"epoch\"\n        )\n        return model_checkpoint\n\n    def find_optimal_threshold_cv(self, ytrue, yproba, metric, thresholds=np.arange(.05, .35, .05), n_splits=7):\n\n      # instantiate KFold\n      kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n      threshold_scores = []\n\n      for threshold in thresholds:\n\n        cv_scores = []\n        for train_index, val_index in kf.split(ytrue):\n\n          ytrue_val = ytrue[val_index]\n          yproba_val = yproba[val_index]\n\n          ypred_val = (yproba_val &gt;= threshold).astype(int)\n          score = metric(ytrue_val, ypred_val, average=\"macro\")\n          cv_scores.append(score)\n\n        mean_score = np.mean(cv_scores)\n        threshold_scores.append((threshold, mean_score))\n\n        # Find the threshold with the highest mean score\n        best_threshold, best_score = max(threshold_scores, key=lambda x: x[1])\n      return best_threshold, best_score\n\nconfig = Config()"
  },
  {
    "objectID": "posts/toxic_comment_filter/toxic_comment_filter.html#eda",
    "href": "posts/toxic_comment_filter/toxic_comment_filter.html#eda",
    "title": "Toxic Comment Filter",
    "section": "3.1 EDA",
    "text": "3.1 EDA\nFirst a check on the dataset to find possible missing values and imbalances.\n\n3.1.1 Frequency\n\nCode\nlibrary(reticulate)\ndf_r &lt;- py$df\nnew_labels_r &lt;- py$config$new_labels\n\ndf_r_grouped &lt;- df_r %&gt;% \n  select(all_of(new_labels_r)) %&gt;%\n  pivot_longer(\n    cols = all_of(new_labels_r),\n    names_to = \"label\",\n    values_to = \"value\"\n  ) %&gt;% \n  group_by(label) %&gt;%\n  summarise(count = sum(value)) %&gt;% \n  mutate(freq = round(count / sum(count), 4))\n\ndf_r_grouped\n\n\n\n\nTable 2: Absolute and relative labels frequency\n\n\n\n# A tibble: 7 × 3\n  label          count   freq\n  &lt;chr&gt;          &lt;dbl&gt;  &lt;dbl&gt;\n1 clean         143346 0.803 \n2 identity_hate   1405 0.0079\n3 insult          7877 0.0441\n4 obscene         8449 0.0473\n5 severe_toxic    1595 0.0089\n6 threat           478 0.0027\n7 toxic          15294 0.0857\n\n\n\n\n\n\n3.1.2 Barchart\n\n\nCode\nlibrary(reticulate)\nbarchart &lt;- df_r_grouped %&gt;%\n  ggplot(aes(x = reorder(label, count), y = count, fill = label)) +\n  geom_col() +\n  labs(\n    x = \"Labels\",\n    y = \"Count\"\n  ) +\n  # sort bars in descending order\n  scale_x_discrete(limits = df_r_grouped$label[order(df_r_grouped$count, decreasing = TRUE)]) +\n  scale_fill_brewer(type = \"seq\", palette = \"RdYlBu\") +\n  theme_minimal()\nggplotly(barchart)\n\n\n\n\n\n\n\n\nFigure 1: Imbalance in the dataset with clean variable\n\n\n\n\nIt is visible how much the dataset in imbalanced. This means it could be useful to check for the class weight and use this argument during the training.\nIt is clear that most of our text are clean. We are talking about 0.8033 of the observations which are clean. Only 0.1967 are toxic comments."
  },
  {
    "objectID": "posts/toxic_comment_filter/toxic_comment_filter.html#sequence-lenght-definition",
    "href": "posts/toxic_comment_filter/toxic_comment_filter.html#sequence-lenght-definition",
    "title": "Toxic Comment Filter",
    "section": "3.2 Sequence lenght definition",
    "text": "3.2 Sequence lenght definition\nTo convert the text in a useful input for a NN, it is necessary to use a TextVectorization layer. See the Section 4 section.\nOne of the method is output_sequence_length: to better define it, it is useful to analyze our text length. To simulate what the model we do, we are going to remove the punctuation and the new lines from the comments.\n\n3.2.1 Summary\n\nCode\nlibrary(reticulate)\ndf_r %&gt;% \n  mutate(\n    comment_text_clean = comment_text %&gt;%\n      tolower() %&gt;% \n      str_remove_all(\"[[:punct:]]\") %&gt;% \n      str_replace_all(\"\\n\", \" \"),\n    text_length = comment_text_clean %&gt;% str_count()\n    ) %&gt;% \n  pull(text_length) %&gt;% \n  summary() %&gt;% \n  as.list() %&gt;% \n  as_tibble()\n\n\n\n\nTable 3: Summary of text length\n\n\n\n# A tibble: 1 × 6\n   Min. `1st Qu.` Median  Mean `3rd Qu.`  Max.\n  &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1     4        91    196  378.       419  5000\n\n\n\n\n\n\n3.2.2 Boxplot\n\n\nCode\nlibrary(reticulate)\nboxplot &lt;- df_r %&gt;% \n  mutate(\n    comment_text_clean = comment_text %&gt;%\n      tolower() %&gt;% \n      str_remove_all(\"[[:punct:]]\") %&gt;% \n      str_replace_all(\"\\n\", \" \"),\n    text_length = comment_text_clean %&gt;% str_count()\n    ) %&gt;% \n  # pull(text_length) %&gt;% \n  ggplot(aes(y = text_length)) +\n  geom_boxplot() +\n  theme_minimal()\nggplotly(boxplot)\n\n\n\n\n\n\n\n\nFigure 2: Text length boxplot\n\n\n\n\n\n\n3.2.3 Histogram\n\n\nCode\nlibrary(reticulate)\ndf_ &lt;- df_r %&gt;% \n  mutate(\n    comment_text_clean = comment_text %&gt;%\n      tolower() %&gt;% \n      str_remove_all(\"[[:punct:]]\") %&gt;% \n      str_replace_all(\"\\n\", \" \"),\n    text_length = comment_text_clean %&gt;% str_count()\n  )\n\nQ1 &lt;- quantile(df_$text_length, 0.25)\nQ3 &lt;- quantile(df_$text_length, 0.75)\nIQR &lt;- Q3 - Q1\nupper_fence &lt;- as.integer(Q3 + 1.5 * IQR)\n\nhistogram &lt;- df_ %&gt;% \n  ggplot(aes(x = text_length)) +\n  geom_histogram(bins = 50) +\n  geom_vline(aes(xintercept = upper_fence), color = \"red\", linetype = \"dashed\", linewidth = 1) +\n  theme_minimal() +\n  xlab(\"Text Length\") +\n  ylab(\"Frequency\") +\n  xlim(0, max(df_$text_length, upper_fence))\nggplotly(histogram)\n\n\n\n\n\n\n\n\nFigure 3: Text length histogram with boxplot upper fence\n\n\n\n\nConsidering all the above analysis, I think a good starting value for the output_sequence_length is 911, the upper fence of the boxplot. In the last plot, it is the dashed red vertical line.. Doing so, we are removing the outliers, which are a small part of our dataset."
  },
  {
    "objectID": "posts/toxic_comment_filter/toxic_comment_filter.html#dataset",
    "href": "posts/toxic_comment_filter/toxic_comment_filter.html#dataset",
    "title": "Toxic Comment Filter",
    "section": "3.3 Dataset",
    "text": "3.3 Dataset\nNow we can split the dataset in 3: train, test and validation sets. Considering there is not a function in sklearn which lets split in these 3 sets, we can do the following: - split between a train and temporary set with a 0.3 split. - split the temporary set in 2 equal sized test and val sets.\n\n\nCode\nx = df[config.features].values\ny = df[config.labels].values\n\nxtrain, xtemp, ytrain, ytemp = train_test_split(\n  x,\n  y,\n  test_size=config.temp_split, # .3\n  random_state=config.random_state\n  )\nxtest, xval, ytest, yval = train_test_split(\n  xtemp,\n  ytemp,\n  test_size=config.test_split, # .5\n  random_state=config.random_state\n  )\n\n\nxtrain shape: py$xtrain.shape ytrain shape: py$ytrain.shape xtest shape: py$xtest.shape ytest shape: py$ytest.shape xval shape: py$xval.shape yval shape: py$yval.shape\nThe datasets are created using the tf.data.Dataset function. It creates a data input pipeline. The tf.data API makes it possible to handle large amounts of data, read from different data formats, and perform complex transformations. The tf.data.Dataset is an abstraction that represents a sequence of elements, in which each element consists of one or more components. Here each dataset is creates using from_tensor_slices. It create a tf.data.Dataset from a tuple (features, labels). .batch let us work in batches to improve performance, while .prefetch overlaps the preprocessing and model execution of a training step. While the model is executing training step s, the input pipeline is reading the data for step s+1. Check the documentation for further informations.\n\n\nCode\ntrain_ds = (\n    tf.data.Dataset\n    .from_tensor_slices((xtrain, ytrain))\n    .shuffle(xtrain.shape[0])\n    .batch(config.batch_size)\n    .prefetch(tf.data.experimental.AUTOTUNE)\n)\n\ntest_ds = (\n    tf.data.Dataset\n    .from_tensor_slices((xtest, ytest))\n    .batch(config.batch_size)\n    .prefetch(tf.data.experimental.AUTOTUNE)\n)\n\nval_ds = (\n    tf.data.Dataset\n    .from_tensor_slices((xval, yval))\n    .batch(config.batch_size)\n    .prefetch(tf.data.experimental.AUTOTUNE)\n)\n\n\n\n\nCode\nprint(\n  f\"train_ds cardinality: {train_ds.cardinality()}\\n\",\n  f\"val_ds cardinality: {val_ds.cardinality()}\\n\",\n  f\"test_ds cardinality: {test_ds.cardinality()}\\n\"\n  )\n\n\ntrain_ds cardinality: 3491\n val_ds cardinality: 748\n test_ds cardinality: 748\n\n\nCheck the first element of the dataset to be sure that the preprocessing is done correctly.\n\n\nCode\ntrain_ds.as_numpy_iterator().next()\n\n\n(array([b\"Meivazhi\\nI've had a go at restarting the Meivazhi article in a style that's more standard for Wikipedia articles. Someone would probably have deleted it pretty quickly if it had stayed in the form you posted. I'd be grateful if you could help at Talk:Meivazhi about the accuracy. \\n\\nUnfortunately I had to remove your links. The conflict of interest guidelines advise against editors linking to their own sites, and also Wikipedia's attribution policy WP:ATT requires that information should come from third-party published sources rather than personal websites. Do you know of any good newspaper/book accounts of Meivazhi?  \\n\\nPS What is the salaimanimudi.indlist.com site? A personal site by a member?\",\n       b'Very well, thanks for the clarification.',\n       b'\"\\n\\nHi Emopunkundead13, and Welcome to Wikipedia!  \\nWelcome to Wikipedia! I hope you enjoy the encyclopedia and want to stay. As a first step, you may wish to read the Introduction.\\n\\nIf you have any questions, feel free to ask me at my talk page \\xe2\\x80\\x94 I\\'m happy to help. Or, you can ask your question at the New contributors\\' help page.\\n\\n  \\nHere are some more resources to help you as you explore and contribute to the world\\'s largest encyclopedia...\\n\\n Finding your way around: \\n\\n Table of Contents\\n\\n Department directory\\n\\n Need help? \\n\\n Questions \\xe2\\x80\\x94 a guide on where to ask questions.\\n Cheatsheet \\xe2\\x80\\x94 quick reference on Wikipedia\\'s mark-up codes.\\n\\n Wikipedia\\'s 5 pillars \\xe2\\x80\\x94 an overview of Wikipedia\\'s foundations\\n The Simplified Ruleset \\xe2\\x80\\x94 a summary of Wikipedia\\'s most important rules.\\n\\n How you can help: \\n\\n Contributing to Wikipedia \\xe2\\x80\\x94 a guide on how you can help.\\n\\n Community Portal \\xe2\\x80\\x94 Wikipedia\\'s hub of activity.\\n\\n Additional tips...  \\n\\n Please sign your messages on talk pages with four tildes (~~~~). This will automatically insert your \"\"signature\"\" (your username and a date stamp). The  button, on the tool bar above Wikipedia\\'s text editing window, also does this. \\n\\n If you would like to play around with your new Wiki skills the Sandbox is for you. \\n\\n Good luck, and have fun. \\n\"',\n       b'the entire human race is quite impressive',\n       b'\"\\n\\n{{unblock|1=\\nRuslik0 is incorrect in his block of me. I wasn\\'t edit waring as defined by Wikipedia: I made one reversion of an edit today.\\n\\nFurthermore, Ruslik0 is incorrect that I made a false accusation regarding the objective fact that Jeffro77 is engaging in vandalism: Jeffro77 replaced this entrywhich is very similar to the version that existed there since October 31, 2008, with some improvementswith this entry, giving the excuse in his edit summary of \"\"WP:FRINGE,\"\" which doesn\\'t even make sense as an explanation for his edits: i.e., deletion of a number of citations; deletion of the information on theologian Prof. Wolfhart Pannenberg\\'s defense of the theology of the Omega Point Theory; etc. As well, there\\'s no need for the large displayed quote, as the previous entry already stated that Prof. David Deutsch doesn\\'t agree that the Omega Point is God; furthermore, Jeffro77\\'s edit deletes the mention of the fact that Prof. Deutsch endorses the physics of the Omega Point Theory.\\n\\nIn addtion, Jeffro77\\'s edit isn\\'t even literate, as he give the following mangled citation to Prof. Deutsch: \"\"Chapter 14: \"\"The Ends of the Universe,\"\" with additional comments by Frank J. Tipler; also available here\"\". Whereas the version before was properly cited: \"\"David Deutsch, The Fabric of Reality: The Science of Parallel Universes\\xe2\\x80\\x94and Its Implications (London: Allen Lane The Penguin Press, 1997), ISBN 0713990619. Extracts from Chapter 14: \"\"The Ends of the Universe,\"\" with additional comments by Frank J. Tipler; also available here and here.\"\"\\n\\nAs stated above, Jeffro77\\'s excuse in his edit summary doesn\\'t even make sense, as it doesn\\'t explain why he would delete mention of Prof. Deutsch\\'s endorsement of the physics of the Omega Point Theory, particularly since Jeffro77 himself called Deutsch an \"\"eminent physicist\"\" in his own edit (i.e., that statement wasn\\'t there before): of which argues against the notion that Jeffro77 considers the physics of the Omega Point Theory as fringe. Further, Jeffro77 deleted mention of the fact that Prof. Wolfhart Pannenberg, who is one of the leading theologians in the world, has defended the theology of the Omega Point Theory and Tipler\\'s position that the Omega Point is consistent with the Judeo-Christian God.\\n\\nAdditionally, while Jeffro77\\'s \"\"fringe\"\" claim\\'s aren\\'t even relevant to his edit, they have already been refuted numerous times. Indeed, Jeffro77 himself refutes this claim in this very edit of his: to state again, therein Jeffro77 himself called Prof. Deutsch an \"\"eminent physicist\"\" in his own words. So obviously Jeffro77 himself must consider Deutsch\\'s endorsement of the physics of the Omega Point Theory to be noteworthy, and yet he deleted this endoresement in an area where Jeffro77 himself agrees that Deutsch is eminently qualified and replaced it with a large displayed quotation regarding a matter that Deutsch has no qualification or erudition in, even though the previous version already clearly mentioned that Deutsch disagrees that Omega Point is God. Moreover, in this edit Jeffro77 deletes all mention of the fact that an actual trained theologian, Prof. Wolfhart Pannenberg, who is one of the world\\'s leading theologians, has defended the theology of the Omega Point Theory.\\n\\nAs well, Prof. Tipler himself has defended the theology of the Omega Point Theory and his identification of the Omega Point as being God in a peer-reviewed academic journal: see Frank J. Tipler, \"\"The Omega Point as Eschaton: Answers to Pannenberg\\'s Questions for Scientists,\"\" Zygon: Journal of Religion & Science, Vol. 24, Issue 2 (June 1989), pp. 217-253. Regarding the physics, Prof. Tipler has published his Omega Point Theory in many peer-reviewed science journals, including a number of the leading physics journals such as Reports on Progress in Physics (one of the world\\'s leading physics journals) and Monthly Notices of the Royal Astronomical Society (one of the world\\'s leading astrophysics journals). The Wikipedia article on the Omega Point Theory lists seven different mainstream peer-reviewed scientific journals in which Tipler\\'s Omega Point Theory has been published (and that\\'s not including the Zygon journal). That is quite a significant amount.\\n\\nJeffro77 is out of',\n       b'come on dont u have the balls to block me',\n       b'Genealogists routinely make the mistake of assuming that the father-of-record is necessarily the biological father.  However, this is not always the case in polygamous Mormon culture, as wives are officially and unofficially shifted from man to man.  Even sisters have been traded.  We will never really know the full extent of what went on in the Mexican Mormon colony the Romneys sprang fromof course, that was precisely why the left the United States to begin with.  It makes sense, of course, that Romney and his political operatives would want to keep these issues from the American people.',\n       b\"Then I will make a copyright dispute, which could have been avoided had you given me five minutes to complete my comments on the talk page.  I can prove conclusively the text is mine (I coined the term 'infinity snake' myself and use it to find plagiarism) so I'm not that concerned about it.  You however seem to have a personal issue with me (or a simple power trip) that you're acting on very in appropriately, and if you can't have a mature discussion, I'm going to have no choice but to request arbitration.\",\n       b'You live in Nashville, right?',\n       b\"2010 (UTC)\\n\\nSeriously, fuck this guy's fucking face.  I am so fucking sick of fucking seeing it every fucking time I want to fucking read the fucking article about fucking boobs on the fucking internet. 160.39.54.179  02:10, 23 December\",\n       b\"I'd just keep quiet if I were you.\",\n       b'can u tell me reasons why this person is notabl;e but B. S. Sahay is.',\n       b'Secord\\nDo you have any source at all about this guy? What is Captol Records #776 Union Session? If I google it nothing comes up at all.',\n       b'REDIRECT Talk:The Fast and the Furious: Tokyo Drift (Original Motion Picture Score)',\n       b\"But I LOVE pussy!  Just kidding.  I understand what you mean.  And it's okay, becuase I'm sure everyone else knows about Flewis.  He actually is bragging on his page about ge3tting someone blocked.  Man, he might have the right to do that, but bragging about it is like such a little kid I'm ebarassed for him\",\n       b\"Toronto'''\\n\\nHamilton, Toronto,and Oshawa are now tethered together with the criteria set forth in the article. As such, Toronto, Hamilton and Oshawa form I contiguous urban area, and reaches a pop near 6 mil. Check google earth if you have any doubts.\",\n       b'\"\\nThat does not make it yours; when you hit \"\"save\"\" you automatically surrender all rights to it. On your second point, your participation here is a privilege, not a right.  &gt; hane\\xca\\xbc \"',\n       b'Ill be back in a little later then Ill have a minute to type it out . 68.39.152.45',\n       b'\"\\n\\nI\\'m not defending or advocating anything.  It\\'s about notability.  \"\"What makes Wikipediocracy notable?\"\"  Editors are trying to keep the most visible and notable practice out of the lede for reasons that seem to be defensive about what should go there.  Why do we need to tiptoe around it?  It is what is.  Calling them pedophiles would cross the line without reliable sources and that doesn\\'t seem to be supported and is, of course, a crime.  There should be literally no resistance to pointing out their main avenue of notability -&gt; which is investigating and exposing editors they believe are doing harm.  That\\'s what they do.  That\\'s what they are known for.  I\\'m not morally weighing whether they are right or wrong, just stating what is the elephant in the room.    \"',\n       b'\"\\n\\n Please do not remove all content from pages without explanation, as you did with this edit to Witch and Wizard. If you continue to do so, you will be blocked from editing.  \\xe2\\x80\\xa2talk\\xe2\\x80\\xa2trib \"',\n       b\"The origin of the Pearl of Great Price \\n\\nI recently added a paragraph on the origin of the Pearl of Great Price.  The paragraph was reverted.  Before we get into a reversion war, I would like to know what the problem is with my paragraph.  I'm happy to put it wherever deemed appropriate.  But the origin should be noted, correct.  Also, the fact that it was, according to scholars, mis-translated should also be noted, correct?  I think these facts are important to the article.\",\n       b\"YOU'RE RETARDED!=\\n\\nSCREW YOU FOR DELETING MY ARTICLE! YOU'RE RETARDED! GO KILL YOURSELF IN A BARREL FULL OF SHIT!\",\n       b'\"==Image source problem with Image:Visuel2.jpg==\\n\\nThanks for uploading Image:Visuel2.jpg. I noticed that the file\\'s description page currently doesn\\'t specify who created the content, so the copyright status is unclear. If you did not create this file yourself, you will need to specify the owner of the copyright. If you obtained it from a website, then a link to the website from which it was taken, together with a restatement of that website\\'s terms of use of its content, is usually sufficient information. However, if the copyright holder is different from the website\\'s publisher, their copyright should also be acknowledged.\\n\\nAs well as adding the source, please add a proper copyright licensing tag if the file doesn\\'t have one already. If you created/took the picture, audio, or video then the  tag can be used to release it under the GFDL. If you believe the media meets the criteria at Wikipedia:Non-free content, use a tag such as  or one of the other tags listed at Wikipedia:Image copyright tags#Fair use. See Wikipedia:Image copyright tags for the full list of copyright tags that you can use.\\n\\nIf you have uploaded other files, consider checking that you have specified their source and tagged them, too. You can find a list of files you have uploaded by following this link. Unsourced and untagged images may be deleted one week after they have been tagged, as described on criteria for speedy deletion. If the image is copyrighted under a non-free license (per Wikipedia:Fair use) then the image will be deleted 48 hours after . If you have any questions please ask them at the Media copyright questions page. Thank you.  (Talk, ) \"',\n       b'Also, he is a bad person',\n       b\"SSSSSSSSSSSSShhhhhhhh \\n\\nGo away. Don't use my suerpage again for any reason or you will regret it.\",\n       b\"Photos\\nHow come are only 5 pictures with Madonna?Others articles have more pictures.Don't you think that a photo from Blond Ambition Tour,and a screen shot from Like a virgin;Frozen or Hung up videos should be added?Thank you!thesweetlamb.\",\n       b\"Hypocricy and Jealousy\\nI want to let everyone here know that the author deleted my articles when I attempted to create new ones. This is due to the author's insecurites and racist views about the accuracy of my info. I know that many of you feel the same. But don't worry. We'll definitely get around this and the racist author.\",\n       b'fair enough\\n\\nits a fair point...... but why oh why did i do so much fulfilling all of the wikpedia guidlines for personal content , editting style , correct sourcing and referencing , wiki formatting etc .....months tireless work on an article which was to be so casually consigned to the dustbin....... \\n\\nhowever much i respect your views, a redirect to the clairvoyance article  would however be entirely meaningless... one is not exchangeable for the other...  like cheese isn`t milk etc.....\\n\\ncovering the groundwork general background to establish a context for understanding extra sensory phenomena in general needed to be done  and it might as well happen at the Clairsentience page for now.....  \\n\\nthe reason i did it there was because fundamental doubts were being expressed about my earliest article`s conents were concerned implicitly announcing that the background for understanding extra sensory phenomena in general had not been done.... which was  ... as remember saying to you a few weeks ago   the frustrating context which motivated the writing of the second article with all of its references to the background research of brennan , lylle , mckenna , bohm , wilber , sheldrake   etc......because this background had to be established before any specifics about clairsentience could even be begun to be aproached  ....... thus the many weeks days and months of toil which has been endured to even get this background matereal into a wikpedia format and guidlines shape  ......  all of which criterea have been met  .... or at leasst  were until the finished product was mindlessly deleted...\\n\\nthe new additional altered states matereal  was a tentative beginning into finding a context for tentatively describing the specifics of the clairsentience phenomena itself...\\n\\nbut , agian it was trashed with no thought or care....',\n       b'\"\\n\\nPlease replace the deleted page \"\"A Veritable Smorgasboard\"\". Deleting it was completely unnecessary. Honestly, do you think Wikipdedia can\\'t handle another 150kB page? I would fix this for you, but I do not know my way around Wikipedia\\'s syntax.\\n(Apologies in advance for my deficient signature) - Fiantres\"',\n       b\"hahaha, nice try.  but they don't agree with your ethnocentric bullshit.  try that on for size!   17:08, 16 Jun 2005 (UTC)\",\n       b'Oppose There is no doubt the Nazi used Gun Control as a means of suppression. To deny that is idiotic or agenda promoting. The argument that the right to bear arms is a deterent to suppression is very strong and logical. It seems logic and political agenda pushers often diverge. Suppressed people do not have the right to bear arms. Those who seek to suppress people for their political purposes seek to suppress the right to bear arms. 172.56.11.104',\n       b'\"\\n\\n Pozdrav \\n\\nPozdrav Direktore, ovo je tekst koji sam stavio odmah iza tvog odgovora na diskusionoj strani od clanka \"\"Differences between standard Serbian/Croatian/Bosnian language\"\". Pozdrav tebi i sve najbolje:\\nTotally agree with you Direktor, and I am sure that your highly reasonable opinions and facts you are presenting on wikipedia are highly respected and accepted by the waste majority of its readers. Please, if you can, - do your regular check-ups of the articles concerning South Slavic languages and make sure they\\xe2\\x80\\x99re not presenting some misleading information. I would just add to this, that although \\xe2\\x80\\x98the political reason is the officially maintained distaste of the so-called \"\"pan-Yugoslav commonness\\xe2\\x80\\x9d\\xe2\\x80\\x99, this distaste should be \\xe2\\x80\\x98expressed\\xe2\\x80\\x99 in a more civilized manner than creating some non-acceptable partial maps showing only a half of the Shtokavian speaking area, or trying to camouflage the factual state of the close ties within the Central South Slavic system (language) at the templates featured in the articles about South Slavic languages and dialects. And most importantly - that ridiculous \\xe2\\x80\\x98pan-Yugo\\xe2\\x80\\x99 distaste which openly sends a message of hatred and separatism, should not, by any mean be reflected in the language area, because it only shows how low and how uncivilized its \\xe2\\x80\\x98supporters\\xe2\\x80\\x99 can be, no matter what \\xe2\\x80\\x98not-supported-by anyone-in-scientific-world\\xe2\\x80\\x99 theories they may point out as their sources. \\nAnd, at the end, as an example of a civilized political distaste between nations, here\\xe2\\x80\\x99s the example of Americans and Canadians. Majority of Canadians feel in a different extent, a kind of aversion to Americans, especially since the start of the \\xe2\\x80\\x98George-Bush-era\\xe2\\x80\\x99 in USA. But still, nobody says that Canadians speak a \\xe2\\x80\\x98different language\\xe2\\x80\\x99 than Americans, and nobody tries to hide the common history facts and the strong cultural and economical ties that exist between these 2 countries. This is a typical way of overcoming any kind of \\xe2\\x80\\x98distaste\\xe2\\x80\\x99 in a human and civilized way, all the other ways are just a shame for humanity.\\nBest Regards to you Direktor and to your beautiful and cosmopolitan Split and Dalmatia. Best Cheerful Greetings;\"'],\n      dtype=object), array([[0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [1, 1, 1, 0, 1, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [1, 0, 1, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0],\n       [1, 0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [1, 0, 1, 0, 1, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0]]))\n\n\nAnd we check also the shape. We expect a feature of shape (batch, ) and a target of shape (batch, number of labels).\n\n\nCode\nprint(\n  f\"text train shape: {train_ds.as_numpy_iterator().next()[0].shape}\\n\",\n  f\" text train type: {train_ds.as_numpy_iterator().next()[0].dtype}\\n\",\n  f\"label train shape: {train_ds.as_numpy_iterator().next()[1].shape}\\n\",\n  f\"label train type: {train_ds.as_numpy_iterator().next()[1].dtype}\\n\"\n  )\n\n\ntext train shape: (32,)\n  text train type: object\n label train shape: (32, 6)\n label train type: int64"
  },
  {
    "objectID": "posts/toxic_comment_filter/toxic_comment_filter.html#definition",
    "href": "posts/toxic_comment_filter/toxic_comment_filter.html#definition",
    "title": "Toxic Comment Filter",
    "section": "5.1 Definition",
    "text": "5.1 Definition\nDefine the model using the Functional API.\n\n\nCode\ndef get_deeper_lstm_model():\n    clear_session()\n    inputs = Input(shape=(None,), dtype=tf.int64, name=\"inputs\")\n    embedding = Embedding(\n        input_dim=config.max_tokens,\n        output_dim=config.embedding_dim,\n        mask_zero=True,\n        name=\"embedding\"\n    )(inputs)\n    x = Bidirectional(LSTM(256, return_sequences=True, name=\"bilstm_1\"))(embedding)\n    x = Bidirectional(LSTM(128, return_sequences=True, name=\"bilstm_2\"))(x)\n    # Global average pooling\n    x = GlobalAveragePooling1D()(x)\n    # Add regularization\n    x = Dropout(0.3)(x)\n    x = Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n    x = LayerNormalization()(x)\n    outputs = Dense(len(config.labels), activation='sigmoid', name=\"outputs\")(x)\n    model = Model(inputs, outputs)\n    model.compile(optimizer='adam', loss=\"binary_crossentropy\", metrics=config.metrics, steps_per_execution=32)\n    \n    return model\n\nlstm_model = get_deeper_lstm_model()\nlstm_model.summary()"
  },
  {
    "objectID": "posts/toxic_comment_filter/toxic_comment_filter.html#callbacks",
    "href": "posts/toxic_comment_filter/toxic_comment_filter.html#callbacks",
    "title": "Toxic Comment Filter",
    "section": "5.2 Callbacks",
    "text": "5.2 Callbacks\nFinally, the model has been trained using 2 callbacks: - Early Stopping, to avoid to consume the kaggle GPU time. - Model Checkpoint, to retrieve the best model training information.\n\n\nCode\n# callbacks\nmy_es = config.get_early_stopping()\nmy_mc = config.get_model_checkpoint(filepath=\"/checkpoint.keras\")\ncallbacks = [my_es, my_mc]"
  },
  {
    "objectID": "posts/toxic_comment_filter/toxic_comment_filter.html#final-preparation-before-fit",
    "href": "posts/toxic_comment_filter/toxic_comment_filter.html#final-preparation-before-fit",
    "title": "Toxic Comment Filter",
    "section": "5.3 Final preparation before fit",
    "text": "5.3 Final preparation before fit\nConsidering the dataset is imbalanced, to increase the performance we need to calculate the class weight. This will be passed during the training of the model.\n\n\nCode\nlab = pd.DataFrame(columns=config.labels, data=ytrain)\nr = lab.sum() / len(ytrain)\nclass_weight = dict(zip(range(len(config.labels)), r))\ndf_class_weight = pd.DataFrame.from_dict(\n  data=class_weight,\n  orient='index',\n  columns=['class_weight']\n  )\ndf_class_weight.index = config.labels\n\n\n\nCode\nlibrary(reticulate)\npy$df_class_weight\n\n\n\n\nTable 4: Class weight\n\n\n\n              class_weight\ntoxic          0.095900590\nsevere_toxic   0.009928468\nobscene        0.052757858\nthreat         0.003061800\ninsult         0.049132042\nidentity_hate  0.008710911\n\n\n\n\nIt is also useful to define the steps per epoch for train and validation dataset. This step is required to avoid to not consume entirely the dataset during the fit, which happened to me.\n\n\nCode\nsteps_per_epoch = config.train_samples // config.batch_size\nvalidation_steps = config.val_samples // config.batch_size"
  },
  {
    "objectID": "posts/toxic_comment_filter/toxic_comment_filter.html#fit",
    "href": "posts/toxic_comment_filter/toxic_comment_filter.html#fit",
    "title": "Toxic Comment Filter",
    "section": "5.4 Fit",
    "text": "5.4 Fit\nThe fit has been done on Kaggle to levarage the GPU. Some considerations about the model:\n\n.repeat() ensure the model sees all the dataset.\nepocs is set to 100.\nvalidation_data has the same repeat.\ncallbacks are the one defined before.\nclass_weight ensure the model is trained using the frequency of each class, because our dataset is imbalanced.\nsteps_per_epoch and validation_steps depend on the use of repeat.\n\n\n\nCode\nhistory = model.fit(\n  processed_train_ds.repeat(),\n  epochs=config.epochs,\n  validation_data=processed_val_ds.repeat(),\n  callbacks=callbacks,\n  class_weight=class_weight,\n  steps_per_epoch=steps_per_epoch,\n  validation_steps=validation_steps\n  )\n\n\nNow we can import the model and the history trained on Kaggle.\n\n\nCode\nmodel = load_model(filepath=config.model)\nhistory = pd.read_excel(config.history)"
  },
  {
    "objectID": "posts/toxic_comment_filter/toxic_comment_filter.html#evaluate",
    "href": "posts/toxic_comment_filter/toxic_comment_filter.html#evaluate",
    "title": "Toxic Comment Filter",
    "section": "5.5 Evaluate",
    "text": "5.5 Evaluate\n\n\nCode\n\nvalidation = model.evaluate(\n  processed_val_ds.repeat(),\n  steps=validation_steps, # 748\n  verbose=0\n  )\n\n\n\nCode\nval_metrics &lt;- tibble(\n  metric = c(\"loss\", \"precision\", \"recall\", \"auc\", \"f1_score\"),\n  value = py$validation\n  )\nval_metrics\n\n\n\n\nTable 5: Model validation metric\n\n\n\n# A tibble: 5 × 2\n  metric     value\n  &lt;chr&gt;      &lt;dbl&gt;\n1 loss      0.0542\n2 precision 0.789 \n3 recall    0.671 \n4 auc       0.957 \n5 f1_score  0.0293"
  },
  {
    "objectID": "posts/toxic_comment_filter/toxic_comment_filter.html#predict",
    "href": "posts/toxic_comment_filter/toxic_comment_filter.html#predict",
    "title": "Toxic Comment Filter",
    "section": "5.6 Predict",
    "text": "5.6 Predict\nFor the prediction, the model does not need to repeat the dataset, because it has already been trained on all of the train data. Now it has just to consume the new data to make the prediction.\n\n\nCode\n\npredictions = model.predict(processed_test_ds, verbose=0)"
  },
  {
    "objectID": "posts/toxic_comment_filter/toxic_comment_filter.html#confusion-matrix",
    "href": "posts/toxic_comment_filter/toxic_comment_filter.html#confusion-matrix",
    "title": "Toxic Comment Filter",
    "section": "5.7 Confusion Matrix",
    "text": "5.7 Confusion Matrix\nThe best way to assess the performance of a multi label classification is using a confusion matrix. Sklearn has a specific function to create a multi label classification matrix to handle the fact that there could be multiple labels for one prediction.\n\n5.7.1 Grid Search Cross Validation for best threshold\nGrid Search CV is a technique for fine-tuning hyperparameter of a ML model. It systematically search through a set of hyperparamenter values to find the combination which led to the best model performance. In this case, I am using a KFold Cross Validation is a resempling technique to split the data into k consecutive folds. Each fold is used once as a validation while the k - 1 remaining folds are the training set. See the documentation for more information.\nThe model is trained to optimize the recall. The decision was made because the cost of missing a True Positive is greater than a False Positive. In this case, missing a injurious observation is worst than classifying a clean one as bad.\n\n\n5.7.2 Confidence threshold and Precision-Recall trade off\nWhilst the KFold GDCV technique is usefull to test multiple hyperparameter, it is important to understand the problem we are facing. A multi label deep learning classifier outputs a vector of per-class probabilities. These need to be converted to a binary vector using a confidence threshold.\n\nThe higher the threshold, the less classes the model predicts, increasing model confidence [higher Precision] and increasing missed classes [lower Recall].\nThe lower the threshold, the more classes the model predicts, decreasing model confidence [lower Precision] and decreasing missed classes [higher Recall].\n\nThreshold selection mean we have to decide which metric to prioritize, based on the problem we are facing and the relative cost of misduging. We can consider the toxic comment filtering a problem similiar to cancer diagnostic. It is better to predict cancer in people who do not have it [False Positive] and perform further analysis than do not predict cancer when the patient has the disease [False Negative].\nI decide to train the model on the F1 score to have a balanced model in both precision and recall and leave to the threshold selection to increase the recall performance.\nMoreover, the model has been trained on the macro avarage F1 score, which is a single performance indicator obtained by the mean of the Precision and Recall scores of individual classses.\n\\[\nF1\\ macro\\ avg = \\frac{\\sum_{i=1}^{n} F1_i}{n}\n\\]\nIt is useful with imbalanced classes, because it weights each classes equally. It is not influenced by the number of samples of each classes. This is sette both in the config.metrics and find_optimal_threshold_cv.\n\n5.7.2.1 f1_score\n\n\nCode\nytrue = ytest.astype(int)\ny_pred_proba = predictions\noptimal_threshold_f1, best_score_f1 = config.find_optimal_threshold_cv(ytrue, y_pred_proba, f1_score)\n\nprint(f\"Optimal threshold: {optimal_threshold_f1}\")\n\n\nOptimal threshold: 0.15000000000000002\n\n\nCode\nprint(f\"Best score: {best_score_f1}\")\n\n\nBest score: 0.4788653077945807\n\n\nCode\n\n# Use the optimal threshold to make predictions\nfinal_predictions_f1 = (y_pred_proba &gt;= optimal_threshold_f1).astype(int)\n\n\nOptimal threshold f1 score: 0.15. Best score: 0.4788653.\n\n\n5.7.2.2 recall_score\n\n\nCode\nytrue = ytest.astype(int)\ny_pred_proba = predictions\noptimal_threshold_recall, best_score_recall = config.find_optimal_threshold_cv(ytrue, y_pred_proba, recall_score)\n\n# Use the optimal threshold to make predictions\nfinal_predictions_recall = (y_pred_proba &gt;= optimal_threshold_recall).astype(int)\n\n\nOptimal threshold recall: 0.05. Best score: 0.8095814.\n\n\n5.7.2.3 roc_auc_score\n\n\nCode\nytrue = ytest.astype(int)\ny_pred_proba = predictions\noptimal_threshold_roc, best_score_roc = config.find_optimal_threshold_cv(ytrue, y_pred_proba, roc_auc_score)\n\nprint(f\"Optimal threshold: {optimal_threshold_roc}\")\n\n\nOptimal threshold: 0.05\n\n\nCode\nprint(f\"Best score: {best_score_roc}\")\n\n\nBest score: 0.8809499649742268\n\n\nCode\n\n# Use the optimal threshold to make predictions\nfinal_predictions_roc = (y_pred_proba &gt;= optimal_threshold_roc).astype(int)\n\n\nOptimal threshold roc: 0.05. Best score: 0.88095.\n\n\n\n5.7.3 Confusion Matrix Plot\n\n\nCode\n# convert probability predictions to predictions\nypred = predictions &gt;=  optimal_threshold_recall # .05\nypred = ypred.astype(int)\n\n# create a plot with 3 by 2 subplots\nfig, axes = plt.subplots(3, 2, figsize=(15, 15))\naxes = axes.flatten()\nmcm = multilabel_confusion_matrix(ytrue, ypred)\n# plot the confusion matrices for each label\nfor i, (cm, label) in enumerate(zip(mcm, config.labels)):\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n    disp.plot(ax=axes[i], colorbar=False)\n    axes[i].set_title(f\"Confusion matrix for label: {label}\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 4: Multi Label Confusion matrix"
  },
  {
    "objectID": "posts/toxic_comment_filter/toxic_comment_filter.html#classification-report",
    "href": "posts/toxic_comment_filter/toxic_comment_filter.html#classification-report",
    "title": "Toxic Comment Filter",
    "section": "5.8 Classification Report",
    "text": "5.8 Classification Report\n\n\nCode\n\ncr = classification_report(\n  ytrue,\n  ypred,\n  target_names=config.labels,\n  digits=4,\n  output_dict=True\n  )\ndf_cr = pd.DataFrame.from_dict(cr).reset_index()\n\n\n\nCode\nlibrary(reticulate)\ndf_cr &lt;- py$df_cr %&gt;% dplyr::rename(names = index)\ncols &lt;- df_cr %&gt;% colnames()\ndf_cr %&gt;% \n  pivot_longer(\n    cols = -names,\n    names_to = \"metrics\",\n    values_to = \"values\"\n  ) %&gt;% \n  pivot_wider(\n    names_from = names,\n    values_from = values\n  )\n\n\n\n\nTable 6: Classification report\n\n\n\n# A tibble: 10 × 5\n   metrics       precision recall `f1-score` support\n   &lt;chr&gt;             &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;\n 1 toxic            0.552  0.890      0.682     2262\n 2 severe_toxic     0.236  0.917      0.375      240\n 3 obscene          0.550  0.936      0.692     1263\n 4 threat           0.0366 0.493      0.0681      69\n 5 insult           0.471  0.915      0.622     1170\n 6 identity_hate    0.116  0.720      0.200      207\n 7 micro avg        0.416  0.896      0.569     5211\n 8 macro avg        0.327  0.812      0.440     5211\n 9 weighted avg     0.495  0.896      0.629     5211\n10 samples avg      0.0502 0.0848     0.0597    5211"
  },
  {
    "objectID": "posts/toxic_comment_filter/history/f1score/toxic-comment-filter-bilstm.html",
    "href": "posts/toxic_comment_filter/history/f1score/toxic-comment-filter-bilstm.html",
    "title": "Simone Brazzi",
    "section": "",
    "text": "::: {#cell-0 .cell _uuid=‘8f2839f25d086af736a60e9eeb907d3b93b6e0e5’ _cell_guid=‘b1076dfc-b9ad-4769-8c92-a6c4dae69d19’ trusted=‘true’}\n\nCode\n# This Python 3 environment comes with many helpful analytics libraries installed# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python# For example, here's several helpful packages to loadimport numpy as np # linear algebraimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)# Input data files are available in the read-only \"../input/\" directory# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directoryimport osfor dirname, _, filenames in os.walk('/kaggle/input'):    for filename in filenames:        print(os.path.join(dirname, filename))# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n:::\n\n\nCode\nimport pandas as pdimport numpy as npimport matplotlib.pyplot as pltimport tensorflow as tfimport kerasimport keras_nlpfrom keras.backend import clear_sessionfrom keras.models import Model, load_modelfrom keras.layers import TextVectorization, Input, Dense, Embedding, Dropout, GlobalAveragePooling1D, LSTM, Bidirectional, GlobalMaxPool1D, Flatten, Attentionfrom keras.metrics import Precision, Recall, AUC, SensitivityAtSpecificity, SpecificityAtSensitivity, F1Scorefrom sklearn.model_selection import train_test_split, KFoldfrom sklearn.metrics import multilabel_confusion_matrix, classification_report, ConfusionMatrixDisplay, precision_recall_curve, f1_score, recall_score, roc_auc_scoreclass Config():    def __init__(self):        self.url = \"https://s3.eu-west-3.amazonaws.com/profession.ai/datasets/Filter_Toxic_Comments_dataset.csv\"        self.path = \"/Users/simonebrazzi/datasets/toxic_comment/Filter_Toxic_Comments_dataset.csv\"        self.max_tokens = 20000        self.output_sequence_length = 911 # check the analysis done to establish this value        self.embedding_dim = 128        self.batch_size = 32        self.epochs = 100        self.temp_split = 0.3        self.test_split = 0.5        self.random_state = 42        self.total_samples = 159571 # total train samples        self.train_samples = 111699        self.val_samples = 23936        self.features = 'comment_text'        self.labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']        self.new_labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate', \"clean\"]        self.label_mapping = {label: i for i, label in enumerate(self.labels)}        self.new_label_mapping = {label: i for i, label in enumerate(self.labels)}                self.model = \"model_f1.keras\"        self.checkpoint = \"checkpoint.lstm_model_f1.keras\"        self.history = \"lstm_model_f1.xlsx\"        self.matrix_file = \"confusion_matrices.png\"                self.metrics = [            Precision(name='precision'),            Recall(name='recall'),            AUC(name='auc', multi_label=True, num_labels=len(self.labels)),            F1Score(name=\"f1\", average=\"macro\")                    ]    def get_early_stopping(self):        early_stopping = keras.callbacks.EarlyStopping(            monitor=\"val_f1\", # \"val_recall\",            min_delta=0.2,            patience=10,            verbose=0,            mode=\"max\",            restore_best_weights=True,            start_from_epoch=3        )        return early_stopping    def get_model_checkpoint(self, filepath):        model_checkpoint = keras.callbacks.ModelCheckpoint(            filepath=filepath,            monitor=\"val_f1\", # \"val_recall\",            verbose=0,            save_best_only=True,            save_weights_only=False,            mode=\"max\",            save_freq=\"epoch\"        )        return model_checkpoint    def find_optimal_threshold_cv(self, ytrue, yproba, metric, thresholds=np.arange(.05, .35, .05), n_splits=7):      # instantiate KFold      kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)      threshold_scores = []      for threshold in thresholds:        cv_scores = []        for train_index, val_index in kf.split(ytrue):          ytrue_val = ytrue[val_index]          yproba_val = yproba[val_index]          ypred_val = (yproba_val &gt;= threshold).astype(int)          score = metric(ytrue_val, ypred_val, average=\"macro\")          cv_scores.append(score)        mean_score = np.mean(cv_scores)        threshold_scores.append((threshold, mean_score))        # Find the threshold with the highest mean score        best_threshold, best_score = max(threshold_scores, key=lambda x: x[1])      return best_threshold, best_scoreconfig = Config()\n\n\n\n\nCode\nfile = tf.keras.utils.get_file(\"Filter_Toxic_Comments_dataset.csv\", config.url)df = pd.read_csv(file)# df = pd.read_csv(config.path)df.loc[df.sum_injurious == 0, \"clean\"] = 1df.loc[df.sum_injurious != 0, \"clean\"] = 0x = df[config.features].valuesy = df[config.labels].valuesxtrain, xtemp, ytrain, ytemp = train_test_split(  x,  y,  test_size=config.temp_split, # .3  random_state=config.random_state  )xtest, xval, ytest, yval = train_test_split(  xtemp,  ytemp,  test_size=config.test_split, # .5  random_state=config.random_state  )print(    f\"df shape: {df.shape[0]}\\n\",  f\"xtrain shape: {xtrain.shape}\\n\",  f\"ytrain shape: {ytrain.shape}\\n\",  f\"xtest shape: {xtest.shape}\\n\",  f\"ytest shape: {ytest.shape}\\n\",  f\"xval shape: {xval.shape}\\n\",  f\"yval shape: {yval.shape}\\n\"  f\"labels: {config.labels}\\n\"  )text_vectorization = TextVectorization(  max_tokens=config.max_tokens,  standardize=\"lower_and_strip_punctuation\",  split=\"whitespace\",  output_mode=\"int\",  output_sequence_length=config.output_sequence_length,  pad_to_max_tokens=True  )train_ds = (    tf.data.Dataset    .from_tensor_slices((xtrain, ytrain))    .shuffle(xtrain.shape[0])    .batch(config.batch_size)    .prefetch(tf.data.experimental.AUTOTUNE))test_ds = (    tf.data.Dataset    .from_tensor_slices((xtest, ytest))    .batch(config.batch_size)    .prefetch(tf.data.experimental.AUTOTUNE))val_ds = (    tf.data.Dataset    .from_tensor_slices((xval, yval))    .batch(config.batch_size)    .prefetch(tf.data.experimental.AUTOTUNE))# Prepare a dataset that only yields raw text inputs (no labels)text_train_ds = train_ds.map(lambda x, y: x)# Adapt the text vectorization layer to the text data to index the dataset vocabularytext_vectorization.adapt(text_train_ds)processed_train_ds = train_ds.map(    lambda x, y: (text_vectorization(x), y),    num_parallel_calls=tf.data.experimental.AUTOTUNE)processed_val_ds = val_ds.map(    lambda x, y: (text_vectorization(x), y),    num_parallel_calls=tf.data.experimental.AUTOTUNE)processed_test_ds = test_ds.map(    lambda x, y: (text_vectorization(x), y),    num_parallel_calls=tf.data.experimental.AUTOTUNE)from keras.layers import LayerNormalization, MultiHeadAttention, GlobalAveragePooling1Ddef get_deeper_lstm_model():    clear_session()    inputs = Input(shape=(None,), dtype=tf.int64, name=\"inputs\")    embedding = Embedding(        input_dim=config.max_tokens,        output_dim=config.embedding_dim,        mask_zero=True,        name=\"embedding\"    )(inputs)    x = Bidirectional(LSTM(256, return_sequences=True, name=\"bilstm_1\"))(embedding)    x = Bidirectional(LSTM(128, return_sequences=True, name=\"bilstm_2\"))(x)    # Global average pooling    x = GlobalAveragePooling1D()(x)    # Add regularization    x = Dropout(0.3)(x)    x = Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)    x = LayerNormalization()(x)    outputs = Dense(len(config.labels), activation='sigmoid', name=\"outputs\")(x)    model = Model(inputs, outputs)    model.compile(optimizer='adam', loss=\"binary_crossentropy\", metrics=config.metrics, steps_per_execution=32)    return modelmodel = get_deeper_lstm_model()lab = pd.DataFrame(columns=config.labels, data=ytrain)lab.shaper = lab.sum() / len(ytrain)class_weight = dict(zip(range(len(config.labels)), r))class_weightsteps_per_epoch = config.train_samples // config.batch_sizevalidation_steps = config.val_samples // config.batch_size# callbacksmy_es = config.get_early_stopping()my_mc = config.get_model_checkpoint(filepath=\"/kaggle/working/\"  + config.checkpoint)callbacks = [my_es, my_mc]\n\n\nDownloading data from https://s3.eu-west-3.amazonaws.com/profession.ai/datasets/Filter_Toxic_Comments_dataset.csv\n66034407/66034407 ━━━━━━━━━━━━━━━━━━━━ 3s 0us/step\ndf shape: 159571\n xtrain shape: (111699,)\n ytrain shape: (111699, 6)\n xtest shape: (23936,)\n ytest shape: (23936, 6)\n xval shape: (23936,)\n yval shape: (23936, 6)\nlabels: ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n\n\n\n\n\nCode\nhistory_deeper_lstm_model = model.fit(    processed_train_ds.repeat(),    epochs=config.epochs,    validation_data=processed_val_ds.repeat(),    callbacks=callbacks,    class_weight=class_weight,    steps_per_epoch=steps_per_epoch,    validation_steps=validation_steps)\n\n\nEpoch 1/100\n3490/3490 ━━━━━━━━━━━━━━━━━━━━ 390s 98ms/step - auc: 0.8880 - f1: 0.0441 - loss: 0.0575 - precision: 0.7018 - recall: 0.4701 - val_auc: 0.9494 - val_f1: 0.0293 - val_loss: 0.0641 - val_precision: 0.8939 - val_recall: 0.4625\nEpoch 2/100\n3490/3490 ━━━━━━━━━━━━━━━━━━━━ 315s 90ms/step - auc: 0.9557 - f1: 0.0290 - loss: 0.0050 - precision: 0.7994 - recall: 0.6490 - val_auc: 0.9601 - val_f1: 0.0294 - val_loss: 0.0535 - val_precision: 0.7908 - val_recall: 0.6617\nEpoch 3/100\n3490/3490 ━━━━━━━━━━━━━━━━━━━━ 315s 90ms/step - auc: 0.9672 - f1: 0.0291 - loss: 0.0044 - precision: 0.8166 - recall: 0.6815 - val_auc: 0.9633 - val_f1: 0.0295 - val_loss: 0.0544 - val_precision: 0.7733 - val_recall: 0.6961\nEpoch 4/100\n3490/3490 ━━━━━━━━━━━━━━━━━━━━ 315s 90ms/step - auc: 0.9708 - f1: 0.0288 - loss: 0.0040 - precision: 0.8243 - recall: 0.7117 - val_auc: 0.9559 - val_f1: 0.0293 - val_loss: 0.0546 - val_precision: 0.7886 - val_recall: 0.6696\nEpoch 5/100\n3490/3490 ━━━━━━━━━━━━━━━━━━━━ 315s 90ms/step - auc: 0.9718 - f1: 0.0290 - loss: 0.0036 - precision: 0.8409 - recall: 0.7476 - val_auc: 0.9539 - val_f1: 0.0294 - val_loss: 0.0601 - val_precision: 0.7354 - val_recall: 0.6837\nEpoch 6/100\n3490/3490 ━━━━━━━━━━━━━━━━━━━━ 316s 90ms/step - auc: 0.9748 - f1: 0.0290 - loss: 0.0034 - precision: 0.8470 - recall: 0.7679 - val_auc: 0.9406 - val_f1: 0.0295 - val_loss: 0.0710 - val_precision: 0.7332 - val_recall: 0.6863\nEpoch 7/100\n3490/3490 ━━━━━━━━━━━━━━━━━━━━ 315s 90ms/step - auc: 0.9768 - f1: 0.0293 - loss: 0.0031 - precision: 0.8592 - recall: 0.7822 - val_auc: 0.9384 - val_f1: 0.0292 - val_loss: 0.0724 - val_precision: 0.7471 - val_recall: 0.6667\nEpoch 8/100\n3490/3490 ━━━━━━━━━━━━━━━━━━━━ 316s 90ms/step - auc: 0.9756 - f1: 0.0290 - loss: 0.0030 - precision: 0.8650 - recall: 0.7878 - val_auc: 0.9359 - val_f1: 0.0294 - val_loss: 0.0767 - val_precision: 0.7302 - val_recall: 0.6641\nEpoch 9/100\n3490/3490 ━━━━━━━━━━━━━━━━━━━━ 315s 90ms/step - auc: 0.9769 - f1: 0.0290 - loss: 0.0028 - precision: 0.8764 - recall: 0.8020 - val_auc: 0.9415 - val_f1: 0.0295 - val_loss: 0.0750 - val_precision: 0.7159 - val_recall: 0.6955\nEpoch 10/100\n3490/3490 ━━━━━━━━━━━━━━━━━━━━ 316s 90ms/step - auc: 0.9785 - f1: 0.0293 - loss: 0.0028 - precision: 0.8752 - recall: 0.8072 - val_auc: 0.9263 - val_f1: 0.0295 - val_loss: 0.0795 - val_precision: 0.7649 - val_recall: 0.6282\nEpoch 11/100\n3490/3490 ━━━━━━━━━━━━━━━━━━━━ 315s 90ms/step - auc: 0.9775 - f1: 0.0291 - loss: 0.0027 - precision: 0.8837 - recall: 0.8062 - val_auc: 0.9360 - val_f1: 0.0296 - val_loss: 0.0721 - val_precision: 0.7470 - val_recall: 0.6254\nEpoch 12/100\n3490/3490 ━━━━━━━━━━━━━━━━━━━━ 313s 89ms/step - auc: 0.9791 - f1: 0.0297 - loss: 0.0027 - precision: 0.8825 - recall: 0.8113 - val_auc: 0.9368 - val_f1: 0.0296 - val_loss: 0.0776 - val_precision: 0.7307 - val_recall: 0.6644\nEpoch 13/100\n3490/3490 ━━━━━━━━━━━━━━━━━━━━ 316s 90ms/step - auc: 0.9788 - f1: 0.0293 - loss: 0.0026 - precision: 0.8795 - recall: 0.8193 - val_auc: 0.9370 - val_f1: 0.0295 - val_loss: 0.0743 - val_precision: 0.7382 - val_recall: 0.6413\nEpoch 14/100\n3490/3490 ━━━━━━━━━━━━━━━━━━━━ 314s 89ms/step - auc: 0.9781 - f1: 0.0295 - loss: 0.0025 - precision: 0.8892 - recall: 0.8181 - val_auc: 0.9309 - val_f1: 0.0295 - val_loss: 0.0855 - val_precision: 0.7386 - val_recall: 0.6492\n\n\n\n\nCode\nmodel.save(\"/kaggle/working/\" + config.model)pd.DataFrame.from_dict(history_deeper_lstm_model.history).to_excel(\"/kaggle/working/\" + config.history)\n\n\n\n\nCode\nimport kerasfrom keras.models import load_modelmodel = load_model(filepath=\"/kaggle/working/\" + config.model)history = pd.read_excel(\"/kaggle/working/\" + config.history)\n\n\n\n\nCode\nconfig.model, config.history\n\n\n('model_f1.keras', 'lstm_model_f1.xlsx')\n\n\n\n\nCode\nvalidation = model.evaluate(  processed_val_ds.repeat(),  steps=validation_steps, # 748    verbose=1  )\n\n\n748/748 ━━━━━━━━━━━━━━━━━━━━ 87s 38ms/step - auc: 0.9542 - f1: 0.0287 - loss: 0.0529 - precision: 0.7858 - recall: 0.6763\n\n\n\n\nCode\npredictions = model.predict(processed_test_ds, verbose=1)\n\n\n748/748 ━━━━━━━━━━━━━━━━━━━━ 45s 41ms/step\n\n\n\n\nCode\nytrue = ytest.astype(int)y_pred_proba = predictionsoptimal_threshold_recall, best_score_recall = config.find_optimal_threshold_cv(ytrue, y_pred_proba, recall_score)# Use the optimal threshold to make predictionsfinal_predictions_recall = (y_pred_proba &gt;= optimal_threshold_recall).astype(int)\n\n\n\n\nCode\n# convert probability predictions to predictionsypred = predictions &gt;=  optimal_threshold_recall # .05ypred = ypred.astype(int)# create a plot with 3 by 2 subplotsfig, axes = plt.subplots(3, 2, figsize=(15, 15))axes = axes.flatten()mcm = multilabel_confusion_matrix(ytrue, ypred)# plot the confusion matrices for each labelfor i, (cm, label) in enumerate(zip(mcm, config.labels)):    disp = ConfusionMatrixDisplay(confusion_matrix=cm)    disp.plot(ax=axes[i], colorbar=False)    axes[i].set_title(f\"Confusion matrix for label: {label}\")plt.tight_layout()plt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\ncr = classification_report(  ytrue,  ypred,  target_names=config.labels,  digits=4  )print(cr)\n\n\n               precision    recall  f1-score   support\n\n        toxic     0.5524    0.8899    0.6817      2262\n severe_toxic     0.2355    0.9167    0.3748       240\n      obscene     0.5495    0.9359    0.6924      1263\n       threat     0.0366    0.4928    0.0681        69\n       insult     0.4712    0.9145    0.6219      1170\nidentity_hate     0.1164    0.7198    0.2004       207\n\n    micro avg     0.4164    0.8958    0.5685      5211\n    macro avg     0.3269    0.8116    0.4399      5211\n weighted avg     0.4947    0.8958    0.6295      5211\n  samples avg     0.0502    0.0848    0.0597      5211\n\n\n\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n\n\n\n\nCode"
  }
]