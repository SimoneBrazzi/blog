[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Nice to meet you! My name is Simone Brazzi. I am 33 years old guy from Bologna, Italy.\nI decided to open this blog as a personal journal (what a new idea! Who ever thought about this?) of my data journey. This also means it is an opportunity to display my portfolio and shares my projects.\nAt the date of writing, I am a self made data professional. I am currently working as a Data Analyst/Engineer for UniCredit Group.\nI have too many hobbies, but I will try to summarize them in a sparse order:\n\nTraining: my 2024 goal is to be able to run at least an half marathon.\nStudy: I am studying data science to further improve my knowledge in the data field. I would like to do a Master Degree in the field, but it is difficult to combine the private life with an international MSc, at least for now.\nGaming, movies and manga: of course! I mean, it is part of being a nerd, right?! For cliche purpose, I love Quentin Tarantino blabbering, Nolan craziness, even though my favorite genre is horror. My favorites directors are Jordan Pelee and Ari Aster. I am currently reading Berserker, My Hero Academia, Jujutsu Kaisen, One Piece, Chainsaw Man and Kagurabachi.\nCooking, I think it is in my vein being italian (joking!) and considering my mother is a chef.\n\nFor now, I think I have annoyed you enough: stay tuned for future posts!"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Toxic Comment Filter\n\n\n\n\n\n\ncode\n\n\nDeep Learning\n\n\nPython, R\n\n\n\nBiLSTM model to make a multi label classification for a toxic comment filter\n\n\n\n\n\nAug 2, 2024\n\n\nSimone Brazzi\n\n\n\n\n\n\n\n\n\n\n\n\nEurostat Homicide Data\n\n\n\n\n\n\ncode\n\n\nshiny\n\n\nR\n\n\n\nA primer on Shiny to analyze gender differences in homicide rates\n\n\n\n\n\nDec 29, 2023\n\n\nSimone Brazzi\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/eurostat_homicide_rate/eurostat_homicide_rate.html",
    "href": "posts/eurostat_homicide_rate/eurostat_homicide_rate.html",
    "title": "Eurostat Homicide Data",
    "section": "",
    "text": "Hi there and welcome to my first project for the blog! The topic is a sad one, but I would like to explain why I decided to start with this. I was trying Shiny for different dashboards, but I wasn’t satisfied to learn using the classic examples. Unfortunately, italian crime news suffocated the public debate with a case of homicide, in which the victim is a young women. The public debate was focusing so baaaaaadly on the concept of femicide and the data, that I decided to clear the situation with a simple dashboard.\nFirst of all, at this link you can find the dashboard published using shinyapps.io. Also at this link you can find the Github repo. As you can see, even if it is the main branch, there are some details which are not uber perfect, but that don’t interfere with the code.\nNow lets jump into the detail of how to create a Shiny dashboard!"
  },
  {
    "objectID": "posts/eurostat_homicide_rate/eurostat_homicide_rate.html#global.r",
    "href": "posts/eurostat_homicide_rate/eurostat_homicide_rate.html#global.r",
    "title": "Eurostat Homicide Data",
    "section": "global.R",
    "text": "global.R\nAs said, this file is our usual R Script file. First thing first, we import our libraries:\n\nCode# wrangling\nlibrary(\"tidyverse\")\nlibrary(\"readr\")\nlibrary(\"stringr\")\nlibrary(\"dplyr\")\nlibrary(\"magrittr\")\nlibrary(\"forcats\")\nlibrary(\"lubridate\")\nlibrary(\"writexl\")\nlibrary(\"eurostat\")\n# plotting and dashboarding\nlibrary(\"shiny\")\nlibrary(\"shinythemes\")\nlibrary(\"ggplot2\")\nlibrary(\"plotly\")\nlibrary(\"scales\")\nlibrary(\"RColorBrewer\")\nlibrary(\"waiter\")\n# connecting and other\nlibrary(\"rsconnect\")\nlibrary(\"markdown\")\n\n\nLots of packages! The division is merely to remember how everything is managed and because I have OCD for this type of things.\nI want to focus on some packages:\n\ntidyverse, we all know it. As you can see, I also imported lots of single packages which compose the tidyverse, because I was getting errors of missing methods.\neurostat, which lets data flows from the eurostat website to my dashboard. This also lets the dashboard automatically updates when new data is available.\nscales, to nicely scaling my x and y axis.\nRColorBrewer, because I wanted to have a colorblind safe dashboard, even tough I am not.\nwaiter, for nice waiting images while the dashboard is loading.\n\nNow we can focus on the data importing and wrangling. For this, the eurostat library does the job. Lets focus on the crim_hom_vrel dataset.\n\nCode# search in eurostat db\nhomicide &lt;- search_eurostat(\"homicide\")\n\n# import data to variable\ncrim_hom_vrel &lt;- get_eurostat(\"crim_hom_vrel\", time_format = \"date\")\n\n# convert all observations to understandable data\ncrim_hom_vrel &lt;- label_eurostat(crim_hom_vrel)\n\n# label_eurostat_vars(crim_hom_vrel)\n\n# order data by country and date for time series purpose\ncrim_hom_vrel &lt;- crim_hom_vrel %&gt;% \n  arrange(geo, time)\n\ncrim_hom_vrel_grouped &lt;- crim_hom_vrel %&gt;% \n  dplyr::group_by(geo, time, sex, pers_cat, unit) %&gt;% \n  dplyr::summarise(values_grouped = sum(values), .groups = \"drop\") %&gt;% \n  filter(unit == \"Number\") %&gt;% \n  arrange(geo, time, sex)\n\n\nEverything pretty simple. I would like to highlight something about the dplyr::group_by and dplyr::summarise. As you can see, after having grouped and summarized, I need to drop the groups with the method .groups = \"drop\". With dplyr v.1.1.0 we can do the same with the help of the .by method in summarise.\n\nCodecrim_hom_vrel_grouped &lt;- crim_hom_vrel %&gt;% \n  dplyr::summarise(\n    values_grouped = sum(values),\n    .by = c(geo, time, sex, pers_cat, unit)\n    ) %&gt;% \n  filter(unit == \"Number\") %&gt;% \n  arrange(geo, time, sex)\n\n\nCopying from the dplyr website the differences between .by and group_by() are:\n\n\n.by\ngroup_by()\n\n\n\nGrouping only affects a single verb\nGrouping is persistent across multiple verbs\n\n\nSelects variables with tidy-select\n\nComputes expressions with data-masking\n\n\n\nSummaries use existing order of group keys\nSummaries sort group keys in ascending order\n\n\n\nLast part is all about colors.\n\nCode# brewer.pal(11, \"RdYlBu\")\npalette &lt;- c(\"#A50026\", \"#D73027\", \"#F46D43\", \"#FDAE61\", \"#FEE090\", \"#FFFFBF\", \"#E0F3F8\", \"#ABD9E9\", \"#74ADD1\", \"#4575B4\", \"#313695\")\n\npalette_crim_hom_vrel_grouped &lt;- rep(\n  palette,\n  length.out = crim_hom_vrel_grouped$geo %&gt;% str_unique() %&gt;% length()\n  )\n\n\nHere I defined the palette using ColorBrewer. Using rep I replicated the 11 colours for the length of the unique geo values."
  },
  {
    "objectID": "posts/eurostat_homicide_rate/eurostat_homicide_rate.html#ui.r",
    "href": "posts/eurostat_homicide_rate/eurostat_homicide_rate.html#ui.r",
    "title": "Eurostat Homicide Data",
    "section": "ui.R",
    "text": "ui.R"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Simone Brazzi",
    "section": "",
    "text": "I am a (data) nerd, with lot of passion and some mistake along the way. Data analyst by day, aspiring data wizard by night! I love using data to tell stories and drive business decisions. But I’m not content with stopping there. My passion for data and desire to expand my skillset has led me on a quest to become a data scientist. I’m a fearless problem solver with an insatiable curiosity, and I’m always seeking new challenges and opportunities to learn and grow. Let’s make some magic with data! When I don’t stare tolines of code or spreadsheet, I like to read and play video game or spend quality time with my dog."
  },
  {
    "objectID": "posts/toxic_comment_filter/toxic_comment_filter.html",
    "href": "posts/toxic_comment_filter/toxic_comment_filter.html",
    "title": "Toxic Comment Filter",
    "section": "",
    "text": "Costruire un modello in grado di filtrare i commenti degli utenti in base al grado di dannosità del linguaggio.\nPreprocessare il testo eliminando l’insieme di token che non danno contributo significativo a livello semantico.\nTrasformare il corpus testuale in sequenze.\nCostruire un modello di Deep Learning comprendente dei layer ricorrenti per un task di classificazione multilabel.\n\nIn prediction time, il modello deve ritornare un vettore contenente un 1 o uno 0 in corrispondenza di ogni label presente nel dataset (toxic, severe_toxic, obscene, threat, insult, identity_hate). In questo modo, un commento non dannoso sarà classificato da un vettore di soli 0 [0,0,0,0,0,0]. Al contrario, un commento pericoloso presenterà almeno un 1 tra le 6 labels."
  },
  {
    "objectID": "posts/toxic_comment_filter/toxic_comment_filter.html#import-r-libraries",
    "href": "posts/toxic_comment_filter/toxic_comment_filter.html#import-r-libraries",
    "title": "Toxic Comment Filter",
    "section": "Import R libraries",
    "text": "Import R libraries\nImport R libraries. These will be used for both the rendering of the document and data analysis. The reason is I prefer ggplot2 over matplotlib. I will also use colorblind safe palettes.\n\n\nCode\nlibrary(tidyverse, verbose = FALSE)\nlibrary(tidymodels, verbose = FALSE)\nlibrary(reticulate)\nlibrary(ggplot2)\nlibrary(plotly)\nlibrary(RColorBrewer)\nlibrary(bslib)\nlibrary(Metrics)\n\nreticulate::use_virtualenv(\"r-tf\")"
  },
  {
    "objectID": "posts/toxic_comment_filter/toxic_comment_filter.html#class-config",
    "href": "posts/toxic_comment_filter/toxic_comment_filter.html#class-config",
    "title": "Toxic Comment Filter",
    "section": "Class Config",
    "text": "Class Config\nI created a class with all the basic configuration of the model, to improve the readability.\n\n\nCode\nclass Config():\n    def __init__(self):\n        self.url = \"https://s3.eu-west-3.amazonaws.com/profession.ai/datasets/Filter_Toxic_Comments_dataset.csv\"\n        self.max_tokens = 20000\n        self.output_sequence_length = 911 # check the analysis done to establish this value\n        self.embedding_dim = 128\n        self.batch_size = 32\n        self.epochs = 100\n        self.temp_split = 0.3\n        self.test_split = 0.5\n        self.random_state = 42\n        self.total_samples = 159571 # total train samples\n        self.train_samples = 111699\n        self.val_samples = 23936\n        self.features = 'comment_text'\n        self.labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n        self.new_labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate', \"clean\"]\n        self.label_mapping = {label: i for i, label in enumerate(self.labels)}\n        self.new_label_mapping = {label: i for i, label in enumerate(self.labels)}\n        self.path = \"/Users/simonebrazzi/R/blog/posts/toxic_comment_filter/history/f1score/\"\n        self.model =  self.path + \"model_f1.keras\"\n        self.checkpoint = self.path + \"checkpoint.lstm_model_f1.keras\"\n        self.history = self.path + \"lstm_model_f1.xlsx\"\n        \n        self.metrics = [\n            Precision(name='precision'),\n            Recall(name='recall'),\n            AUC(name='auc', multi_label=True, num_labels=len(self.labels)),\n            F1Score(name=\"f1\", average=\"macro\")\n            \n        ]\n    def get_early_stopping(self):\n        early_stopping = keras.callbacks.EarlyStopping(\n            monitor=\"val_f1\", # \"val_recall\",\n            min_delta=0.2,\n            patience=10,\n            verbose=0,\n            mode=\"max\",\n            restore_best_weights=True,\n            start_from_epoch=3\n        )\n        return early_stopping\n\n    def get_model_checkpoint(self, filepath):\n        model_checkpoint = keras.callbacks.ModelCheckpoint(\n            filepath=filepath,\n            monitor=\"val_f1\", # \"val_recall\",\n            verbose=0,\n            save_best_only=True,\n            save_weights_only=False,\n            mode=\"max\",\n            save_freq=\"epoch\"\n        )\n        return model_checkpoint\n\n    def find_optimal_threshold_cv(self, ytrue, yproba, metric, thresholds=np.arange(.05, .35, .05), n_splits=7):\n\n      # instantiate KFold\n      kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n      threshold_scores = []\n\n      for threshold in thresholds:\n\n        cv_scores = []\n        for train_index, val_index in kf.split(ytrue):\n\n          ytrue_val = ytrue[val_index]\n          yproba_val = yproba[val_index]\n\n          ypred_val = (yproba_val &gt;= threshold).astype(int)\n          score = metric(ytrue_val, ypred_val, average=\"macro\")\n          cv_scores.append(score)\n\n        mean_score = np.mean(cv_scores)\n        threshold_scores.append((threshold, mean_score))\n\n        # Find the threshold with the highest mean score\n        best_threshold, best_score = max(threshold_scores, key=lambda x: x[1])\n      return best_threshold, best_score\n\nconfig = Config()"
  },
  {
    "objectID": "posts/toxic_comment_filter/toxic_comment_filter.html#import-python-packages",
    "href": "posts/toxic_comment_filter/toxic_comment_filter.html#import-python-packages",
    "title": "Toxic Comment Filter",
    "section": "Import Python packages",
    "text": "Import Python packages\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport keras\nimport keras_nlp\n\nfrom keras.backend import clear_session\nfrom keras.models import Model, load_model\nfrom keras.layers import TextVectorization, Input, Dense, Embedding, Dropout, GlobalAveragePooling1D, LSTM, Bidirectional, GlobalMaxPool1D, Flatten, Attention\nfrom keras.metrics import Precision, Recall, AUC, SensitivityAtSpecificity, SpecificityAtSensitivity, F1Score\n\n\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.metrics import multilabel_confusion_matrix, classification_report, ConfusionMatrixDisplay, precision_recall_curve, f1_score, recall_score, roc_auc_score\n\n\nCreate a Config class to store all the useful parameters for the model and for the project."
  },
  {
    "objectID": "posts/toxic_comment_filter/toxic_comment_filter.html#eda",
    "href": "posts/toxic_comment_filter/toxic_comment_filter.html#eda",
    "title": "Toxic Comment Filter",
    "section": "EDA",
    "text": "EDA\nFirst a check on the dataset to find possible missing values and imbalances.\n\nFrequency\n\nCode\nlibrary(reticulate)\ndf_r &lt;- py$df\nnew_labels_r &lt;- py$config$new_labels\n\ndf_r_grouped &lt;- df_r %&gt;% \n  select(all_of(new_labels_r)) %&gt;%\n  pivot_longer(\n    cols = all_of(new_labels_r),\n    names_to = \"label\",\n    values_to = \"value\"\n  ) %&gt;% \n  group_by(label) %&gt;%\n  summarise(count = sum(value)) %&gt;% \n  mutate(freq = round(count / sum(count), 4))\n\ndf_r_grouped\n\n\n\n\nTable 2: Absolute and relative labels frequency\n\n\n\n# A tibble: 7 × 3\n  label          count   freq\n  &lt;chr&gt;          &lt;dbl&gt;  &lt;dbl&gt;\n1 clean         143346 0.803 \n2 identity_hate   1405 0.0079\n3 insult          7877 0.0441\n4 obscene         8449 0.0473\n5 severe_toxic    1595 0.0089\n6 threat           478 0.0027\n7 toxic          15294 0.0857\n\n\n\n\n\n\nBarchart\n\n\nCode\nlibrary(reticulate)\nbarchart &lt;- df_r_grouped %&gt;%\n  ggplot(aes(x = reorder(label, count), y = count, fill = label)) +\n  geom_col() +\n  labs(\n    x = \"Labels\",\n    y = \"Count\"\n  ) +\n  # sort bars in descending order\n  scale_x_discrete(limits = df_r_grouped$label[order(df_r_grouped$count, decreasing = TRUE)]) +\n  scale_fill_brewer(type = \"seq\", palette = \"RdYlBu\")\nggplotly(barchart)\n\n\n\n\n\n\n\n\nFigure 1: Imbalance in the dataset with clean variable\n\n\n\n\nIt is visible how much the dataset in imbalanced. This means it could be useful to check for the class weight and use this argument during the training.\nIt is clear that most of our text are clean. We are talking about 0.8033 of the observations which are clean. Only 0.1967 are toxic comments."
  },
  {
    "objectID": "posts/toxic_comment_filter/toxic_comment_filter.html#sequence-lenght-definition",
    "href": "posts/toxic_comment_filter/toxic_comment_filter.html#sequence-lenght-definition",
    "title": "Toxic Comment Filter",
    "section": "Sequence lenght definition",
    "text": "Sequence lenght definition\nTo convert the text in a useful input for a NN, it is necessary to use a TextVectorization layer. See the Section 4 section.\nOne of the method is output_sequence_length: to better define it, it is useful to analyze our text length. To simulate what the model we do, we are going to remove the punctuation and the new lines from the comments.\n\nSummary\n\nCode\nlibrary(reticulate)\ndf_r %&gt;% \n  mutate(\n    comment_text_clean = comment_text %&gt;%\n      tolower() %&gt;% \n      str_remove_all(\"[[:punct:]]\") %&gt;% \n      str_replace_all(\"\\n\", \" \"),\n    text_length = comment_text_clean %&gt;% str_count()\n    ) %&gt;% \n  pull(text_length) %&gt;% \n  summary() %&gt;% \n  as.list() %&gt;% \n  as_tibble()\n\n\n\n\nTable 3: Summary of text length\n\n\n\n# A tibble: 1 × 6\n   Min. `1st Qu.` Median  Mean `3rd Qu.`  Max.\n  &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1     4        91    196  378.       419  5000\n\n\n\n\n\n\nBoxplot\n\n\nCode\nlibrary(reticulate)\nboxplot &lt;- df_r %&gt;% \n  mutate(\n    comment_text_clean = comment_text %&gt;%\n      tolower() %&gt;% \n      str_remove_all(\"[[:punct:]]\") %&gt;% \n      str_replace_all(\"\\n\", \" \"),\n    text_length = comment_text_clean %&gt;% str_count()\n    ) %&gt;% \n  # pull(text_length) %&gt;% \n  ggplot(aes(y = text_length)) +\n  geom_boxplot() +\n  theme_minimal()\nggplotly(boxplot)\n\n\n\n\n\n\n\n\nFigure 2: Text length boxplot\n\n\n\n\n\n\nHistogram\n\n\nCode\nlibrary(reticulate)\ndf_ &lt;- df_r %&gt;% \n  mutate(\n    comment_text_clean = comment_text %&gt;%\n      tolower() %&gt;% \n      str_remove_all(\"[[:punct:]]\") %&gt;% \n      str_replace_all(\"\\n\", \" \"),\n    text_length = comment_text_clean %&gt;% str_count()\n  )\n\nQ1 &lt;- quantile(df_$text_length, 0.25)\nQ3 &lt;- quantile(df_$text_length, 0.75)\nIQR &lt;- Q3 - Q1\nupper_fence &lt;- as.integer(Q3 + 1.5 * IQR)\n\nhistogram &lt;- df_ %&gt;% \n  ggplot(aes(x = text_length)) +\n  geom_histogram(bins = 50) +\n  geom_vline(aes(xintercept = upper_fence), color = \"red\", linetype = \"dashed\", linewidth = 1) +\n  theme_minimal() +\n  xlab(\"Text Length\") +\n  ylab(\"Frequency\") +\n  xlim(0, max(df_$text_length, upper_fence))\nggplotly(histogram)\n\n\n\n\n\n\n\n\nFigure 3: Text length histogram with boxplot upper fence\n\n\n\n\nConsidering all the above analysis, I think a good starting value for the output_sequence_length is 911, the upper fence of the boxplot. In the last plot, it is the dashed red vertical line.. Doing so, we are removing the outliers, which are a small part of our dataset."
  },
  {
    "objectID": "posts/toxic_comment_filter/toxic_comment_filter.html#split-dataset",
    "href": "posts/toxic_comment_filter/toxic_comment_filter.html#split-dataset",
    "title": "Toxic Comment Filter",
    "section": "Split Dataset",
    "text": "Split Dataset\nNow we can split the dataset in 3: train, test and validation sets. Considering there is not a function in sklearn which lets split in these 3 sets, we can do the following: - split between a train and temporary set with a 0.3 split. - split the temporary set in 2 equal sized test and val sets.\n\n\nCode\nx = df[config.features].values\ny = df[config.labels].values\n\nxtrain, xtemp, ytrain, ytemp = train_test_split(\n  x,\n  y,\n  test_size=config.temp_split, # .3\n  random_state=config.random_state\n  )\nxtest, xval, ytest, yval = train_test_split(\n  xtemp,\n  ytemp,\n  test_size=config.test_split, # .5\n  random_state=config.random_state\n  )\n\n\nxtrain shape: py$xtrain.shape ytrain shape: py$ytrain.shape xtest shape: py$xtest.shape ytest shape: py$ytest.shape xval shape: py$xval.shape yval shape: py$yval.shape\nThe datasets are created using the tf.data.Dataset function. It creates a data input pipeline. The tf.data API makes it possible to handle large amounts of data, read from different data formats, and perform complex transformations. The tf.data.Dataset is an abstraction that represents a sequence of elements, in which each element consists of one or more components. Here each dataset is creates using from_tensor_slices. It create a tf.data.Dataset from a tuple (features, labels). .batch let us work in batches to improve performance, while .prefetch overlaps the preprocessing and model execution of a training step. While the model is executing training step s, the input pipeline is reading the data for step s+1. Check the documentation for further informations.\n\n\nCode\ntrain_ds = (\n    tf.data.Dataset\n    .from_tensor_slices((xtrain, ytrain))\n    .shuffle(xtrain.shape[0])\n    .batch(config.batch_size)\n    .prefetch(tf.data.experimental.AUTOTUNE)\n)\n\ntest_ds = (\n    tf.data.Dataset\n    .from_tensor_slices((xtest, ytest))\n    .batch(config.batch_size)\n    .prefetch(tf.data.experimental.AUTOTUNE)\n)\n\nval_ds = (\n    tf.data.Dataset\n    .from_tensor_slices((xval, yval))\n    .batch(config.batch_size)\n    .prefetch(tf.data.experimental.AUTOTUNE)\n)\n\n\n\n\nCode\nprint(\n  f\"train_ds cardinality: {train_ds.cardinality()}\\n\",\n  f\"val_ds cardinality: {val_ds.cardinality()}\\n\",\n  f\"test_ds cardinality: {test_ds.cardinality()}\\n\"\n  )\n\n\ntrain_ds cardinality: 3491\n val_ds cardinality: 748\n test_ds cardinality: 748\n\n\nCheck the first element of the dataset to be sure that the preprocessing is done correctly.\n\n\nCode\ntrain_ds.as_numpy_iterator().next()\n\n\n(array([b\"You see, they are colorcoded. Pink, for example, are animals and plants.  I din't know till now.\",\n       b\"Discussion is finished \\n\\nI'm done discussing the matter at hand. I am also done discussing things with the other user. I've been threatened by you twice about receiving sanctions, and yet another user can beat me down within the guidelines of personal attacks and nothing happens to them.  It's bad enough the other user puts high levels of stress and anxiety on me, but now the mediators are doing the same, by taking sides. This whole situation is very discouraging and disheartening!\",\n       b'\"The notion that people are actively trying to suppress this story from gaining national attention is absurd.  Sad as it may be, the story simply got filed to the later pages of the papers and small blurbs because other issues seemed more... \"\"newsworthy.\"\"\"',\n       b'. I will not engage in battleground attitude, personal attacks, long-term tendentious editing. I am willing to accept content and subject blocks as a condition that my block be lifted.',\n       b\"On reflection, you're right.\",\n       b\"Hold on a second here.  The link to e-ariana.com is not even original content.  This is the heading of the source article reads:\\n\\n International Herald Tribune\\n07/23/2007\\nBy Barry Bearak \\n\\nIf I'm not mistaken, that makes your makes your objection to the source void.  In addition, this Zahir Shah article itself cites a Barry Bearak article published in the New York Times.\",\n       b'\"\\nLet\\'s stop and think about how the Jimbo gif thing has gone since it started and maybe those of you who are voicing that I\\'m \"\"stubborn\"\" could rethink that as an unfair characterization: a admin comes here, tells me that he/she was going to review my block but now won\\'t because the Jimbo animation makes him/her nauseated and that I will never be taken seriously because it\\'s there.  Then, the same admin states after my reply that it\\'s essentially my fault that he/she didn\\'t review the block.  Think about how *you* would react to an attitude like that.  Now, consider the statements made by those who are obviously better at communicating what they really mean: \"\"it\\'s hard for me to read the page with it there\"\".  If that had been said in the first place, how would you have reacted to that instead?  I know that if that had been stated to me initially, I would have understood - because it\\'s a reasonable statement and observation, and I don\\'t want people to have problems reading the page - and had no problem removing the Jimbo gif from my page.  Snarkily saying that the Jimbo gif makes one either annoyed or \"\"nauseated\"\" and will keep others from taking me seriously, is neither helpful nor clear as to the real issue, is it?    \"',\n       b'(a) Because they reflect the last known positions of editors based on their actual participation in a related discussion, and (b) so that the scorecard reflects this historical debate.  You would prefer that I ignore the previous discussion on this topic?  Remember, WP:CCC but it seems appropriate to use as a starting point the previous level of consensus?',\n       b'\"\\n\\nHi Pterantula,\\n\\nThe encyclopedia entry requires attributions and references.  For example, the studies of the \"\"effects of Landmark Education\"\" is from the third-party surveys.  If, instead, the encyclopedia entry were to have customer testimonials instead, they could be struck on grounds they are not verifiable.  (We\\'ve gone around in circles on that many times as well.)\\n\\nAs well, the 20 points of critique are all against Landmark Education (with rebuttals in many cases).\\n\\nAs for litigation, I am aware of five outbound cases:\\n\\nPre-emptive warning\\n\\n1) Traci Hukill \"\"The Est of Friends\"\" (1998, pre-emptive warning, no suit filed)\\n\\nActual litigation, outbound\\n\\n Elle Magazine   (1998, LE achieved no result)\\n Cynthia Kisser  (1997, LE won retraction)\\n Margaret Singer (1997, LE won retraction)\\n Rick Ross       (2005, LE cancelled suit for product disparagement)\\n\\nThere are about three inbound cases (against LE):\\n Stefanie Ney (court ruled in LE\\'s favor)\\n Been versus Weed (2002; 10 claims, 3 settled, 7 dropped;  LE was a cross-defendent)\\n Tracy Neff (1997, sexual harrassment, settled out of court;  LE lacked a \"\"sexual harrassment\"\" policy at the time and got pulled in on this)\\n\\nDoes anyone have anything more than this from LE\\'s inception in 1991 (14 years ago)?  Litigation regarding \"\"est\"\" should be on the \"\"est\"\" page.   For a company with $70 million in revenue and over 800,000 customers of the Landmark Forum, this is a tiny amount of litigation.\\n\\n \"',\n       b'\"\\n\\n Challenge/Response and Backscatter \\n\\nOne anti-spam proposal has been the challenge/response type of system, which sends challenge messages to unknown senders of incoming mail and awaits a response before delivering a message.  Such systems are generally declared as failures and backscatter is the reason.  Concerning a legitimate message, the challenge would typically be sent to the sender.  However, for spam, this is often a forged address, so such a challenge \"\"raises the noise floor\"\" for its innocent recipient.  Although such a recipient will fail to reply, thus causing the originating spam to be discarded, one is effectively placing the decision into the hands of an unknown third party, who is annoyed by the unsolicited challenge message.\\n\\nFurthermore, for a challenge to be useful, it must somehow refer to the original message sent, and even include a (partial) copy of such.  When a C/R system does so, effectively it becomes a spam relay.  A spammer, knowing that a certain C/R system includes a copy, will design his message so that the spam payload is among the copied portion, and forge his desired recipient\\'s address as if he were the sender, thus allowing the C/R system to be the actual delivering server of the spam (as a challenge message payload) to his desired target.\\n\\nThe definition of backscatter as used by some individuals very well includes such challenges from C/R systems, especially those which can be tricked by spammers to deliver their message payloads.  Such challenges are not technically \"\"bounce messages\"\" as they are not NDRs but otherwise satisfy all the other common requirements to be considered backscatter (if not spam in its own right).\\n\\n- 71.106.213.194  \"',\n       b\"Ravana Sri Lanka and Sinahalese , neutrality \\nHe was a Sinhalese. Sri Lankan and  North Indian (Hindi), who wrote orginal Ramayana believes Ravan was Sinhalese ( The Hindu reference). \\nAs per Sri Lankan myths Ravana is very powerful , Raksha king lived in Sri Lanka who had two children named meganand and other but not mentioned any of Sita or Indian invasion. This page is needed to rewritten in neutral manner. Some Sri Lankan myths says Ravana was very powerful capable ruler and others couldn't fight with Ravana so they wrote a book in which they made him as a antagonist and reverted his real world victories by writing a novel. \\nI doesn't mean to remove any of myths written against him. But to mention Ravana's cruelty, raping women stories are taken from the hindu myth story Ramayanaya. Otherwise stating them as real facts will harm the image of a famous Sri Lankan hero. I will put neutrality tag for heavily based on Ramayanaya.\",\n       b'\":::::::::::Relevant policies: Failure or refusal to \"\"get the point\"\" and competence is required. Go argue with US Library of Congress Country Profile source as to why they have not included \"\"Cimerians, Persians, Assyrians, Akkadians, Phrygians (who were not Thracians), etc, etc.\"\" It\\'s also funny you want Persians mentioned in the lead since their invasion was like a blink of an eye, given the scope of history.   \\n\\n\"',\n       b'\"\\nI find it hard to believe that you didn\\'t know you write a verbatim copy of the website, but the content was \\n{{Unreferenced|date=December 2012}}\\n{{Infobox university\\n|name           = Darbhanga Medical College and Hospital \\n|native_name    = \\xe0\\xa4\\xa6\\xe0\\xa4\\xb0\\xe0\\xa4\\xad\\xe0\\xa4\\x82\\xe0\\xa4\\x97\\xe0\\xa4\\xbe \\xe0\\xa4\\xae\\xe0\\xa5\\x87\\xe0\\xa4\\xa1\\xe0\\xa4\\xbf\\xe0\\xa4\\x95\\xe0\\xa4\\xb2 \\xe0\\xa4\\x95\\xe0\\xa5\\x89\\xe0\\xa4\\xb2\\xe0\\xa5\\x87\\xe0\\xa4\\x9c \\xe0\\xa4\\x94\\xe0\\xa4\\xb0 \\xe0\\xa4\\x85\\xe0\\xa4\\xb8\\xe0\\xa5\\x8d\\xe0\\xa4\\xaa\\xe0\\xa4\\xa4\\xe0\\xa4\\xbe\\xe0\\xa4\\xb2\\n|image          = | \\n|established    = 1946\\n|type           = [[Public university|Public]] \\n|city           = [[Darbhanga]] \\n|country        = [[India]] \\n|students       =  \\n|staff          = \\n|campus         = [[urban area|Urban]]\\n}}\\'\\'\\'History\\'\\'\\' \\'\\'\\' http://www.darbhangamedicalcollege.in/aboutus.php{{dead link|date=December 2012}}\\n\\nthen the exact content of the body of http://www.darbhangamedicalcollege.in/aboutus.php and then\\n\\n==Courses Offered==\\nFollowing courses are offered:{{cite | url = http://mohfw.nic.in/ubihar.html{{dead link|date=December 2012}} | title = List of Recognised Medical Qualifications}}\\n* MBBS (90 seats)\\n* MD - Biochem, Forensic Med, Microbiol, Med, Paediatrics, Pathol, Pharmacol, Physiology, Preventive & Social Med, Radio-diagnosis, \\n* MS - Anaesthesiology, Anatomy, ENT Surgery, Obst & Gynae, Ophthal, Orthopaedics\\n* Diplomas - DA, DCH, DCP, DGO, DMRD, DOMS, DTMH, DLO \\n\\n==Address==\\nLaheriasarai, Bihar, India - 846003\\n\\n==References==\\n{{Reflist}}\\n\\n==External links==\\n*[http://institutions.education4india.com/3649/darbhanga-medical-college-laheriasarai-846003-darbhanga-bihar/ Educational Institutes of India ]\\n*[http://gov.bih.nic.in/Profile/Institutions.htm Educational Institutes of Bihar]\\n\\n{{coord missing|Bihar}}\\n\\n[[Category:Universities and colleges in Bihar]]\\n[[Category:Medical colleges in Bihar]]\"',\n       b'\"\\n\\nYour young age\\n\\nI\\'ve figured out what makes you so \"\"interested\"\" in that donkey punch article.  It\\'s your age.  You\\'re a minor.  A child, in other words.  You\\'re under one or both ages of majority in the United States, 18 or 21.  I suspect you are a high school student under 18.  Earlier, you claimed to be \"\"Steven Andrew Miller, a student living in Printer\\'s Row, Chicago.\"\"  Most university students advertise their university affiliations.  You didn\\'t.  Makes me think you\\'re a child.  So, just how young are you?  People should know whom their dealing with, so they don\\'t get in trouble for using profanity to a child or insulting a child or saying negative things to a child or whatever is illegal with children nowadays.  It\\'s problematic to deal with children on the internet, and you should put up a userbox indicating your age to avoid those problems.  \"',\n       b\"Well, it's now at FoxNews and the New York Post, and his publicist has made a statement concerning it, so is it now ready for the Article page, or are we going to play games that is doesn't exist? Naaaanaaanaaa I can't hear you, my fingers are in my ears! Honestly folks, it's a sad day when Wikipedia skews an article even when established respectable news outlets are publishing stories about it. Sad day indeed.\",\n       b'\":I wish you had done as I asked, when I said, \"\"...please just go ahead and block me for a week or two now\"\".  Thank you for at least doing so now; better late than never.  I won\\'t respond to the rest of your post, as clearly the 5 \"\"bad phrase\"\" examples and the block show I am not allowed to defend my actions, or point out improper actions of others, as when I do, I am blocked for it; you\\'ve clearly shown that I can\\'t do that so I won\\'t attempt it.  PS I am sure you don\\'t see this block as being done at my request; no need to point it out. (t\\xe2\\x80\\xa2c) \\n\\n\"',\n       b'So what? I mentioned private family holding and it is already well known as the picture Browne produces in his materials. You want to play this game. Very well. Watch this.',\n       b'\"\\n\\nWell, we now have a copyvio in the history of the temp page. The current version could still profit from some more rephrasing (and from including reformulated stuff from the second page from Medicinenet - click on the small \"\"2\"\" at the bottom, it\\'ll show you some info on treatments). It\\'s not ideal, but since we usually tolerate copyvios in the history, I think we might be fine. Alternatively, you could just copy your temp version offline, delete the temp page, and recreate it anew with the saved content. After all, you\\'re an admin, and I don\\'t think anybody would object to your temporarily deleting a temp page to get rid of copyvio in ther history.  22:42, 20 Dec 2004 (UTC)\"',\n       b'\"\\n\\n Troubling aspects of the artiicle as it exists right now \\n\\n1) I believe if Eric Altermann is to be accorded the prestige of being associated with msnbc.com, it should be noted that he is a blogger for msnbc.com. I say that because I found conservative columnist and author Michelle Malkin referred to as a blogger in another article and I do believe that\\'s meant to diminish the value of her statement. So, again, yes. We HAVE to see the forest for the trees. No one article in Wikipedia exists in a vaccum. There has to be a baseline standard and I reject any argumentation that suggests implies or directs that if \\'I don\\'t like that article, go change it.\\' Instead, one way or the other (and I don\\'t care which way) liberals and conservatives have to be characterized by the same fair measuring stick.\\n\\n2) This is new. It\\'s from Conason: \"\"Conason goes on to point out that Coulter\\'s critical nature is blunted by her pre-assumed opinions, making many of the conclusions she draws irrelevant to the actual nature of her arguments.\"\" Wow! That seems a little unfair just hanging out there like that. I propose either eliminating it or...and I hate this approach as much as the next guy...scotch-taping a quote after that which refutes this obvious cheap-shot.\\n\\n3) This is old. The McVeigh section, as should be no surprise to anyone who\\'s followed my work here, is troubling. Let\\'s start with this quote: \"\" She was willing to characterize members of the Branch Davidians at the Waco compound as \"\"harmless American citizens\"\" [6] even after the survivors of the raid and subsequent immolation of the group by their leader were convicted.\"\"\\nWell, some Waco compounders were convicted of something or another. But the fact is that there WERE mostly innocent victims of David Koresh in that compound. It\\'s unbalanced to just add the tag line about conviced Davidians without acknowledging what I GUARANTEE you Ann was referring to, which were the overwhelming number that were innocent. \\n\\nThis quote is also troubling: \"\"Timothy McVeigh made similar criticism as partial justification for his bombing.\"\" Well, McVeigh had LOTS of reasons for his bombing. But who cares what a madman thinks? And if you do, then I understand he also had some uncomplimentary things to say about the Bible. Therefore, if we are to include this in Coulter\\'s section, I say we include it\\'s compelement in Bill Maher\\'s article. It would go something like this:\\n\"\"Naher has frequently criticized the Bible calling it a \\'bunch of fairy tales\\' and claiming \\'God doesn\\'t write books.\\'  Oklahoma City bomber Timothy McVeigh made similar criticism as partial justification for his distrust of Christianity, though the Bible has been widely criticized throughout history.\"\"\\n\\nWhady\\'a say?\\n\\n(Much more on this section later.)\\n\\n4) Racism is not the same as being anti-Islamic terrorists that want to kill Americans for supporting Israel etc. To title Ann\\'s comments on Islamic Radicals as allegations of racism seems dishonest to me.\\n\\n5) Did Ann really say that ALL  women are \"\"not as bright\"\" as men\\'?\\nSome editor in Wikipedia wants us to think so. I say this quote be taken out until we get confirmation of her whole statement in context. Now, I know for a fact that Ann is only half-joking when she argues women should not be allowed to vote so she is far from imune when it comes to suggesting she holds less-than-boilerplate-feminist views. But this quote supposedly out of H & C seems like it\\'s probably another smear. Again, if proven wrong, I\\'ll gladly man up and stand corrected. But, unitl I\\'m proven it wrong, it should be excised in my view. \\n\\nMore later... \"',\n       b'\"\\n\\nGeorge, you are taking the role of the Arbitration Committee, all by your self. The community already decided that my ArbCom case page should stand, you step beyond your powers by \"\"ruling\"\" it disruptive.\\n\\nMy ArbCom elections participation referred to an ANI in which I was admonished, yet my asking \"\"any general concerns by the five days of silence from admins in this ANI\"\" in a question to candidates is taken as, \"\"purpose of causing trouble for people Harry has disagreed with\"\", \"\"veiled personal attack\"\" and \"\"some kind of agenda behind them\"\". It\\'s almost as if the admin community had some vestigial shame on how that ANI was handled, why else suppress it by removing the questions from the candidates pages?\\n\\nI have been advised not to make edits to articles if I wish my ArbCom case page to be regarded as timeous, then you say I\\'m, \"\"not even trying to build articles anymore\"\". However I regarded ArbCom elections, and editing WP:AGF - , ,  as exceptions. As was my WP:AGF talk page participation.\\n\\nIf I was going to re-write the satire,  I would satirise the admin behaviour it was aimed at by comparison to the mindset of the religious police of the Iranian state.\\n\\nThe religious police are to Islam and the Koran as some admins are to Wikipedia and it\\'s Policies and Guidelines.\\n\\nYou can draw further parallels with conservative and liberal wings among admins, but no one likes an extremest.\\n\\nIf you don\\'t like that comparison then compare them with the mindset of US soldiers at Abu Ghraib.\\n\\nOver-the-top comparisons? Remember I\\'m talking about mindset here; but given the chance, how many admins would you expect to behave as abhorrently as the religious police? Now take that answer and multiply by three. If the product is greater than zero, then you must have thought of at least one admin who you would expect to abuse others human rights if they had real powers in the real world. Don\\'t you think that these admins should have the tools gently removed from their hands?\\n\\nPart of the humour of the  was the alternative spelling of Constable to Cunstable - not very subtle humour & a very minor part near the end, but obviously intended as humour, and read with good faith can only be regarded as such - however this was used as a pretext to speedy delete after 5 hours in WP:MFD.  was put up with the Brainstorming template from the first, indicating an appeal for participation in it\\'s production and alteration, as someone did when it was in WP:Namespace. If the cited reason of \"\"calling admins cunts by typographic means\"\" was the only objection, then it would have been quite easy to participate and call for those changes. Instead of participation, admins afraid of satire, suppressed the satire rather than examine the underlying concern the piece was obviously addressing - WP:religious police, another satire anyone?\\n\\nI would remind you of the story of Dickens being sued by some gentlemen over Oliver Twist, citing that the book used their person as the basis for the poorhouse character who refused Oliver\\'s request for more (despite it being politely put; I know how he feels), the judge summed up that anyone who saw themselves in that character wasn\\'t up to much (I paraphrase). If you felt you were being targeted by my satire, then I would ask which character from Oliver you think the general readership would regard you as.\\n\\nI trust that my useful, and still standing, contributions to WP:AGF, and for the other rebuttals of your given reasons for the block, you will change your view to one which I hold myself - I am here to build an encyclopaedia; and lift the block.\\n\\nFinally I would say, all that is required for evil to flourish is for good Wikipedians to do nothing.   18:0',\n       b\"I haven't been following the conversation too much, but I'm assuming he's referring to me and my comments I made on this very talk page to him asking him to cut out the sockpuppet crap? It's difficult to tell...because your comments are the only ones on here, it makes it look very one-sided. At any rate, I'm not who he claims me to be, and I certainly do believe he has exhibited negative behavior both in past and present incidents such as this one.\",\n       b'Hmmm... just my two cents, but I find it quite bizarre that no fewer than three administrators (',\n       b\"HELLO Liberal MAGGOT Part TWO\\nCensorship will not make me go away... thanks to you, Rob, I am now using my influence, to undermine the crediablity of Wikipedidiots, like you, and Wikipedia.  To bad you don't want to acknowledge the fact that you and your kind are ignorant, knuckle dragging,CyberThugs. Your shadow Joe.\",\n       b'\"\\n\\n A barnstar for you! \\n\\n  The Original Barnstar Thanks for all the standardization work you\\'ve been doing recently, on IPA templates. (And from a glance at various archives, thanks also for many years of work improving the templates themselves!) \\xe2\\x80\\x93  \"',\n       b'Simon Boliva over claimed \\n\\nMany historians have noted that Simon boliva Deliberately claimed additional land for this mountain country even though he knew bolivia will always be a land of mountain people, why is this not mention in the article, its historical facts!!!!',\n       b', 22 July 2008 (UTC)\\n\\nYeah, nice to take a back seat for a while.   22:21',\n       b'\"\\n\\nRose\\'s surname should be capitalized in the page title (\"\"Phyllis rose\"\"). I can\\'t figure out how to edit that   \"',\n       b\"Could you help me, perhaps?  I'm the Finkbeiner of the Finkbeiner test.  I'd this article to link to my original post.  Right now, it links to a re-post of my post on another site.  This subject has gotten a lot of attention and I'm happy for Christie Aschwanden and Double XX Science to get most of it.  But would you mind adding this link? http://www.lastwordonnothing.com/2013/01/17/5266/  The title of the original post is What I'm Not Going To Do.\\n\\nI tried to do this myself but my html skills are rudimentary.  As is, not even there.\\n\\nThank you for considering it.\\n\\nAnn Finkbeiner\",\n       b'\"\\n\\n Article Absolutely Neutral \\nThe article is Absolutely Neutral and it is not a \"\"point of view\"\" but absolute facts and references have been cited.User:Vaishnava\"',\n       b\"Doug, \\nto doug\\nYou'll find on a second reading that the edit is not an essay but disproves the Original Research as portrayed in the article attempting to masquerade as fact. The source texts are peer-reviewed external third-party books, if you'd care to re-check. Thus the Disputation stands the test of Wikipedia edit requirements as it not original research, but, the required rebuttal of Original Research in order for an article which does not meet any criteria of POV Neutrality and research that diverges from academic mainstream discourse. Additionally, the links I use add to Wikipedia in the framework of cross-linking to related articles.\\nFrankly, if more articles were written as thoroughly as the Disputation alone- Wikipedia would not be a steaming pile of politicised horse manure and would not be ridiculed a d banned from even primary and secondary school referencing (as per UK, Australia and Indonesia school curriculau)\",\n       b'\"\\n\\noh please, I think my arguments need a better response than just you calling for ceasing accusations.Its just that you dont even have an answer and you know exactly what you meant when you said that I am trying to make someone \"\"LOOK\"\" european. what do you mean by concentrating on the topic? I said enough about the topic. we BOTH agreed that borders are not established yet, that is why putting them in several regions but europe is uaaceptable.  It either needs to be in Asian section as it is now already, or in the western asian section where it is now already as well, or the european section where it is listed as western asian or it does not have to be there at all.it  is everywjere but in europe even though borders are not official yet? WHY do you think it is very easy for you to put them in asia, or western asia and not europe even though the borders are not established ? why do you think those people and me( I can imagine how big you think of you as european)  are trying to \"\"LOOK\"\" and be european? what gives you the right to give that supercilious evaluation ? you are BIASED , thats the only explanation.YES, I am accusing you and I am accusing you of being a racist and being a paranoid that thinks that everyone is trying to be European and HUSOND is a brave person who does everything to protect them from \"\"Unnessesary\"\" people and people who he thinks are LESS than him personally because he is european. Europe is like a priviledged section where countries can be put thats why its hard for them to be there and not hard to be in middle east even though the article itself says that first european homo georgicus was found in georgia.next time if you will bother to write a response, I urge you to adress all these REAL issues instead of calling for ceasing accusations.    \"',\n       b'\"\\nDear , you say that in RWP there are \"\"Many small unsignificant parties are claiming as \"\"the official representative\"\", \"\"president\"\", etc.\"\" It\\'s not right. After the National Congress III (few days ago), some leaders of these parties were jailed by Indonesian government. These organizations were mostly closed after getting their politicians jailed. The only parties left were WPLO (I am representative of it in Russia), KNPB and WPNLA. And all these parties made a coalition at 25th October. So, now West Papuan separatism is much more united and serious than it was before.   \"'],\n      dtype=object), array([[0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [1, 0, 0, 0, 1, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0]]))\n\n\nAnd we check also the shape. We expect a feature of shape (batch, ) and a target of shape (batch, number of labels).\n\n\nCode\nprint(\n  f\"text train shape: {train_ds.as_numpy_iterator().next()[0].shape}\\n\",\n  f\" text train type: {train_ds.as_numpy_iterator().next()[0].dtype}\\n\",\n  f\"label train shape: {train_ds.as_numpy_iterator().next()[1].shape}\\n\",\n  f\"label train type: {train_ds.as_numpy_iterator().next()[1].dtype}\\n\"\n  )\n\n\ntext train shape: (32,)\n  text train type: object\n label train shape: (32, 6)\n label train type: int64"
  },
  {
    "objectID": "posts/toxic_comment_filter/toxic_comment_filter.html#definition",
    "href": "posts/toxic_comment_filter/toxic_comment_filter.html#definition",
    "title": "Toxic Comment Filter",
    "section": "Definition",
    "text": "Definition\nDefine the model using the Functional API.\n\n\nCode\ndef get_deeper_lstm_model():\n    clear_session()\n    inputs = Input(shape=(None,), dtype=tf.int64, name=\"inputs\")\n    embedding = Embedding(\n        input_dim=config.max_tokens,\n        output_dim=config.embedding_dim,\n        mask_zero=True,\n        name=\"embedding\"\n    )(inputs)\n    x = Bidirectional(LSTM(256, return_sequences=True, name=\"bilstm_1\"))(embedding)\n    x = Bidirectional(LSTM(128, return_sequences=True, name=\"bilstm_2\"))(x)\n    # Global average pooling\n    x = GlobalAveragePooling1D()(x)\n    # Add regularization\n    x = Dropout(0.3)(x)\n    x = Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n    x = LayerNormalization()(x)\n    outputs = Dense(len(config.labels), activation='sigmoid', name=\"outputs\")(x)\n    model = Model(inputs, outputs)\n    model.compile(optimizer='adam', loss=\"binary_crossentropy\", metrics=config.metrics, steps_per_execution=32)\n    \n    return model\n\nlstm_model = get_deeper_lstm_model()\nlstm_model.summary()"
  },
  {
    "objectID": "posts/toxic_comment_filter/toxic_comment_filter.html#callbacks",
    "href": "posts/toxic_comment_filter/toxic_comment_filter.html#callbacks",
    "title": "Toxic Comment Filter",
    "section": "Callbacks",
    "text": "Callbacks\nFinally, the model has been trained using 2 callbacks: - Early Stopping, to avoid to consume the kaggle GPU time. - Model Checkpoint, to retrieve model training information.\n\n\nCode\n# callbacks\nmy_es = config.get_early_stopping()\nmy_mc = config.get_model_checkpoint(filepath=\"/checkpoint.keras\")\ncallbacks = [my_es, my_mc]"
  },
  {
    "objectID": "posts/toxic_comment_filter/toxic_comment_filter.html#final-preparation-before-fit",
    "href": "posts/toxic_comment_filter/toxic_comment_filter.html#final-preparation-before-fit",
    "title": "Toxic Comment Filter",
    "section": "Final preparation before fit",
    "text": "Final preparation before fit\nConsidering the dataset is imbalanced, to increase the performance we need to calculate the class weight. This will be passed during the training of the model.\n\n\nCode\nlab = pd.DataFrame(columns=config.labels, data=ytrain)\nr = lab.sum() / len(ytrain)\nclass_weight = dict(zip(range(len(config.labels)), r))\ndf_class_weight = pd.DataFrame.from_dict(\n  data=class_weight,\n  orient='index',\n  columns=['class_weight']\n  )\ndf_class_weight.index = config.labels\n\n\n\nCode\nlibrary(reticulate)\npy$df_class_weight\n\n\n\n\nTable 4: Class weight\n\n\n\n              class_weight\ntoxic          0.095900590\nsevere_toxic   0.009928468\nobscene        0.052757858\nthreat         0.003061800\ninsult         0.049132042\nidentity_hate  0.008710911\n\n\n\n\nIt is also useful to define the steps per epoch for train and validation dataset. This step is required to avoid\n\n\nCode\nsteps_per_epoch = config.train_samples // config.batch_size\nvalidation_steps = config.val_samples // config.batch_size"
  },
  {
    "objectID": "posts/toxic_comment_filter/toxic_comment_filter.html#fit",
    "href": "posts/toxic_comment_filter/toxic_comment_filter.html#fit",
    "title": "Toxic Comment Filter",
    "section": "Fit",
    "text": "Fit\nThe fit has been done on Kaggle to levarage the GPU. Some considerations about the model:\n\n.repeat() ensure the model sees all the daataset.\nepocs is set to 100.\nvalidation_data has the same repeat.\ncallbacks are the one defined before.\nclass_weight ensure the model is trained using the frequency of each class, because our dataset is imbalanced.\nsteps_per_epoch and validation_steps depend on the use of repeat.\n\n\n\nCode\nhistory = model.fit(\n  processed_train_ds.repeat(),\n  epochs=config.epochs,\n  validation_data=processed_val_ds.repeat(),\n  callbacks=callbacks,\n  class_weight=class_weight,\n  steps_per_epoch=steps_per_epoch,\n  validation_steps=validation_steps\n  )\n\n\nNow we can import the model and the history trained on Kaggle.\n\n\nCode\nmodel = load_model(filepath=config.model)\nhistory = pd.read_excel(config.history)"
  },
  {
    "objectID": "posts/toxic_comment_filter/toxic_comment_filter.html#evaluate",
    "href": "posts/toxic_comment_filter/toxic_comment_filter.html#evaluate",
    "title": "Toxic Comment Filter",
    "section": "Evaluate",
    "text": "Evaluate\n\n\nCode\nvalidation = model.evaluate(\n  processed_val_ds.repeat(),\n  steps=validation_steps, # 748\n  verbose=0\n  )\n\n\n\nCode\ntibble(\n  metric = c(\"loss\", \"precision\", \"recall\", \"auc\", \"f1_score\"),\n  value = py$validation\n  )\n\n\n\n\nTable 5: Model validation metric\n\n\n\n# A tibble: 5 × 2\n  metric     value\n  &lt;chr&gt;      &lt;dbl&gt;\n1 loss      0.0542\n2 precision 0.789 \n3 recall    0.671 \n4 auc       0.957 \n5 f1_score  0.0293"
  },
  {
    "objectID": "posts/toxic_comment_filter/toxic_comment_filter.html#predict",
    "href": "posts/toxic_comment_filter/toxic_comment_filter.html#predict",
    "title": "Toxic Comment Filter",
    "section": "Predict",
    "text": "Predict\nFor the prediction, the model does not need to repeat the dataset, because the model has already been trained and now it has just to consume the data to make the prediction.\n\n\nCode\npredictions = model.predict(processed_test_ds, verbose=0)"
  },
  {
    "objectID": "posts/toxic_comment_filter/toxic_comment_filter.html#confusion-matrix",
    "href": "posts/toxic_comment_filter/toxic_comment_filter.html#confusion-matrix",
    "title": "Toxic Comment Filter",
    "section": "Confusion Matrix",
    "text": "Confusion Matrix\nThe best way to assess the performance of a multi label classification is using a confusion matrix. Sklearn has a specific function to create a multi label classification matrix to handle the fact that there could be multiple labels for one prediction.\n\nGrid Search Cross Validation for best threshold\nGrid Search CV is a technique for fine-tuning hyperparameter of a ML model. It systematically search through a set of hyperparamenter values to find the combination which led to the best model performance. In this case, I am using a KFold Cross Validation is a resempling technique to split the data into k consecutive folds. Each fold is used once as a validation while the k - 1 remaining folds are the training set. See the documentation for more information.\nThe model is trained to optimize the recall. The decision was made because the cost of missing a True Positive is greater than a False Positive. In this case, missing a injurious observation is worst than classifying a clean one as bad.\nHaving said this, I still want to test different metrics other than the recall_score to have more possibility of decision of the best threshold.\n\nf1_score\n\n\nCode\nytrue = ytest.astype(int)\ny_pred_proba = predictions\noptimal_threshold_f1, best_score_f1 = config.find_optimal_threshold_cv(ytrue, y_pred_proba, f1_score)\n\nprint(f\"Optimal threshold: {optimal_threshold_f1}\")\n\n\nOptimal threshold: 0.15000000000000002\n\n\nCode\nprint(f\"Best score: {best_score_f1}\")\n\n\nBest score: 0.4788653077945807\n\n\nCode\n\n# Use the optimal threshold to make predictions\nfinal_predictions_f1 = (y_pred_proba &gt;= optimal_threshold_f1).astype(int)\n\n\nOptimal threshold f1 score: 0.15. Best score: 0.4788653.\n\n\nrecall_score\n\n\nCode\nytrue = ytest.astype(int)\ny_pred_proba = predictions\noptimal_threshold_recall, best_score_recall = config.find_optimal_threshold_cv(ytrue, y_pred_proba, recall_score)\n\n# Use the optimal threshold to make predictions\nfinal_predictions_recall = (y_pred_proba &gt;= optimal_threshold_recall).astype(int)\n\n\nOptimal threshold recall: 0.05. Best score: 0.8095814.\n\n\nroc_auc_score\n\n\nCode\nytrue = ytest.astype(int)\ny_pred_proba = predictions\noptimal_threshold_roc, best_score_roc = config.find_optimal_threshold_cv(ytrue, y_pred_proba, roc_auc_score)\n\nprint(f\"Optimal threshold: {optimal_threshold_roc}\")\n\n\nOptimal threshold: 0.05\n\n\nCode\nprint(f\"Best score: {best_score_roc}\")\n\n\nBest score: 0.8809499649742268\n\n\nCode\n\n# Use the optimal threshold to make predictions\nfinal_predictions_roc = (y_pred_proba &gt;= optimal_threshold_roc).astype(int)\n\n\nOptimal threshold roc: 0.05. Best score: 0.88095.\n\n\n\nConfusion Matrix Plot\nThe confusion matrix is plotted using the multilabel_confusion_matrix function in scikit-learn. We have to plot a confusion matrix for each label. To plot the confusion matrix, we need to convert the predicted probability of a label to a proper prediction. To do so, we use the calculated optimal threshold for the recall, which is 0.05. The confusion matrix plotted hete, considering we have a multi label task, is not a big one with all the labels as columns and indices. We plot a confusion matrix for each label with a simple for loop, which extract for each loop the confusion matrix and the associated label.\n\n\nCode\n# convert probability predictions to predictions\nypred = predictions &gt;=  optimal_threshold_f1 # .05\nypred = ypred.astype(int)\n\n# create a plot with 3 by 2 subplots\nfig, axes = plt.subplots(3, 2, figsize=(15, 15))\naxes = axes.flatten()\nmcm = multilabel_confusion_matrix(ytrue, ypred)\n# plot the confusion matrices for each label\nfor i, (cm, label) in enumerate(zip(mcm, config.labels)):\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n    disp.plot(ax=axes[i], colorbar=False)\n    axes[i].set_title(f\"Confusion matrix for label: {label}\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 4: Multi Label Confusion matrix"
  },
  {
    "objectID": "posts/toxic_comment_filter/toxic_comment_filter.html#dataset",
    "href": "posts/toxic_comment_filter/toxic_comment_filter.html#dataset",
    "title": "Toxic Comment Filter",
    "section": "Dataset",
    "text": "Dataset\nNow we can split the dataset in 3: train, test and validation sets. Considering there is not a function in sklearn which lets split in these 3 sets, we can do the following: - split between a train and temporary set with a 0.3 split. - split the temporary set in 2 equal sized test and val sets.\n\n\nCode\nx = df[config.features].values\ny = df[config.labels].values\n\nxtrain, xtemp, ytrain, ytemp = train_test_split(\n  x,\n  y,\n  test_size=config.temp_split, # .3\n  random_state=config.random_state\n  )\nxtest, xval, ytest, yval = train_test_split(\n  xtemp,\n  ytemp,\n  test_size=config.test_split, # .5\n  random_state=config.random_state\n  )\n\n\nxtrain shape: py$xtrain.shape ytrain shape: py$ytrain.shape xtest shape: py$xtest.shape ytest shape: py$ytest.shape xval shape: py$xval.shape yval shape: py$yval.shape\nThe datasets are created using the tf.data.Dataset function. It creates a data input pipeline. The tf.data API makes it possible to handle large amounts of data, read from different data formats, and perform complex transformations. The tf.data.Dataset is an abstraction that represents a sequence of elements, in which each element consists of one or more components. Here each dataset is creates using from_tensor_slices. It create a tf.data.Dataset from a tuple (features, labels). .batch let us work in batches to improve performance, while .prefetch overlaps the preprocessing and model execution of a training step. While the model is executing training step s, the input pipeline is reading the data for step s+1. Check the documentation for further informations.\n\n\nCode\ntrain_ds = (\n    tf.data.Dataset\n    .from_tensor_slices((xtrain, ytrain))\n    .shuffle(xtrain.shape[0])\n    .batch(config.batch_size)\n    .prefetch(tf.data.experimental.AUTOTUNE)\n)\n\ntest_ds = (\n    tf.data.Dataset\n    .from_tensor_slices((xtest, ytest))\n    .batch(config.batch_size)\n    .prefetch(tf.data.experimental.AUTOTUNE)\n)\n\nval_ds = (\n    tf.data.Dataset\n    .from_tensor_slices((xval, yval))\n    .batch(config.batch_size)\n    .prefetch(tf.data.experimental.AUTOTUNE)\n)\n\n\n\n\nCode\nprint(\n  f\"train_ds cardinality: {train_ds.cardinality()}\\n\",\n  f\"val_ds cardinality: {val_ds.cardinality()}\\n\",\n  f\"test_ds cardinality: {test_ds.cardinality()}\\n\"\n  )\n\n\ntrain_ds cardinality: 3491\n val_ds cardinality: 748\n test_ds cardinality: 748\n\n\nCheck the first element of the dataset to be sure that the preprocessing is done correctly.\n\n\nCode\ntrain_ds.as_numpy_iterator().next()\n\n\n(array([b\"Although interesting, this is one of the stranger proposals to deal with this issue.  It's like saying BLP vios are okay, if only done in small font.  The images themselves are a significant part of the issue, and covering the event without using the images is incomplete.  We can make a conscious decision to sacrifice encyclopedia quality for the goal of free content, and in so doing diverge from most every other information source int he world, but if so that's what it is.\",\n       b'Calvin Harris (album) \\n\\nI have nominationed Calvin Harris (album) for deletion at Articles for deletion as it was the article creator  who removed the original PROD tag when removing my CSD tag, rather than me when adding the CSD tag. As such, it counts as PROD contest, which means the article no longer qualify for PROD. Thanks!',\n       b\", I'm actually supporting it now based on that one book, and the point  made at the beginning of this conversation about her heritage.\",\n       b'\"\\nI don\\'t know if those TOC links are entirely useful. Wikipedia needs to come up with a way to link to items within a large table like that. Perhaps we could use the HTML code with links with the # like that?\\n\\n \\xe2\\x89\\x88talk \"',\n       b'Piss off, she is an ignorant bitch.',\n       b'\"\\n\\nHey, nobody\\'s wrong here, and nobody is defininitely a loser. I think that user Abhishek191288 reverted Markshen1985\\'s edits because there are some opinions to his edits such as  logo is a brilliant red kapok delicately adoring a blue vertical tail fin. Also, much of \"\"History and Development is unsourced. Having said that, Markshen1985\\'s work is remarkable. Also, there seems to be a touch of discrimination here, with \"\"Chinese\"\" being thrown around. I\\'m going report this to .  (Talk) (Contributions)(Feed back needed @ Talk page) \"',\n       b'He means redirect.',\n       b'. i would also like it to be noted that Amultic was the one who gave me my previous warnings after defending my self against a personnel attack by another user seehere ignoring the other users remarks, i question the merits of those warning which lead to this.',\n       b\"Oh wait, you're an American asshole... Let me explain how the law here work, if telenet uncovers who I am, they are violating several laws. I have free speech here.\",\n       b'gay \\n\\nwhy you so gay? you hiding something, fatty?',\n       b'Article is on editing process... \\n\\nGreetings to all Editors!\\n\\nThis article is on-going process of revision and editing, please give us more time to work for it. The person in this article is notable, he do have a lot of contributions and achievement not only here at Asia, Europe and across America. I included all the possible references and links to support his notability. Any consideration to this is highly appreciated and with thanks. Best Regards. Turki Faisal Al Rasheed',\n       b\"Does anyone know if he performed in woodstock 1970.  They make Woodstock 70 a bold title.  And by boldness i do mean there was a woodstock 70.  i can't remember all of the performers and they are understated. i know some not enough.  i always thought the other players kept track better than me. None have come forward.  Could Jackson C.\\nFrank be a link?\",\n       b'\"\\nMONKEY MONKEY MONKEY MONKEY MONKEY  \\xe2\\x80\\x94Preceding unsigned comment added by 210.43.128.18   \"',\n       b'COOL ANYTHING ELSE D, are you gonna report me for being sarcastic as well?',\n       b\"~~Assuming that the first sentence in the Casablanca article is NOT supposed to be that it's the worst movie ever, someone has altered it to say so.  I don't have time right now to read the entire article to see if anything else has been childishly tampered with, but I thought I'd try to point it out.  If no one has fixed it up in a few days I'll try to wander back, though I know very little about the movie (I ended up here because I wanted info on the movie, not because I had it)  Cheers  (24 Nov 06)\",\n       b'Let me go further, this organization has been around for at least a decade. Given traditional high-level turnover rate of campaign staff there are probably (50 states x 10 years x ~5 state director turnover rate) ~2,500 former AFD state Exec Directors running around. Are we going to add articles on each one that gets mentioned in RS?',\n       b'It was a fine tree.',\n       b\"By saying Jubilees is not canonical in any mainstream denomination, you are revealing your abject ignorance and bias POV, because while your personal religion may not consider it canonical, the Ethiopian Orthodox Christians and Ethiopian Jews do, and they are Abrahamic.  The Orthodox Christians make up a majority in Ethiopia.  The Constantinian Christians in the Roman Empire,  and the Pharisee Sanhedrin both tried to do away with Jubilees, but wikipedia is neutral and does not subscribe to any POV in disputes like that, nor will it assist you in attemting to marginalize Ethiopians' religious beliefs.\",\n       b'February 2015 (UTC)directly where relevant.   02:49, 18',\n       b'\" July 2006 (UTC)\\n\\nI haven\\'t attacked anyone on any page.   23:05, 24\"',\n       b'But Abeg92 is a faggot.',\n       b\"Sir Gawain and the Green Knight\\nThanks for bringing this to my attention. I've alerted the editor who has done the most to develop the article over the past several months, and I personally will take a more thorough look when I have time. t/c\",\n       b\"Homage to Mussorgsky: Epithalamium \\n\\nAm I the only Wikipedian who was at the premi\\xc3\\xa8re? If not, please share your thoughts. As it happens, my father is the pianist's agent, so that's hwy I was there: I wasn't sure what to think of the piece - I preferred the Lyapunov Transcendental Studies that Malcolm Binns also played that night.Vox  8'\",\n       b'VfD\\nOn May 4, 2005, this article was nominated for deletion.  The result was merge/redirect.  See Wikipedia:Votes for deletion/Boc for a record of the discussion.  (spill yours?)',\n       b'Had a look. Started making changes. Noticed WP:Copyvio, logged my concern via helpme and page got deleted. See John_W._R._Taylor',\n       b'Thank you for your patience. I hope all of this will end well. 92.36.173.254',\n       b'Hi I have found some more links for you http://www.wwfindia.org/about_wwf/what_we_do/freshwater_wetlands/our_work/ramsar_sites/harike_wetlands_.cfm and  http://www.birding.in/birdingsites/harike_lake.htm and also http://www.gisdevelopment.net/aars/acrs/1997/ts7/ts7008.asp and finally a brilliant article on the History http://www.punjabheritage.org/architectural-heritage/local-enthusiasm-but-official-neglect-for-anglo-sikh-war-monuments2704.html',\n       b'\"\\n\\n Yes, I noticed, the text, mostly in the section \"\"Lennox Crisis\"\" is a summary of this article, Macauley, Sarah. \\'The Lennox Crisis\\', 1558-1563.\\', Northern History 41.2 (2004). I can\\'t tell if this text a good summary of that article, or if that article is useful from this summary.  \"',\n       b'YOUR A JERK!!!!!!! \\n\\nYou are a jerk yo lazy butt!',\n       b\"thanks,  it appears that you didn't read any of the links above. here i will provide the key one again: Wikipedia:Conflict_of_interest#Writing_about_yourself_and_your_work. i think your response would have been different, had you read that.  would you please read that, and reply again?  thanks.\",\n       b'\"\\nResponse much appreciated, thanks. Well, I\\'m trying to retain as much of your text as possible. There\\'s a lot of good stuff in there which simply needs sourcing and updating in with the other material. That\\'s why I abandoned the sandbox idea and thought it best to just run with what we have. The article is going to get massive in the next few weeks though to make it comprehensive as possible, but it\\'ll be split and condensed later. I really do need your help if possible though on tracing the source of the unsourced material. I\\'ll try to do what I can but I\\'ll approach you if I can\\'t find a source for something.\\xe2\\x99\\xa6  \"',\n       b\"To: User:Protonk, treating people like they are children will not get you any respect from free thinking individuals and I'd like you to know you're up for review.\"],\n      dtype=object), array([[0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [1, 1, 1, 0, 1, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [1, 0, 1, 0, 1, 1],\n       [1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [1, 0, 1, 0, 1, 1],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [1, 0, 1, 0, 1, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0]]))\n\n\nAnd we check also the shape. We expect a feature of shape (batch, ) and a target of shape (batch, number of labels).\n\n\nCode\nprint(\n  f\"text train shape: {train_ds.as_numpy_iterator().next()[0].shape}\\n\",\n  f\" text train type: {train_ds.as_numpy_iterator().next()[0].dtype}\\n\",\n  f\"label train shape: {train_ds.as_numpy_iterator().next()[1].shape}\\n\",\n  f\"label train type: {train_ds.as_numpy_iterator().next()[1].dtype}\\n\"\n  )\n\n\ntext train shape: (32,)\n  text train type: object\n label train shape: (32, 6)\n label train type: int64"
  },
  {
    "objectID": "posts/toxic_comment_filter/toxic_comment_filter.html#classification-report",
    "href": "posts/toxic_comment_filter/toxic_comment_filter.html#classification-report",
    "title": "Toxic Comment Filter",
    "section": "Classification Report",
    "text": "Classification Report\n\n\nCode\ncr = classification_report(\n  ytrue,\n  ypred,\n  target_names=config.labels,\n  digits=4,\n  output_dict=True\n  )\ndf_cr = pd.DataFrame.from_dict(cr).reset_index()\n\n\n\nCode\nlibrary(reticulate)\ndf_cr &lt;- py$df_cr %&gt;% dplyr::rename(names = index)\ncols &lt;- df_cr %&gt;% colnames()\ndf_cr %&gt;% \n  pivot_longer(\n    cols = -names,\n    names_to = \"metrics\",\n    values_to = \"values\"\n  ) %&gt;% \n  pivot_wider(\n    names_from = names,\n    values_from = values\n  )\n\n\n\n\nTable 6: Classification report\n\n\n\n# A tibble: 10 × 5\n   metrics       precision recall `f1-score` support\n   &lt;chr&gt;             &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;\n 1 toxic            0.677  0.840      0.749     2262\n 2 severe_toxic     0.328  0.812      0.468      240\n 3 obscene          0.655  0.888      0.754     1263\n 4 threat           0      0          0           69\n 5 insult           0.574  0.849      0.685     1170\n 6 identity_hate    0.154  0.372      0.218      207\n 7 micro avg        0.584  0.822      0.683     5211\n 8 macro avg        0.398  0.627      0.479     5211\n 9 weighted avg     0.603  0.822      0.692     5211\n10 samples avg      0.0603 0.0764     0.0641    5211"
  },
  {
    "objectID": "posts/toxic_comment_filter/history/f1score/toxic-comment-filter-bilstm.html",
    "href": "posts/toxic_comment_filter/history/f1score/toxic-comment-filter-bilstm.html",
    "title": "Simone Brazzi",
    "section": "",
    "text": "::: {#cell-0 .cell _uuid=‘8f2839f25d086af736a60e9eeb907d3b93b6e0e5’ _cell_guid=‘b1076dfc-b9ad-4769-8c92-a6c4dae69d19’ trusted=‘true’}\n\nCode\n# This Python 3 environment comes with many helpful analytics libraries installed# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python# For example, here's several helpful packages to loadimport numpy as np # linear algebraimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)# Input data files are available in the read-only \"../input/\" directory# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directoryimport osfor dirname, _, filenames in os.walk('/kaggle/input'):    for filename in filenames:        print(os.path.join(dirname, filename))# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n:::\n\n\nCode\nimport pandas as pdimport numpy as npimport matplotlib.pyplot as pltimport tensorflow as tfimport kerasimport keras_nlpfrom keras.backend import clear_sessionfrom keras.models import Model, load_modelfrom keras.layers import TextVectorization, Input, Dense, Embedding, Dropout, GlobalAveragePooling1D, LSTM, Bidirectional, GlobalMaxPool1D, Flatten, Attentionfrom keras.metrics import Precision, Recall, AUC, SensitivityAtSpecificity, SpecificityAtSensitivity, F1Scorefrom sklearn.model_selection import train_test_split, KFoldfrom sklearn.metrics import multilabel_confusion_matrix, classification_report, ConfusionMatrixDisplay, precision_recall_curve, f1_score, recall_score, roc_auc_scoreclass Config():    def __init__(self):        self.url = \"https://s3.eu-west-3.amazonaws.com/profession.ai/datasets/Filter_Toxic_Comments_dataset.csv\"        self.path = \"/Users/simonebrazzi/datasets/toxic_comment/Filter_Toxic_Comments_dataset.csv\"        self.max_tokens = 20000        self.output_sequence_length = 911 # check the analysis done to establish this value        self.embedding_dim = 128        self.batch_size = 32        self.epochs = 100        self.temp_split = 0.3        self.test_split = 0.5        self.random_state = 42        self.total_samples = 159571 # total train samples        self.train_samples = 111699        self.val_samples = 23936        self.features = 'comment_text'        self.labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']        self.new_labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate', \"clean\"]        self.label_mapping = {label: i for i, label in enumerate(self.labels)}        self.new_label_mapping = {label: i for i, label in enumerate(self.labels)}                self.model = \"model_f1.keras\"        self.checkpoint = \"checkpoint.lstm_model_f1.keras\"        self.history = \"lstm_model_f1.xlsx\"        self.matrix_file = \"confusion_matrices.png\"                self.metrics = [            Precision(name='precision'),            Recall(name='recall'),            AUC(name='auc', multi_label=True, num_labels=len(self.labels)),            F1Score(name=\"f1\", average=\"macro\")                    ]    def get_early_stopping(self):        early_stopping = keras.callbacks.EarlyStopping(            monitor=\"val_f1\", # \"val_recall\",            min_delta=0.2,            patience=10,            verbose=0,            mode=\"max\",            restore_best_weights=True,            start_from_epoch=3        )        return early_stopping    def get_model_checkpoint(self, filepath):        model_checkpoint = keras.callbacks.ModelCheckpoint(            filepath=filepath,            monitor=\"val_f1\", # \"val_recall\",            verbose=0,            save_best_only=True,            save_weights_only=False,            mode=\"max\",            save_freq=\"epoch\"        )        return model_checkpoint    def find_optimal_threshold_cv(self, ytrue, yproba, metric, thresholds=np.arange(.05, .35, .05), n_splits=7):      # instantiate KFold      kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)      threshold_scores = []      for threshold in thresholds:        cv_scores = []        for train_index, val_index in kf.split(ytrue):          ytrue_val = ytrue[val_index]          yproba_val = yproba[val_index]          ypred_val = (yproba_val &gt;= threshold).astype(int)          score = metric(ytrue_val, ypred_val, average=\"macro\")          cv_scores.append(score)        mean_score = np.mean(cv_scores)        threshold_scores.append((threshold, mean_score))        # Find the threshold with the highest mean score        best_threshold, best_score = max(threshold_scores, key=lambda x: x[1])      return best_threshold, best_scoreconfig = Config()\n\n\n\n\nCode\nfile = tf.keras.utils.get_file(\"Filter_Toxic_Comments_dataset.csv\", config.url)df = pd.read_csv(file)# df = pd.read_csv(config.path)df.loc[df.sum_injurious == 0, \"clean\"] = 1df.loc[df.sum_injurious != 0, \"clean\"] = 0x = df[config.features].valuesy = df[config.labels].valuesxtrain, xtemp, ytrain, ytemp = train_test_split(  x,  y,  test_size=config.temp_split, # .3  random_state=config.random_state  )xtest, xval, ytest, yval = train_test_split(  xtemp,  ytemp,  test_size=config.test_split, # .5  random_state=config.random_state  )print(    f\"df shape: {df.shape[0]}\\n\",  f\"xtrain shape: {xtrain.shape}\\n\",  f\"ytrain shape: {ytrain.shape}\\n\",  f\"xtest shape: {xtest.shape}\\n\",  f\"ytest shape: {ytest.shape}\\n\",  f\"xval shape: {xval.shape}\\n\",  f\"yval shape: {yval.shape}\\n\"  f\"labels: {config.labels}\\n\"  )text_vectorization = TextVectorization(  max_tokens=config.max_tokens,  standardize=\"lower_and_strip_punctuation\",  split=\"whitespace\",  output_mode=\"int\",  output_sequence_length=config.output_sequence_length,  pad_to_max_tokens=True  )train_ds = (    tf.data.Dataset    .from_tensor_slices((xtrain, ytrain))    .shuffle(xtrain.shape[0])    .batch(config.batch_size)    .prefetch(tf.data.experimental.AUTOTUNE))test_ds = (    tf.data.Dataset    .from_tensor_slices((xtest, ytest))    .batch(config.batch_size)    .prefetch(tf.data.experimental.AUTOTUNE))val_ds = (    tf.data.Dataset    .from_tensor_slices((xval, yval))    .batch(config.batch_size)    .prefetch(tf.data.experimental.AUTOTUNE))# Prepare a dataset that only yields raw text inputs (no labels)text_train_ds = train_ds.map(lambda x, y: x)# Adapt the text vectorization layer to the text data to index the dataset vocabularytext_vectorization.adapt(text_train_ds)processed_train_ds = train_ds.map(    lambda x, y: (text_vectorization(x), y),    num_parallel_calls=tf.data.experimental.AUTOTUNE)processed_val_ds = val_ds.map(    lambda x, y: (text_vectorization(x), y),    num_parallel_calls=tf.data.experimental.AUTOTUNE)processed_test_ds = test_ds.map(    lambda x, y: (text_vectorization(x), y),    num_parallel_calls=tf.data.experimental.AUTOTUNE)from keras.layers import LayerNormalization, MultiHeadAttention, GlobalAveragePooling1Ddef get_deeper_lstm_model():    clear_session()    inputs = Input(shape=(None,), dtype=tf.int64, name=\"inputs\")    embedding = Embedding(        input_dim=config.max_tokens,        output_dim=config.embedding_dim,        mask_zero=True,        name=\"embedding\"    )(inputs)    x = Bidirectional(LSTM(256, return_sequences=True, name=\"bilstm_1\"))(embedding)    x = Bidirectional(LSTM(128, return_sequences=True, name=\"bilstm_2\"))(x)    # Global average pooling    x = GlobalAveragePooling1D()(x)    # Add regularization    x = Dropout(0.3)(x)    x = Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)    x = LayerNormalization()(x)    outputs = Dense(len(config.labels), activation='sigmoid', name=\"outputs\")(x)    model = Model(inputs, outputs)    model.compile(optimizer='adam', loss=\"binary_crossentropy\", metrics=config.metrics, steps_per_execution=32)    return modelmodel = get_deeper_lstm_model()lab = pd.DataFrame(columns=config.labels, data=ytrain)lab.shaper = lab.sum() / len(ytrain)class_weight = dict(zip(range(len(config.labels)), r))class_weightsteps_per_epoch = config.train_samples // config.batch_sizevalidation_steps = config.val_samples // config.batch_size# callbacksmy_es = config.get_early_stopping()my_mc = config.get_model_checkpoint(filepath=\"/kaggle/working/\"  + config.checkpoint)callbacks = [my_es, my_mc]\n\n\nDownloading data from https://s3.eu-west-3.amazonaws.com/profession.ai/datasets/Filter_Toxic_Comments_dataset.csv\n66034407/66034407 ━━━━━━━━━━━━━━━━━━━━ 3s 0us/step\ndf shape: 159571\n xtrain shape: (111699,)\n ytrain shape: (111699, 6)\n xtest shape: (23936,)\n ytest shape: (23936, 6)\n xval shape: (23936,)\n yval shape: (23936, 6)\nlabels: ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n\n\n\n\n\nCode\nhistory_deeper_lstm_model = model.fit(    processed_train_ds.repeat(),    epochs=config.epochs,    validation_data=processed_val_ds.repeat(),    callbacks=callbacks,    class_weight=class_weight,    steps_per_epoch=steps_per_epoch,    validation_steps=validation_steps)\n\n\nEpoch 1/100\n3490/3490 ━━━━━━━━━━━━━━━━━━━━ 390s 98ms/step - auc: 0.8880 - f1: 0.0441 - loss: 0.0575 - precision: 0.7018 - recall: 0.4701 - val_auc: 0.9494 - val_f1: 0.0293 - val_loss: 0.0641 - val_precision: 0.8939 - val_recall: 0.4625\nEpoch 2/100\n3490/3490 ━━━━━━━━━━━━━━━━━━━━ 315s 90ms/step - auc: 0.9557 - f1: 0.0290 - loss: 0.0050 - precision: 0.7994 - recall: 0.6490 - val_auc: 0.9601 - val_f1: 0.0294 - val_loss: 0.0535 - val_precision: 0.7908 - val_recall: 0.6617\nEpoch 3/100\n3490/3490 ━━━━━━━━━━━━━━━━━━━━ 315s 90ms/step - auc: 0.9672 - f1: 0.0291 - loss: 0.0044 - precision: 0.8166 - recall: 0.6815 - val_auc: 0.9633 - val_f1: 0.0295 - val_loss: 0.0544 - val_precision: 0.7733 - val_recall: 0.6961\nEpoch 4/100\n3490/3490 ━━━━━━━━━━━━━━━━━━━━ 315s 90ms/step - auc: 0.9708 - f1: 0.0288 - loss: 0.0040 - precision: 0.8243 - recall: 0.7117 - val_auc: 0.9559 - val_f1: 0.0293 - val_loss: 0.0546 - val_precision: 0.7886 - val_recall: 0.6696\nEpoch 5/100\n3490/3490 ━━━━━━━━━━━━━━━━━━━━ 315s 90ms/step - auc: 0.9718 - f1: 0.0290 - loss: 0.0036 - precision: 0.8409 - recall: 0.7476 - val_auc: 0.9539 - val_f1: 0.0294 - val_loss: 0.0601 - val_precision: 0.7354 - val_recall: 0.6837\nEpoch 6/100\n3490/3490 ━━━━━━━━━━━━━━━━━━━━ 316s 90ms/step - auc: 0.9748 - f1: 0.0290 - loss: 0.0034 - precision: 0.8470 - recall: 0.7679 - val_auc: 0.9406 - val_f1: 0.0295 - val_loss: 0.0710 - val_precision: 0.7332 - val_recall: 0.6863\nEpoch 7/100\n3490/3490 ━━━━━━━━━━━━━━━━━━━━ 315s 90ms/step - auc: 0.9768 - f1: 0.0293 - loss: 0.0031 - precision: 0.8592 - recall: 0.7822 - val_auc: 0.9384 - val_f1: 0.0292 - val_loss: 0.0724 - val_precision: 0.7471 - val_recall: 0.6667\nEpoch 8/100\n3490/3490 ━━━━━━━━━━━━━━━━━━━━ 316s 90ms/step - auc: 0.9756 - f1: 0.0290 - loss: 0.0030 - precision: 0.8650 - recall: 0.7878 - val_auc: 0.9359 - val_f1: 0.0294 - val_loss: 0.0767 - val_precision: 0.7302 - val_recall: 0.6641\nEpoch 9/100\n3490/3490 ━━━━━━━━━━━━━━━━━━━━ 315s 90ms/step - auc: 0.9769 - f1: 0.0290 - loss: 0.0028 - precision: 0.8764 - recall: 0.8020 - val_auc: 0.9415 - val_f1: 0.0295 - val_loss: 0.0750 - val_precision: 0.7159 - val_recall: 0.6955\nEpoch 10/100\n3490/3490 ━━━━━━━━━━━━━━━━━━━━ 316s 90ms/step - auc: 0.9785 - f1: 0.0293 - loss: 0.0028 - precision: 0.8752 - recall: 0.8072 - val_auc: 0.9263 - val_f1: 0.0295 - val_loss: 0.0795 - val_precision: 0.7649 - val_recall: 0.6282\nEpoch 11/100\n3490/3490 ━━━━━━━━━━━━━━━━━━━━ 315s 90ms/step - auc: 0.9775 - f1: 0.0291 - loss: 0.0027 - precision: 0.8837 - recall: 0.8062 - val_auc: 0.9360 - val_f1: 0.0296 - val_loss: 0.0721 - val_precision: 0.7470 - val_recall: 0.6254\nEpoch 12/100\n3490/3490 ━━━━━━━━━━━━━━━━━━━━ 313s 89ms/step - auc: 0.9791 - f1: 0.0297 - loss: 0.0027 - precision: 0.8825 - recall: 0.8113 - val_auc: 0.9368 - val_f1: 0.0296 - val_loss: 0.0776 - val_precision: 0.7307 - val_recall: 0.6644\nEpoch 13/100\n3490/3490 ━━━━━━━━━━━━━━━━━━━━ 316s 90ms/step - auc: 0.9788 - f1: 0.0293 - loss: 0.0026 - precision: 0.8795 - recall: 0.8193 - val_auc: 0.9370 - val_f1: 0.0295 - val_loss: 0.0743 - val_precision: 0.7382 - val_recall: 0.6413\nEpoch 14/100\n3490/3490 ━━━━━━━━━━━━━━━━━━━━ 314s 89ms/step - auc: 0.9781 - f1: 0.0295 - loss: 0.0025 - precision: 0.8892 - recall: 0.8181 - val_auc: 0.9309 - val_f1: 0.0295 - val_loss: 0.0855 - val_precision: 0.7386 - val_recall: 0.6492\n\n\n\n\nCode\nmodel.save(\"/kaggle/working/\" + config.model)pd.DataFrame.from_dict(history_deeper_lstm_model.history).to_excel(\"/kaggle/working/\" + config.history)\n\n\n\n\nCode\nimport kerasfrom keras.models import load_modelmodel = load_model(filepath=\"/kaggle/working/\" + config.model)history = pd.read_excel(\"/kaggle/working/\" + config.history)\n\n\n\n\nCode\nconfig.model, config.history\n\n\n('model_f1.keras', 'lstm_model_f1.xlsx')\n\n\n\n\nCode\nvalidation = model.evaluate(  processed_val_ds.repeat(),  steps=validation_steps, # 748    verbose=1  )\n\n\n748/748 ━━━━━━━━━━━━━━━━━━━━ 87s 38ms/step - auc: 0.9542 - f1: 0.0287 - loss: 0.0529 - precision: 0.7858 - recall: 0.6763\n\n\n\n\nCode\npredictions = model.predict(processed_test_ds, verbose=1)\n\n\n748/748 ━━━━━━━━━━━━━━━━━━━━ 45s 41ms/step\n\n\n\n\nCode\nytrue = ytest.astype(int)y_pred_proba = predictionsoptimal_threshold_recall, best_score_recall = config.find_optimal_threshold_cv(ytrue, y_pred_proba, recall_score)# Use the optimal threshold to make predictionsfinal_predictions_recall = (y_pred_proba &gt;= optimal_threshold_recall).astype(int)\n\n\n\n\nCode\n# convert probability predictions to predictionsypred = predictions &gt;=  optimal_threshold_recall # .05ypred = ypred.astype(int)# create a plot with 3 by 2 subplotsfig, axes = plt.subplots(3, 2, figsize=(15, 15))axes = axes.flatten()mcm = multilabel_confusion_matrix(ytrue, ypred)# plot the confusion matrices for each labelfor i, (cm, label) in enumerate(zip(mcm, config.labels)):    disp = ConfusionMatrixDisplay(confusion_matrix=cm)    disp.plot(ax=axes[i], colorbar=False)    axes[i].set_title(f\"Confusion matrix for label: {label}\")plt.tight_layout()plt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\ncr = classification_report(  ytrue,  ypred,  target_names=config.labels,  digits=4  )print(cr)\n\n\n               precision    recall  f1-score   support\n\n        toxic     0.5524    0.8899    0.6817      2262\n severe_toxic     0.2355    0.9167    0.3748       240\n      obscene     0.5495    0.9359    0.6924      1263\n       threat     0.0366    0.4928    0.0681        69\n       insult     0.4712    0.9145    0.6219      1170\nidentity_hate     0.1164    0.7198    0.2004       207\n\n    micro avg     0.4164    0.8958    0.5685      5211\n    macro avg     0.3269    0.8116    0.4399      5211\n weighted avg     0.4947    0.8958    0.6295      5211\n  samples avg     0.0502    0.0848    0.0597      5211\n\n\n\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n\n\n\n\nCode"
  }
]