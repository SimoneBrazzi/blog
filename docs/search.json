[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Nice to meet you! My name is Simone Brazzi. I am 33 years old guy from Bologna, Italy.\nI decided to open this blog as a personal journal (what a new idea! Who ever thought about this?) of my data journey. This also means it is an opportunity to display my portfolio and shares my projects.\nAt the date of writing, I am a self made data professional. I am currently working as a Data Analyst/Engineer for UniCredit Group.\nI have too many hobbies, but I will try to summarize them in a sparse order:\n\nTraining: my 2024 goal is to be able to run at least an half marathon.\nStudy: I am studying data science to further improve my knowledge in the data field. I would like to do a Master Degree in the field, but it is difficult to combine the private life with an international MSc, at least for now.\nGaming, movies and manga: of course! I mean, it is part of being a nerd, right?! For cliche purpose, I love Quentin Tarantino blabbering, Nolan craziness, even though my favorite genre is horror. My favorites directors are Jordan Pelee and Ari Aster. I am currently reading Berserker, My Hero Academia, Jujutsu Kaisen, One Piece, Chainsaw Man and Kagurabachi.\nCooking, I think it is in my vein being italian (joking!) and considering my mother is a chef.\n\nFor now, I think I have annoyed you enough: stay tuned for future posts!"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Toxic Comment Filter\n\n\n\n\n\n\ncode\n\n\nDeep Learning\n\n\nPython, R\n\n\n\nBiLSTM model to make a multi label classification for a toxic comment filter\n\n\n\n\n\nAug 2, 2024\n\n\nSimone Brazzi\n\n\n\n\n\n\n\n\n\n\n\n\nEurostat Homicide Data\n\n\n\n\n\n\ncode\n\n\nshiny\n\n\nR\n\n\n\nA primer on Shiny to analyze gender differences in homicide rates\n\n\n\n\n\nDec 29, 2023\n\n\nSimone Brazzi\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/eurostat_homicide_rate/eurostat_homicide_rate.html",
    "href": "posts/eurostat_homicide_rate/eurostat_homicide_rate.html",
    "title": "Eurostat Homicide Data",
    "section": "",
    "text": "Hi there and welcome to my first project for the blog! The topic is a sad one, but I would like to explain why I decided to start with this. I was trying Shiny for different dashboards, but I wasn’t satisfied to learn using the classic examples. Unfortunately, italian crime news suffocated the public debate with a case of homicide, in which the victim is a young women. The public debate was focusing so baaaaaadly on the concept of femicide and the data, that I decided to clear the situation with a simple dashboard.\nFirst of all, at this link you can find the dashboard published using shinyapps.io. Also at this link you can find the Github repo. As you can see, even if it is the main branch, there are some details which are not uber perfect, but that don’t interfere with the code.\nNow lets jump into the detail of how to create a Shiny dashboard!"
  },
  {
    "objectID": "posts/eurostat_homicide_rate/eurostat_homicide_rate.html#global.r",
    "href": "posts/eurostat_homicide_rate/eurostat_homicide_rate.html#global.r",
    "title": "Eurostat Homicide Data",
    "section": "global.R",
    "text": "global.R\nAs said, this file is our usual R Script file. First thing first, we import our libraries:\n\nCode# wrangling\nlibrary(\"tidyverse\")\nlibrary(\"readr\")\nlibrary(\"stringr\")\nlibrary(\"dplyr\")\nlibrary(\"magrittr\")\nlibrary(\"forcats\")\nlibrary(\"lubridate\")\nlibrary(\"writexl\")\nlibrary(\"eurostat\")\n# plotting and dashboarding\nlibrary(\"shiny\")\nlibrary(\"shinythemes\")\nlibrary(\"ggplot2\")\nlibrary(\"plotly\")\nlibrary(\"scales\")\nlibrary(\"RColorBrewer\")\nlibrary(\"waiter\")\n# connecting and other\nlibrary(\"rsconnect\")\nlibrary(\"markdown\")\n\n\nLots of packages! The division is merely to remember how everything is managed and because I have OCD for this type of things.\nI want to focus on some packages:\n\ntidyverse, we all know it. As you can see, I also imported lots of single packages which compose the tidyverse, because I was getting errors of missing methods.\neurostat, which lets data flows from the eurostat website to my dashboard. This also lets the dashboard automatically updates when new data is available.\nscales, to nicely scaling my x and y axis.\nRColorBrewer, because I wanted to have a colorblind safe dashboard, even tough I am not.\nwaiter, for nice waiting images while the dashboard is loading.\n\nNow we can focus on the data importing and wrangling. For this, the eurostat library does the job. Lets focus on the crim_hom_vrel dataset.\n\nCode# search in eurostat db\nhomicide &lt;- search_eurostat(\"homicide\")\n\n# import data to variable\ncrim_hom_vrel &lt;- get_eurostat(\"crim_hom_vrel\", time_format = \"date\")\n\n# convert all observations to understandable data\ncrim_hom_vrel &lt;- label_eurostat(crim_hom_vrel)\n\n# label_eurostat_vars(crim_hom_vrel)\n\n# order data by country and date for time series purpose\ncrim_hom_vrel &lt;- crim_hom_vrel %&gt;% \n  arrange(geo, time)\n\ncrim_hom_vrel_grouped &lt;- crim_hom_vrel %&gt;% \n  dplyr::group_by(geo, time, sex, pers_cat, unit) %&gt;% \n  dplyr::summarise(values_grouped = sum(values), .groups = \"drop\") %&gt;% \n  filter(unit == \"Number\") %&gt;% \n  arrange(geo, time, sex)\n\n\nEverything pretty simple. I would like to highlight something about the dplyr::group_by and dplyr::summarise. As you can see, after having grouped and summarized, I need to drop the groups with the method .groups = \"drop\". With dplyr v.1.1.0 we can do the same with the help of the .by method in summarise.\n\nCodecrim_hom_vrel_grouped &lt;- crim_hom_vrel %&gt;% \n  dplyr::summarise(\n    values_grouped = sum(values),\n    .by = c(geo, time, sex, pers_cat, unit)\n    ) %&gt;% \n  filter(unit == \"Number\") %&gt;% \n  arrange(geo, time, sex)\n\n\nCopying from the dplyr website the differences between .by and group_by() are:\n\n\n.by\ngroup_by()\n\n\n\nGrouping only affects a single verb\nGrouping is persistent across multiple verbs\n\n\nSelects variables with tidy-select\n\nComputes expressions with data-masking\n\n\n\nSummaries use existing order of group keys\nSummaries sort group keys in ascending order\n\n\n\nLast part is all about colors.\n\nCode# brewer.pal(11, \"RdYlBu\")\npalette &lt;- c(\"#A50026\", \"#D73027\", \"#F46D43\", \"#FDAE61\", \"#FEE090\", \"#FFFFBF\", \"#E0F3F8\", \"#ABD9E9\", \"#74ADD1\", \"#4575B4\", \"#313695\")\n\npalette_crim_hom_vrel_grouped &lt;- rep(\n  palette,\n  length.out = crim_hom_vrel_grouped$geo %&gt;% str_unique() %&gt;% length()\n  )\n\n\nHere I defined the palette using ColorBrewer. Using rep I replicated the 11 colours for the length of the unique geo values."
  },
  {
    "objectID": "posts/eurostat_homicide_rate/eurostat_homicide_rate.html#ui.r",
    "href": "posts/eurostat_homicide_rate/eurostat_homicide_rate.html#ui.r",
    "title": "Eurostat Homicide Data",
    "section": "ui.R",
    "text": "ui.R"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Simone Brazzi",
    "section": "",
    "text": "I am a (data) nerd, with lot of passion and some mistake along the way. Data analyst by day, aspiring data wizard by night! I love using data to tell stories and drive business decisions. But I’m not content with stopping there. My passion for data and desire to expand my skillset has led me on a quest to become a data scientist. I’m a fearless problem solver with an insatiable curiosity, and I’m always seeking new challenges and opportunities to learn and grow. Let’s make some magic with data! When I don’t stare tolines of code or spreadsheet, I like to read and play video game or spend quality time with my dog."
  },
  {
    "objectID": "posts/toxic_comment_filter/toxic_comment_filter.html",
    "href": "posts/toxic_comment_filter/toxic_comment_filter.html",
    "title": "Toxic Comment Filter",
    "section": "",
    "text": "Costruire un modello in grado di filtrare i commenti degli utenti in base al grado di dannosità del linguaggio.\nPreprocessare il testo eliminando l’insieme di token che non danno contributo significativo a livello semantico.\nTrasformare il corpus testuale in sequenze.\nCostruire un modello di Deep Learning comprendente dei layer ricorrenti per un task di classificazione multilabel.\n\nIn prediction time, il modello deve ritornare un vettore contenente un 1 o uno 0 in corrispondenza di ogni label presente nel dataset (toxic, severe_toxic, obscene, threat, insult, identity_hate). In questo modo, un commento non dannoso sarà classificato da un vettore di soli 0 [0,0,0,0,0,0]. Al contrario, un commento pericoloso presenterà almeno un 1 tra le 6 labels."
  },
  {
    "objectID": "posts/toxic_comment_filter/toxic_comment_filter.html#import-r-libraries",
    "href": "posts/toxic_comment_filter/toxic_comment_filter.html#import-r-libraries",
    "title": "Toxic Comment Filter",
    "section": "Import R libraries",
    "text": "Import R libraries\nImport R libraries. These will be used for both the rendering of the document and data analysis. The reason is I prefer ggplot2 over matplotlib. I will also use colorblind safe palettes.\n\n\nCode\nlibrary(tidyverse, verbose = FALSE)\nlibrary(tidymodels, verbose = FALSE)\nlibrary(reticulate)\nlibrary(ggplot2)\nlibrary(plotly)\nlibrary(RColorBrewer)\nlibrary(bslib)\nlibrary(Metrics)\n\nreticulate::use_virtualenv(\"r-tf\")"
  },
  {
    "objectID": "posts/toxic_comment_filter/toxic_comment_filter.html#class-config",
    "href": "posts/toxic_comment_filter/toxic_comment_filter.html#class-config",
    "title": "Toxic Comment Filter",
    "section": "Class Config",
    "text": "Class Config\nI created a class with all the basic configuration of the model, to improve the readability.\n\n\nCode\nclass Config():\n    def __init__(self):\n        self.url = \"https://s3.eu-west-3.amazonaws.com/profession.ai/datasets/Filter_Toxic_Comments_dataset.csv\"\n        self.path = \"/Users/simonebrazzi/datasets/toxic_comment/Filter_Toxic_Comments_dataset.csv\"\n        self.max_tokens = 20000\n        self.output_sequence_length = 911 # check the analysis done to establish this value\n        self.embedding_dim = 128\n        self.batch_size = 32\n        self.epochs = 100\n        self.temp_split = 0.3\n        self.test_split = 0.5\n        self.random_state = 42\n        self.total_samples = 159571 # total train samples\n        self.train_samples = 111699\n        self.val_samples = 23936\n        self.features = 'comment_text'\n        self.labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n        self.new_labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate', \"clean\"]\n        self.label_mapping = {label: i for i, label in enumerate(self.labels)}\n        self.new_label_mapping = {label: i for i, label in enumerate(self.labels)}\n        self.test = \"_final\"\n        self.checkpoint_file = \"checkpoint.lstm_model\" + self.test + \".keras\"\n        self.history_file = \"lstm_model\" + self.test + \".xlsx\"\n        self.matrix_file = \"confusion_matrices\" + self.test + \".png\"\n        self.metrics = [\n            Precision(name='precision'),\n            Recall(name='recall'),\n            AUC(name='auc', multi_label=True, num_labels=len(self.labels))\n        ]\n    def get_early_stopping(self):\n        early_stopping = keras.callbacks.EarlyStopping(\n            monitor=\"val_recall\",\n            min_delta=0.2,\n            patience=10,\n            verbose=0,\n            mode=\"max\",\n            restore_best_weights=True,\n            start_from_epoch=3\n        )\n        return early_stopping\n\n    def get_model_checkpoint(self, filepath):\n        model_checkpoint = keras.callbacks.ModelCheckpoint(\n            filepath=filepath,\n            monitor=\"val_recall\",\n            verbose=0,\n            save_best_only=True,\n            save_weights_only=False,\n            mode=\"max\",\n            save_freq=\"epoch\"\n        )\n        return model_checkpoint\n    \n    def find_optimal_threshold_cv(self, ytrue, yproba, metric, thresholds=np.arange(.05, .35, .05), n_splits=7):\n      \n      # instantiate KFold\n      kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n      threshold_scores = []\n      \n      for threshold in thresholds:\n        \n        cv_scores = []\n        for train_index, val_index in kf.split(ytrue):\n          \n          ytrue_val = ytrue[val_index]\n          yproba_val = yproba[val_index]\n          \n          ypred_val = (yproba_val &gt;= threshold).astype(int)\n          score = metric(ytrue_val, ypred_val, average=\"micro\")\n          cv_scores.append(score)\n        \n        mean_score = np.mean(cv_scores)\n        threshold_scores.append((threshold, mean_score))\n        \n        # Find the threshold with the highest mean score\n        best_threshold, best_score = max(threshold_scores, key=lambda x: x[1])\n      return best_threshold, best_score\n      \nconfig = Config()"
  },
  {
    "objectID": "posts/toxic_comment_filter/toxic_comment_filter.html#import-python-packages",
    "href": "posts/toxic_comment_filter/toxic_comment_filter.html#import-python-packages",
    "title": "Toxic Comment Filter",
    "section": "Import Python packages",
    "text": "Import Python packages\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport keras\nimport keras_nlp\n\nfrom keras.backend import clear_session\nfrom keras.models import Model, load_model\nfrom keras.layers import TextVectorization, Input, Dense, Embedding, Dropout, GlobalAveragePooling1D, LSTM, Bidirectional, GlobalMaxPool1D, Flatten, LayerNormalization\nfrom keras.metrics import Precision, Recall, AUC\n\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.metrics import multilabel_confusion_matrix, classification_report, ConfusionMatrixDisplay, precision_recall_curve, f1_score, recall_score, roc_auc_score\n\n\nCreate a Config class to store all the useful parameters for the model and for the project."
  },
  {
    "objectID": "posts/toxic_comment_filter/toxic_comment_filter.html#eda",
    "href": "posts/toxic_comment_filter/toxic_comment_filter.html#eda",
    "title": "Toxic Comment Filter",
    "section": "EDA",
    "text": "EDA\nFirst a check on the dataset to find possible missing values and imbalances.\n\nFrequency\n\nCode\nlibrary(reticulate)\ndf_r &lt;- py$df\nnew_labels_r &lt;- py$config$new_labels\n\ndf_r_grouped &lt;- df_r %&gt;% \n  select(all_of(new_labels_r)) %&gt;%\n  pivot_longer(\n    cols = all_of(new_labels_r),\n    names_to = \"label\",\n    values_to = \"value\"\n  ) %&gt;% \n  group_by(label) %&gt;%\n  summarise(count = sum(value)) %&gt;% \n  mutate(freq = round(count / sum(count), 4))\n\ndf_r_grouped\n\n\n\n\nTable 2: Absolute and relative labels frequency\n\n\n\n# A tibble: 7 × 3\n  label          count   freq\n  &lt;chr&gt;          &lt;dbl&gt;  &lt;dbl&gt;\n1 clean         143346 0.803 \n2 identity_hate   1405 0.0079\n3 insult          7877 0.0441\n4 obscene         8449 0.0473\n5 severe_toxic    1595 0.0089\n6 threat           478 0.0027\n7 toxic          15294 0.0857\n\n\n\n\n\n\nBarchart\n\n\nCode\nlibrary(reticulate)\nbarchart &lt;- df_r_grouped %&gt;%\n  ggplot(aes(x = reorder(label, count), y = count, fill = label)) +\n  geom_col() +\n  labs(\n    x = \"Labels\",\n    y = \"Count\"\n  ) +\n  # sort bars in descending order\n  scale_x_discrete(limits = df_r_grouped$label[order(df_r_grouped$count, decreasing = TRUE)]) +\n  scale_fill_brewer(type = \"seq\", palette = \"RdYlBu\")\nggplotly(barchart)\n\n\n\n\n\n\n\n\nFigure 1: Imbalance in the dataset with clean variable\n\n\n\n\nIt is visible how much the dataset in imbalanced. This means it could be useful to check for the class weight and use this argument during the training.\nIt is clear that most of our text are clean. We are talking about 0.8033 of the observations which are clean. Only 0.1967 are toxic comments."
  },
  {
    "objectID": "posts/toxic_comment_filter/toxic_comment_filter.html#sequence-lenght-definition",
    "href": "posts/toxic_comment_filter/toxic_comment_filter.html#sequence-lenght-definition",
    "title": "Toxic Comment Filter",
    "section": "Sequence lenght definition",
    "text": "Sequence lenght definition\nTo convert the text in a useful input for a NN, it is necessary to use a TextVectorization layer. See the Section 4 section.\nOne of the method is output_sequence_length: to better define it, it is useful to analyze our text length. To simulate what the model we do, we are going to remove the punctuation and the new lines from the comments.\n\nSummary\n\nCode\nlibrary(reticulate)\ndf_r %&gt;% \n  mutate(\n    comment_text_clean = comment_text %&gt;%\n      tolower() %&gt;% \n      str_remove_all(\"[[:punct:]]\") %&gt;% \n      str_replace_all(\"\\n\", \" \"),\n    text_length = comment_text_clean %&gt;% str_count()\n    ) %&gt;% \n  pull(text_length) %&gt;% \n  summary() %&gt;% \n  as.list() %&gt;% \n  as_tibble()\n\n\n\n\nTable 3: Summary of text length\n\n\n\n# A tibble: 1 × 6\n   Min. `1st Qu.` Median  Mean `3rd Qu.`  Max.\n  &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1     4        91    196  378.       419  5000\n\n\n\n\n\n\nBoxplot\n\n\nCode\nlibrary(reticulate)\nboxplot &lt;- df_r %&gt;% \n  mutate(\n    comment_text_clean = comment_text %&gt;%\n      tolower() %&gt;% \n      str_remove_all(\"[[:punct:]]\") %&gt;% \n      str_replace_all(\"\\n\", \" \"),\n    text_length = comment_text_clean %&gt;% str_count()\n    ) %&gt;% \n  # pull(text_length) %&gt;% \n  ggplot(aes(y = text_length)) +\n  geom_boxplot() +\n  theme_minimal()\nggplotly(boxplot)\n\n\n\n\n\n\n\n\nFigure 2: Text length boxplot\n\n\n\n\n\n\nHistogram\n\n\nCode\nlibrary(reticulate)\ndf_ &lt;- df_r %&gt;% \n  mutate(\n    comment_text_clean = comment_text %&gt;%\n      tolower() %&gt;% \n      str_remove_all(\"[[:punct:]]\") %&gt;% \n      str_replace_all(\"\\n\", \" \"),\n    text_length = comment_text_clean %&gt;% str_count()\n  )\n\nQ1 &lt;- quantile(df_$text_length, 0.25)\nQ3 &lt;- quantile(df_$text_length, 0.75)\nIQR &lt;- Q3 - Q1\nupper_fence &lt;- as.integer(Q3 + 1.5 * IQR)\n\nhistogram &lt;- df_ %&gt;% \n  ggplot(aes(x = text_length)) +\n  geom_histogram(bins = 50) +\n  geom_vline(aes(xintercept = upper_fence), color = \"red\", linetype = \"dashed\", linewidth = 1) +\n  theme_minimal() +\n  xlab(\"Text Length\") +\n  ylab(\"Frequency\") +\n  xlim(0, max(df_$text_length, upper_fence))\nggplotly(histogram)\n\n\n\n\n\n\n\n\nFigure 3: Text length histogram with boxplot upper fence\n\n\n\n\nConsidering all the above analysis, I think a good starting value for the output_sequence_length is 911, the upper fence of the boxplot. In the last plot, it is the dashed red vertical line.. Doing so, we are removing the outliers, which are a small part of our dataset."
  },
  {
    "objectID": "posts/toxic_comment_filter/toxic_comment_filter.html#split-dataset",
    "href": "posts/toxic_comment_filter/toxic_comment_filter.html#split-dataset",
    "title": "Toxic Comment Filter",
    "section": "Split Dataset",
    "text": "Split Dataset\nNow we can split the dataset in 3: train, test and validation sets. Considering there is not a function in sklearn which lets split in these 3 sets, we can do the following: - split between a train and temporary set with a 0.3 split. - split the temporary set in 2 equal sized test and val sets.\n\n\nCode\nx = df[config.features].values\ny = df[config.labels].values\n\nxtrain, xtemp, ytrain, ytemp = train_test_split(\n  x,\n  y,\n  test_size=config.temp_split, # .3\n  random_state=config.random_state\n  )\nxtest, xval, ytest, yval = train_test_split(\n  xtemp,\n  ytemp,\n  test_size=config.test_split, # .5\n  random_state=config.random_state\n  )\n\n\nxtrain shape: py$xtrain.shape ytrain shape: py$ytrain.shape xtest shape: py$xtest.shape ytest shape: py$ytest.shape xval shape: py$xval.shape yval shape: py$yval.shape\nThe datasets are created using the tf.data.Dataset function. It creates a data input pipeline. The tf.data API makes it possible to handle large amounts of data, read from different data formats, and perform complex transformations. The tf.data.Dataset is an abstraction that represents a sequence of elements, in which each element consists of one or more components. Here each dataset is creates using from_tensor_slices. It create a tf.data.Dataset from a tuple (features, labels). .batch let us work in batches to improve performance, while .prefetch overlaps the preprocessing and model execution of a training step. While the model is executing training step s, the input pipeline is reading the data for step s+1. Check the documentation for further informations.\n\n\nCode\ntrain_ds = (\n    tf.data.Dataset\n    .from_tensor_slices((xtrain, ytrain))\n    .shuffle(xtrain.shape[0])\n    .batch(config.batch_size)\n    .prefetch(tf.data.experimental.AUTOTUNE)\n)\n\ntest_ds = (\n    tf.data.Dataset\n    .from_tensor_slices((xtest, ytest))\n    .batch(config.batch_size)\n    .prefetch(tf.data.experimental.AUTOTUNE)\n)\n\nval_ds = (\n    tf.data.Dataset\n    .from_tensor_slices((xval, yval))\n    .batch(config.batch_size)\n    .prefetch(tf.data.experimental.AUTOTUNE)\n)\n\n\n\n\nCode\nprint(\n  f\"train_ds cardinality: {train_ds.cardinality()}\\n\",\n  f\"val_ds cardinality: {val_ds.cardinality()}\\n\",\n  f\"test_ds cardinality: {test_ds.cardinality()}\\n\"\n  )\n\n\ntrain_ds cardinality: 3491\n val_ds cardinality: 748\n test_ds cardinality: 748\n\n\nCheck the first element of the dataset to be sure that the preprocessing is done correctly.\n\n\nCode\ntrain_ds.as_numpy_iterator().next()\n\n\n(array([b\"You see, they are colorcoded. Pink, for example, are animals and plants.  I din't know till now.\",\n       b\"Discussion is finished \\n\\nI'm done discussing the matter at hand. I am also done discussing things with the other user. I've been threatened by you twice about receiving sanctions, and yet another user can beat me down within the guidelines of personal attacks and nothing happens to them.  It's bad enough the other user puts high levels of stress and anxiety on me, but now the mediators are doing the same, by taking sides. This whole situation is very discouraging and disheartening!\",\n       b'\"The notion that people are actively trying to suppress this story from gaining national attention is absurd.  Sad as it may be, the story simply got filed to the later pages of the papers and small blurbs because other issues seemed more... \"\"newsworthy.\"\"\"',\n       b'. I will not engage in battleground attitude, personal attacks, long-term tendentious editing. I am willing to accept content and subject blocks as a condition that my block be lifted.',\n       b\"On reflection, you're right.\",\n       b\"Hold on a second here.  The link to e-ariana.com is not even original content.  This is the heading of the source article reads:\\n\\n International Herald Tribune\\n07/23/2007\\nBy Barry Bearak \\n\\nIf I'm not mistaken, that makes your makes your objection to the source void.  In addition, this Zahir Shah article itself cites a Barry Bearak article published in the New York Times.\",\n       b'\"\\nLet\\'s stop and think about how the Jimbo gif thing has gone since it started and maybe those of you who are voicing that I\\'m \"\"stubborn\"\" could rethink that as an unfair characterization: a admin comes here, tells me that he/she was going to review my block but now won\\'t because the Jimbo animation makes him/her nauseated and that I will never be taken seriously because it\\'s there.  Then, the same admin states after my reply that it\\'s essentially my fault that he/she didn\\'t review the block.  Think about how *you* would react to an attitude like that.  Now, consider the statements made by those who are obviously better at communicating what they really mean: \"\"it\\'s hard for me to read the page with it there\"\".  If that had been said in the first place, how would you have reacted to that instead?  I know that if that had been stated to me initially, I would have understood - because it\\'s a reasonable statement and observation, and I don\\'t want people to have problems reading the page - and had no problem removing the Jimbo gif from my page.  Snarkily saying that the Jimbo gif makes one either annoyed or \"\"nauseated\"\" and will keep others from taking me seriously, is neither helpful nor clear as to the real issue, is it?    \"',\n       b'(a) Because they reflect the last known positions of editors based on their actual participation in a related discussion, and (b) so that the scorecard reflects this historical debate.  You would prefer that I ignore the previous discussion on this topic?  Remember, WP:CCC but it seems appropriate to use as a starting point the previous level of consensus?',\n       b'\"\\n\\nHi Pterantula,\\n\\nThe encyclopedia entry requires attributions and references.  For example, the studies of the \"\"effects of Landmark Education\"\" is from the third-party surveys.  If, instead, the encyclopedia entry were to have customer testimonials instead, they could be struck on grounds they are not verifiable.  (We\\'ve gone around in circles on that many times as well.)\\n\\nAs well, the 20 points of critique are all against Landmark Education (with rebuttals in many cases).\\n\\nAs for litigation, I am aware of five outbound cases:\\n\\nPre-emptive warning\\n\\n1) Traci Hukill \"\"The Est of Friends\"\" (1998, pre-emptive warning, no suit filed)\\n\\nActual litigation, outbound\\n\\n Elle Magazine   (1998, LE achieved no result)\\n Cynthia Kisser  (1997, LE won retraction)\\n Margaret Singer (1997, LE won retraction)\\n Rick Ross       (2005, LE cancelled suit for product disparagement)\\n\\nThere are about three inbound cases (against LE):\\n Stefanie Ney (court ruled in LE\\'s favor)\\n Been versus Weed (2002; 10 claims, 3 settled, 7 dropped;  LE was a cross-defendent)\\n Tracy Neff (1997, sexual harrassment, settled out of court;  LE lacked a \"\"sexual harrassment\"\" policy at the time and got pulled in on this)\\n\\nDoes anyone have anything more than this from LE\\'s inception in 1991 (14 years ago)?  Litigation regarding \"\"est\"\" should be on the \"\"est\"\" page.   For a company with $70 million in revenue and over 800,000 customers of the Landmark Forum, this is a tiny amount of litigation.\\n\\n \"',\n       b'\"\\n\\n Challenge/Response and Backscatter \\n\\nOne anti-spam proposal has been the challenge/response type of system, which sends challenge messages to unknown senders of incoming mail and awaits a response before delivering a message.  Such systems are generally declared as failures and backscatter is the reason.  Concerning a legitimate message, the challenge would typically be sent to the sender.  However, for spam, this is often a forged address, so such a challenge \"\"raises the noise floor\"\" for its innocent recipient.  Although such a recipient will fail to reply, thus causing the originating spam to be discarded, one is effectively placing the decision into the hands of an unknown third party, who is annoyed by the unsolicited challenge message.\\n\\nFurthermore, for a challenge to be useful, it must somehow refer to the original message sent, and even include a (partial) copy of such.  When a C/R system does so, effectively it becomes a spam relay.  A spammer, knowing that a certain C/R system includes a copy, will design his message so that the spam payload is among the copied portion, and forge his desired recipient\\'s address as if he were the sender, thus allowing the C/R system to be the actual delivering server of the spam (as a challenge message payload) to his desired target.\\n\\nThe definition of backscatter as used by some individuals very well includes such challenges from C/R systems, especially those which can be tricked by spammers to deliver their message payloads.  Such challenges are not technically \"\"bounce messages\"\" as they are not NDRs but otherwise satisfy all the other common requirements to be considered backscatter (if not spam in its own right).\\n\\n- 71.106.213.194  \"',\n       b\"Ravana Sri Lanka and Sinahalese , neutrality \\nHe was a Sinhalese. Sri Lankan and  North Indian (Hindi), who wrote orginal Ramayana believes Ravan was Sinhalese ( The Hindu reference). \\nAs per Sri Lankan myths Ravana is very powerful , Raksha king lived in Sri Lanka who had two children named meganand and other but not mentioned any of Sita or Indian invasion. This page is needed to rewritten in neutral manner. Some Sri Lankan myths says Ravana was very powerful capable ruler and others couldn't fight with Ravana so they wrote a book in which they made him as a antagonist and reverted his real world victories by writing a novel. \\nI doesn't mean to remove any of myths written against him. But to mention Ravana's cruelty, raping women stories are taken from the hindu myth story Ramayanaya. Otherwise stating them as real facts will harm the image of a famous Sri Lankan hero. I will put neutrality tag for heavily based on Ramayanaya.\",\n       b'\":::::::::::Relevant policies: Failure or refusal to \"\"get the point\"\" and competence is required. Go argue with US Library of Congress Country Profile source as to why they have not included \"\"Cimerians, Persians, Assyrians, Akkadians, Phrygians (who were not Thracians), etc, etc.\"\" It\\'s also funny you want Persians mentioned in the lead since their invasion was like a blink of an eye, given the scope of history.   \\n\\n\"',\n       b'\"\\nI find it hard to believe that you didn\\'t know you write a verbatim copy of the website, but the content was \\n{{Unreferenced|date=December 2012}}\\n{{Infobox university\\n|name           = Darbhanga Medical College and Hospital \\n|native_name    = \\xe0\\xa4\\xa6\\xe0\\xa4\\xb0\\xe0\\xa4\\xad\\xe0\\xa4\\x82\\xe0\\xa4\\x97\\xe0\\xa4\\xbe \\xe0\\xa4\\xae\\xe0\\xa5\\x87\\xe0\\xa4\\xa1\\xe0\\xa4\\xbf\\xe0\\xa4\\x95\\xe0\\xa4\\xb2 \\xe0\\xa4\\x95\\xe0\\xa5\\x89\\xe0\\xa4\\xb2\\xe0\\xa5\\x87\\xe0\\xa4\\x9c \\xe0\\xa4\\x94\\xe0\\xa4\\xb0 \\xe0\\xa4\\x85\\xe0\\xa4\\xb8\\xe0\\xa5\\x8d\\xe0\\xa4\\xaa\\xe0\\xa4\\xa4\\xe0\\xa4\\xbe\\xe0\\xa4\\xb2\\n|image          = | \\n|established    = 1946\\n|type           = [[Public university|Public]] \\n|city           = [[Darbhanga]] \\n|country        = [[India]] \\n|students       =  \\n|staff          = \\n|campus         = [[urban area|Urban]]\\n}}\\'\\'\\'History\\'\\'\\' \\'\\'\\' http://www.darbhangamedicalcollege.in/aboutus.php{{dead link|date=December 2012}}\\n\\nthen the exact content of the body of http://www.darbhangamedicalcollege.in/aboutus.php and then\\n\\n==Courses Offered==\\nFollowing courses are offered:{{cite | url = http://mohfw.nic.in/ubihar.html{{dead link|date=December 2012}} | title = List of Recognised Medical Qualifications}}\\n* MBBS (90 seats)\\n* MD - Biochem, Forensic Med, Microbiol, Med, Paediatrics, Pathol, Pharmacol, Physiology, Preventive & Social Med, Radio-diagnosis, \\n* MS - Anaesthesiology, Anatomy, ENT Surgery, Obst & Gynae, Ophthal, Orthopaedics\\n* Diplomas - DA, DCH, DCP, DGO, DMRD, DOMS, DTMH, DLO \\n\\n==Address==\\nLaheriasarai, Bihar, India - 846003\\n\\n==References==\\n{{Reflist}}\\n\\n==External links==\\n*[http://institutions.education4india.com/3649/darbhanga-medical-college-laheriasarai-846003-darbhanga-bihar/ Educational Institutes of India ]\\n*[http://gov.bih.nic.in/Profile/Institutions.htm Educational Institutes of Bihar]\\n\\n{{coord missing|Bihar}}\\n\\n[[Category:Universities and colleges in Bihar]]\\n[[Category:Medical colleges in Bihar]]\"',\n       b'\"\\n\\nYour young age\\n\\nI\\'ve figured out what makes you so \"\"interested\"\" in that donkey punch article.  It\\'s your age.  You\\'re a minor.  A child, in other words.  You\\'re under one or both ages of majority in the United States, 18 or 21.  I suspect you are a high school student under 18.  Earlier, you claimed to be \"\"Steven Andrew Miller, a student living in Printer\\'s Row, Chicago.\"\"  Most university students advertise their university affiliations.  You didn\\'t.  Makes me think you\\'re a child.  So, just how young are you?  People should know whom their dealing with, so they don\\'t get in trouble for using profanity to a child or insulting a child or saying negative things to a child or whatever is illegal with children nowadays.  It\\'s problematic to deal with children on the internet, and you should put up a userbox indicating your age to avoid those problems.  \"',\n       b\"Well, it's now at FoxNews and the New York Post, and his publicist has made a statement concerning it, so is it now ready for the Article page, or are we going to play games that is doesn't exist? Naaaanaaanaaa I can't hear you, my fingers are in my ears! Honestly folks, it's a sad day when Wikipedia skews an article even when established respectable news outlets are publishing stories about it. Sad day indeed.\",\n       b'\":I wish you had done as I asked, when I said, \"\"...please just go ahead and block me for a week or two now\"\".  Thank you for at least doing so now; better late than never.  I won\\'t respond to the rest of your post, as clearly the 5 \"\"bad phrase\"\" examples and the block show I am not allowed to defend my actions, or point out improper actions of others, as when I do, I am blocked for it; you\\'ve clearly shown that I can\\'t do that so I won\\'t attempt it.  PS I am sure you don\\'t see this block as being done at my request; no need to point it out. (t\\xe2\\x80\\xa2c) \\n\\n\"',\n       b'So what? I mentioned private family holding and it is already well known as the picture Browne produces in his materials. You want to play this game. Very well. Watch this.',\n       b'\"\\n\\nWell, we now have a copyvio in the history of the temp page. The current version could still profit from some more rephrasing (and from including reformulated stuff from the second page from Medicinenet - click on the small \"\"2\"\" at the bottom, it\\'ll show you some info on treatments). It\\'s not ideal, but since we usually tolerate copyvios in the history, I think we might be fine. Alternatively, you could just copy your temp version offline, delete the temp page, and recreate it anew with the saved content. After all, you\\'re an admin, and I don\\'t think anybody would object to your temporarily deleting a temp page to get rid of copyvio in ther history.  22:42, 20 Dec 2004 (UTC)\"',\n       b'\"\\n\\n Troubling aspects of the artiicle as it exists right now \\n\\n1) I believe if Eric Altermann is to be accorded the prestige of being associated with msnbc.com, it should be noted that he is a blogger for msnbc.com. I say that because I found conservative columnist and author Michelle Malkin referred to as a blogger in another article and I do believe that\\'s meant to diminish the value of her statement. So, again, yes. We HAVE to see the forest for the trees. No one article in Wikipedia exists in a vaccum. There has to be a baseline standard and I reject any argumentation that suggests implies or directs that if \\'I don\\'t like that article, go change it.\\' Instead, one way or the other (and I don\\'t care which way) liberals and conservatives have to be characterized by the same fair measuring stick.\\n\\n2) This is new. It\\'s from Conason: \"\"Conason goes on to point out that Coulter\\'s critical nature is blunted by her pre-assumed opinions, making many of the conclusions she draws irrelevant to the actual nature of her arguments.\"\" Wow! That seems a little unfair just hanging out there like that. I propose either eliminating it or...and I hate this approach as much as the next guy...scotch-taping a quote after that which refutes this obvious cheap-shot.\\n\\n3) This is old. The McVeigh section, as should be no surprise to anyone who\\'s followed my work here, is troubling. Let\\'s start with this quote: \"\" She was willing to characterize members of the Branch Davidians at the Waco compound as \"\"harmless American citizens\"\" [6] even after the survivors of the raid and subsequent immolation of the group by their leader were convicted.\"\"\\nWell, some Waco compounders were convicted of something or another. But the fact is that there WERE mostly innocent victims of David Koresh in that compound. It\\'s unbalanced to just add the tag line about conviced Davidians without acknowledging what I GUARANTEE you Ann was referring to, which were the overwhelming number that were innocent. \\n\\nThis quote is also troubling: \"\"Timothy McVeigh made similar criticism as partial justification for his bombing.\"\" Well, McVeigh had LOTS of reasons for his bombing. But who cares what a madman thinks? And if you do, then I understand he also had some uncomplimentary things to say about the Bible. Therefore, if we are to include this in Coulter\\'s section, I say we include it\\'s compelement in Bill Maher\\'s article. It would go something like this:\\n\"\"Naher has frequently criticized the Bible calling it a \\'bunch of fairy tales\\' and claiming \\'God doesn\\'t write books.\\'  Oklahoma City bomber Timothy McVeigh made similar criticism as partial justification for his distrust of Christianity, though the Bible has been widely criticized throughout history.\"\"\\n\\nWhady\\'a say?\\n\\n(Much more on this section later.)\\n\\n4) Racism is not the same as being anti-Islamic terrorists that want to kill Americans for supporting Israel etc. To title Ann\\'s comments on Islamic Radicals as allegations of racism seems dishonest to me.\\n\\n5) Did Ann really say that ALL  women are \"\"not as bright\"\" as men\\'?\\nSome editor in Wikipedia wants us to think so. I say this quote be taken out until we get confirmation of her whole statement in context. Now, I know for a fact that Ann is only half-joking when she argues women should not be allowed to vote so she is far from imune when it comes to suggesting she holds less-than-boilerplate-feminist views. But this quote supposedly out of H & C seems like it\\'s probably another smear. Again, if proven wrong, I\\'ll gladly man up and stand corrected. But, unitl I\\'m proven it wrong, it should be excised in my view. \\n\\nMore later... \"',\n       b'\"\\n\\nGeorge, you are taking the role of the Arbitration Committee, all by your self. The community already decided that my ArbCom case page should stand, you step beyond your powers by \"\"ruling\"\" it disruptive.\\n\\nMy ArbCom elections participation referred to an ANI in which I was admonished, yet my asking \"\"any general concerns by the five days of silence from admins in this ANI\"\" in a question to candidates is taken as, \"\"purpose of causing trouble for people Harry has disagreed with\"\", \"\"veiled personal attack\"\" and \"\"some kind of agenda behind them\"\". It\\'s almost as if the admin community had some vestigial shame on how that ANI was handled, why else suppress it by removing the questions from the candidates pages?\\n\\nI have been advised not to make edits to articles if I wish my ArbCom case page to be regarded as timeous, then you say I\\'m, \"\"not even trying to build articles anymore\"\". However I regarded ArbCom elections, and editing WP:AGF - , ,  as exceptions. As was my WP:AGF talk page participation.\\n\\nIf I was going to re-write the satire,  I would satirise the admin behaviour it was aimed at by comparison to the mindset of the religious police of the Iranian state.\\n\\nThe religious police are to Islam and the Koran as some admins are to Wikipedia and it\\'s Policies and Guidelines.\\n\\nYou can draw further parallels with conservative and liberal wings among admins, but no one likes an extremest.\\n\\nIf you don\\'t like that comparison then compare them with the mindset of US soldiers at Abu Ghraib.\\n\\nOver-the-top comparisons? Remember I\\'m talking about mindset here; but given the chance, how many admins would you expect to behave as abhorrently as the religious police? Now take that answer and multiply by three. If the product is greater than zero, then you must have thought of at least one admin who you would expect to abuse others human rights if they had real powers in the real world. Don\\'t you think that these admins should have the tools gently removed from their hands?\\n\\nPart of the humour of the  was the alternative spelling of Constable to Cunstable - not very subtle humour & a very minor part near the end, but obviously intended as humour, and read with good faith can only be regarded as such - however this was used as a pretext to speedy delete after 5 hours in WP:MFD.  was put up with the Brainstorming template from the first, indicating an appeal for participation in it\\'s production and alteration, as someone did when it was in WP:Namespace. If the cited reason of \"\"calling admins cunts by typographic means\"\" was the only objection, then it would have been quite easy to participate and call for those changes. Instead of participation, admins afraid of satire, suppressed the satire rather than examine the underlying concern the piece was obviously addressing - WP:religious police, another satire anyone?\\n\\nI would remind you of the story of Dickens being sued by some gentlemen over Oliver Twist, citing that the book used their person as the basis for the poorhouse character who refused Oliver\\'s request for more (despite it being politely put; I know how he feels), the judge summed up that anyone who saw themselves in that character wasn\\'t up to much (I paraphrase). If you felt you were being targeted by my satire, then I would ask which character from Oliver you think the general readership would regard you as.\\n\\nI trust that my useful, and still standing, contributions to WP:AGF, and for the other rebuttals of your given reasons for the block, you will change your view to one which I hold myself - I am here to build an encyclopaedia; and lift the block.\\n\\nFinally I would say, all that is required for evil to flourish is for good Wikipedians to do nothing.   18:0',\n       b\"I haven't been following the conversation too much, but I'm assuming he's referring to me and my comments I made on this very talk page to him asking him to cut out the sockpuppet crap? It's difficult to tell...because your comments are the only ones on here, it makes it look very one-sided. At any rate, I'm not who he claims me to be, and I certainly do believe he has exhibited negative behavior both in past and present incidents such as this one.\",\n       b'Hmmm... just my two cents, but I find it quite bizarre that no fewer than three administrators (',\n       b\"HELLO Liberal MAGGOT Part TWO\\nCensorship will not make me go away... thanks to you, Rob, I am now using my influence, to undermine the crediablity of Wikipedidiots, like you, and Wikipedia.  To bad you don't want to acknowledge the fact that you and your kind are ignorant, knuckle dragging,CyberThugs. Your shadow Joe.\",\n       b'\"\\n\\n A barnstar for you! \\n\\n  The Original Barnstar Thanks for all the standardization work you\\'ve been doing recently, on IPA templates. (And from a glance at various archives, thanks also for many years of work improving the templates themselves!) \\xe2\\x80\\x93  \"',\n       b'Simon Boliva over claimed \\n\\nMany historians have noted that Simon boliva Deliberately claimed additional land for this mountain country even though he knew bolivia will always be a land of mountain people, why is this not mention in the article, its historical facts!!!!',\n       b', 22 July 2008 (UTC)\\n\\nYeah, nice to take a back seat for a while.   22:21',\n       b'\"\\n\\nRose\\'s surname should be capitalized in the page title (\"\"Phyllis rose\"\"). I can\\'t figure out how to edit that   \"',\n       b\"Could you help me, perhaps?  I'm the Finkbeiner of the Finkbeiner test.  I'd this article to link to my original post.  Right now, it links to a re-post of my post on another site.  This subject has gotten a lot of attention and I'm happy for Christie Aschwanden and Double XX Science to get most of it.  But would you mind adding this link? http://www.lastwordonnothing.com/2013/01/17/5266/  The title of the original post is What I'm Not Going To Do.\\n\\nI tried to do this myself but my html skills are rudimentary.  As is, not even there.\\n\\nThank you for considering it.\\n\\nAnn Finkbeiner\",\n       b'\"\\n\\n Article Absolutely Neutral \\nThe article is Absolutely Neutral and it is not a \"\"point of view\"\" but absolute facts and references have been cited.User:Vaishnava\"',\n       b\"Doug, \\nto doug\\nYou'll find on a second reading that the edit is not an essay but disproves the Original Research as portrayed in the article attempting to masquerade as fact. The source texts are peer-reviewed external third-party books, if you'd care to re-check. Thus the Disputation stands the test of Wikipedia edit requirements as it not original research, but, the required rebuttal of Original Research in order for an article which does not meet any criteria of POV Neutrality and research that diverges from academic mainstream discourse. Additionally, the links I use add to Wikipedia in the framework of cross-linking to related articles.\\nFrankly, if more articles were written as thoroughly as the Disputation alone- Wikipedia would not be a steaming pile of politicised horse manure and would not be ridiculed a d banned from even primary and secondary school referencing (as per UK, Australia and Indonesia school curriculau)\",\n       b'\"\\n\\noh please, I think my arguments need a better response than just you calling for ceasing accusations.Its just that you dont even have an answer and you know exactly what you meant when you said that I am trying to make someone \"\"LOOK\"\" european. what do you mean by concentrating on the topic? I said enough about the topic. we BOTH agreed that borders are not established yet, that is why putting them in several regions but europe is uaaceptable.  It either needs to be in Asian section as it is now already, or in the western asian section where it is now already as well, or the european section where it is listed as western asian or it does not have to be there at all.it  is everywjere but in europe even though borders are not official yet? WHY do you think it is very easy for you to put them in asia, or western asia and not europe even though the borders are not established ? why do you think those people and me( I can imagine how big you think of you as european)  are trying to \"\"LOOK\"\" and be european? what gives you the right to give that supercilious evaluation ? you are BIASED , thats the only explanation.YES, I am accusing you and I am accusing you of being a racist and being a paranoid that thinks that everyone is trying to be European and HUSOND is a brave person who does everything to protect them from \"\"Unnessesary\"\" people and people who he thinks are LESS than him personally because he is european. Europe is like a priviledged section where countries can be put thats why its hard for them to be there and not hard to be in middle east even though the article itself says that first european homo georgicus was found in georgia.next time if you will bother to write a response, I urge you to adress all these REAL issues instead of calling for ceasing accusations.    \"',\n       b'\"\\nDear , you say that in RWP there are \"\"Many small unsignificant parties are claiming as \"\"the official representative\"\", \"\"president\"\", etc.\"\" It\\'s not right. After the National Congress III (few days ago), some leaders of these parties were jailed by Indonesian government. These organizations were mostly closed after getting their politicians jailed. The only parties left were WPLO (I am representative of it in Russia), KNPB and WPNLA. And all these parties made a coalition at 25th October. So, now West Papuan separatism is much more united and serious than it was before.   \"'],\n      dtype=object), array([[0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [1, 0, 0, 0, 1, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0]]))\n\n\nAnd we check also the shape. We expect a feature of shape (batch, ) and a target of shape (batch, number of labels).\n\n\nCode\nprint(\n  f\"text train shape: {train_ds.as_numpy_iterator().next()[0].shape}\\n\",\n  f\" text train type: {train_ds.as_numpy_iterator().next()[0].dtype}\\n\",\n  f\"label train shape: {train_ds.as_numpy_iterator().next()[1].shape}\\n\",\n  f\"label train type: {train_ds.as_numpy_iterator().next()[1].dtype}\\n\"\n  )\n\n\ntext train shape: (32,)\n  text train type: object\n label train shape: (32, 6)\n label train type: int64"
  },
  {
    "objectID": "posts/toxic_comment_filter/toxic_comment_filter.html#definition",
    "href": "posts/toxic_comment_filter/toxic_comment_filter.html#definition",
    "title": "Toxic Comment Filter",
    "section": "Definition",
    "text": "Definition\nDefine the model using the Functional API.\n\n\nCode\ndef get_deeper_lstm_model():\n    clear_session()\n    inputs = Input(shape=(None,), dtype=tf.int64, name=\"inputs\")\n    embedding = Embedding(\n        input_dim=config.max_tokens,\n        output_dim=config.embedding_dim,\n        mask_zero=True,\n        name=\"embedding\"\n    )(inputs)\n    x = Bidirectional(LSTM(256, return_sequences=True, name=\"bilstm_1\"))(embedding)\n    x = Bidirectional(LSTM(128, return_sequences=True, name=\"bilstm_2\"))(x)\n    # Global average pooling\n    x = GlobalAveragePooling1D()(x)\n    # Add regularization\n    x = Dropout(0.3)(x)\n    x = Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n    x = LayerNormalization()(x)\n    outputs = Dense(len(config.labels), activation='sigmoid', name=\"outputs\")(x)\n    model = Model(inputs, outputs)\n    model.compile(optimizer='adam', loss=\"binary_crossentropy\", metrics=config.metrics, steps_per_execution=32)\n    \n    return model\n\nlstm_model = get_deeper_lstm_model()\nlstm_model.summary()\n\n\nModel: \"functional_1\"\n┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)        ┃ Output Shape      ┃    Param # ┃ Connected to      ┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ inputs (InputLayer) │ (None, None)      │          0 │ -                 │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ embedding           │ (None, None, 128) │  2,560,000 │ inputs[0][0]      │\n│ (Embedding)         │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ not_equal           │ (None, None)      │          0 │ inputs[0][0]      │\n│ (NotEqual)          │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ bidirectional       │ (None, None, 512) │    788,480 │ embedding[0][0],  │\n│ (Bidirectional)     │                   │            │ not_equal[0][0]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ bidirectional_1     │ (None, None, 256) │    656,384 │ bidirectional[0]… │\n│ (Bidirectional)     │                   │            │ not_equal[0][0]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ global_average_poo… │ (None, 256)       │          0 │ bidirectional_1[… │\n│ (GlobalAveragePool… │                   │            │ not_equal[0][0]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout (Dropout)   │ (None, 256)       │          0 │ global_average_p… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense (Dense)       │ (None, 64)        │     16,448 │ dropout[0][0]     │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalization │ (None, 64)        │        128 │ dense[0][0]       │\n│ (LayerNormalizatio… │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ outputs (Dense)     │ (None, 6)         │        390 │ layer_normalizat… │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n Total params: 4,021,830 (15.34 MB)\n Trainable params: 4,021,830 (15.34 MB)\n Non-trainable params: 0 (0.00 B)"
  },
  {
    "objectID": "posts/toxic_comment_filter/toxic_comment_filter.html#callbacks",
    "href": "posts/toxic_comment_filter/toxic_comment_filter.html#callbacks",
    "title": "Toxic Comment Filter",
    "section": "Callbacks",
    "text": "Callbacks\nFinally, the model has been trained using 2 callbacks: - Early Stopping, to avoid to consume the kaggle GPU time. - Model Checkpoint, to retrieve model training information.\n\n\nCode\n# callbacks\nmy_es = config.get_early_stopping()\nmy_mc = config.get_model_checkpoint(filepath=\"/kaggle/working/checkpoint.keras\")\ncallbacks = [my_es, my_mc]"
  },
  {
    "objectID": "posts/toxic_comment_filter/toxic_comment_filter.html#final-preparation-before-fit",
    "href": "posts/toxic_comment_filter/toxic_comment_filter.html#final-preparation-before-fit",
    "title": "Toxic Comment Filter",
    "section": "Final preparation before fit",
    "text": "Final preparation before fit\nConsidering the dataset is imbalanced, to increase the performance we need to calculate the class weight. This will be passed during the training of the model.\n\n\nCode\nlab = pd.DataFrame(columns=config.labels, data=ytrain)\nr = lab.sum() / len(ytrain)\nclass_weight = dict(zip(range(len(config.labels)), r))\nclass_weight\n\n\n{0: 0.09590058997842416, 1: 0.00992846847330773, 2: 0.05275785817240978, 3: 0.003061800016114737, 4: 0.04913204236385285, 5: 0.008710910572162688}\n\n\nIt is also useful to define the steps per epoch for train and validation dataset. This step is required to avoid\n\n\nCode\nsteps_per_epoch = config.train_samples // config.batch_size\nvalidation_steps = config.val_samples // config.batch_size"
  },
  {
    "objectID": "posts/toxic_comment_filter/toxic_comment_filter.html#fit",
    "href": "posts/toxic_comment_filter/toxic_comment_filter.html#fit",
    "title": "Toxic Comment Filter",
    "section": "Fit",
    "text": "Fit\nThe fit has been done on Kaggle to levarage the GPU. Some considerations about the model:\n\n.repeat() ensure the model sees all the daataset.\nepocs is set to 100.\nvalidation_data has the same repeat.\ncallbacks are the one defined before.\nclass_weight ensure the model is trained using the frequency of each class, because our dataset is imbalanced.\nsteps_per_epoch and validation_steps depend on the use of repeat.\n\n\n\nCode\nhistory_deeper_lstm_model = model.fit(\n  processed_train_ds.repeat(),\n  epochs=config.epochs,\n  validation_data=processed_val_ds.repeat(),\n  callbacks=callbacks,\n  class_weight=class_weight,\n  steps_per_epoch=steps_per_epoch,\n  validation_steps=validation_steps\n  )\n\n\nNow we can import the model and the history trained on Kaggle.\n\n\nCode\nimport keras\nfrom keras.models import load_model\n\nmodel = load_model(filepath=\"/Users/simonebrazzi/R/professionAI_deep_learning/Progetto_Finale/history/model.keras\")\n\nhistory = pd.read_excel(\"~/R/professionAI_deep_learning/Progetto_Finale/history/deep_lstm_model.xlsx\")"
  },
  {
    "objectID": "posts/toxic_comment_filter/toxic_comment_filter.html#evaluate",
    "href": "posts/toxic_comment_filter/toxic_comment_filter.html#evaluate",
    "title": "Toxic Comment Filter",
    "section": "Evaluate",
    "text": "Evaluate\n\n\nCode\nvalidation = model.evaluate(\n  processed_val_ds.repeat(),\n  steps=validation_steps, # 748\n  verbose=0\n  )\n\n\n\nCode\ntibble(\n  metric = c(\"loss\", \"precision\", \"recall\", \"auc\"),\n  value = py$validation\n  )\n\n\n\n\nTable 4: Model validation metric\n\n\n\n# A tibble: 4 × 2\n  metric     value\n  &lt;chr&gt;      &lt;dbl&gt;\n1 loss      0.0875\n2 precision 0.666 \n3 recall    0.701 \n4 auc       0.945"
  },
  {
    "objectID": "posts/toxic_comment_filter/toxic_comment_filter.html#predict",
    "href": "posts/toxic_comment_filter/toxic_comment_filter.html#predict",
    "title": "Toxic Comment Filter",
    "section": "Predict",
    "text": "Predict\nFor the prediction, the model does not need to repeat the dataset, because the model has already been trained and now it has just to consume the data to make the prediction.\n\n\nCode\npredictions = model.predict(processed_test_ds, verbose=0)"
  },
  {
    "objectID": "posts/toxic_comment_filter/toxic_comment_filter.html#confusion-matrix",
    "href": "posts/toxic_comment_filter/toxic_comment_filter.html#confusion-matrix",
    "title": "Toxic Comment Filter",
    "section": "Confusion Matrix",
    "text": "Confusion Matrix\nThe best way to assess the performance of a multi label classification is using a confusion matrix. Sklearn has a specific function to create a multi label classification matrix to handle the fact that there could be multiple labels for one prediction.\n\nGrid Search Cross Validation for best threshold\nGrid Search CV is a technique for fine-tuning hyperparameter of a ML model. It systematically search through a set of hyperparamenter values to find the combination which led to the best model performance. In this case, I am using a KFold Cross Validation is a resempling technique to split the data into k consecutive folds. Each fold is used once as a validation while the k - 1 remaining folds are the training set. See the documentation for more information.\nThe model is trained to optimize the recall. The decision was made because the cost of missing a True Positive is greater than a False Positive. In this case, missing a injurious observation is worst than classifying a clean one as bad.\nHaving said this, I still want to test different metrics other than the recall_score to have more possibility of decision of the best threshold.\n\nf1_score\n\n\nCode\nytrue = ytest.astype(int)\ny_pred_proba = predictions\noptimal_threshold_f1, best_score_f1 = config.find_optimal_threshold_cv(ytrue, y_pred_proba, f1_score)\n\nprint(f\"Optimal threshold: {optimal_threshold_f1}\")\nprint(f\"Best score: {best_score_f1}\")\n\n# Use the optimal threshold to make predictions\nfinal_predictions_f1 = (y_pred_proba &gt;= optimal_threshold_f1).astype(int)\n\n\n\n\nrecall_score\n\n\nCode\nytrue = ytest.astype(int)\ny_pred_proba = predictions\noptimal_threshold_recall, best_score_recall = config.find_optimal_threshold_cv(ytrue, y_pred_proba, recall_score)\n\n# Use the optimal threshold to make predictions\nfinal_predictions_recall = (y_pred_proba &gt;= optimal_threshold_recall).astype(int)\n\n\nOptimal threshold recall: 0.05. Best score: 0.8647006.\n\n\nroc_auc_score\n\n\nCode\nytrue = ytest.astype(int)\ny_pred_proba = predictions\noptimal_threshold_roc, best_score_roc = config.find_optimal_threshold_cv(ytrue, y_pred_proba, roc_auc_score)\n\nprint(f\"Optimal threshold: {optimal_threshold_roc}\")\nprint(f\"Best score: {best_score_roc}\")\n\n# Use the optimal threshold to make predictions\nfinal_predictions_roc = (y_pred_proba &gt;= optimal_threshold_roc).astype(int)\n\n\n\n\n\nConfusion Matrix Plot\nThe confusion matrix is plotted using the multilabel_confusion_matrix function in scikit-learn. We have to plot a confusion matrix for each label. To plot the confusion matrix, we need to convert the predicted probability of a label to a proper prediction. To do so, we use the calculated optimal threshold for the recall, which is 0.05. The confusion matrix plotted hete, considering we have a multi label task, is not a big one with all the labels as columns and indices. We plot a confusion matrix for each label with a simple for loop, which extract for each loop the confusion matrix and the associated label.\n\n\nCode\n# convert probability predictions to predictions\nypred = predictions &gt;=  optimal_threshold_recall # .05\nypred = ypred.astype(int)\n\n# create a plot with 3 by 2 subplots\nfig, axes = plt.subplots(3, 2, figsize=(15, 15))\naxes = axes.flatten()\nmcm = multilabel_confusion_matrix(ytrue, ypred)\n# plot the confusion matrices for each label\nfor i, (cm, label) in enumerate(zip(mcm, config.labels)):\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n    disp.plot(ax=axes[i], colorbar=False)\n    axes[i].set_title(f\"Confusion matrix for label: {label}\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 4: Multi Label Confusion matrix"
  },
  {
    "objectID": "posts/toxic_comment_filter/toxic_comment_filter.html#dataset",
    "href": "posts/toxic_comment_filter/toxic_comment_filter.html#dataset",
    "title": "Toxic Comment Filter",
    "section": "Dataset",
    "text": "Dataset\nNow we can split the dataset in 3: train, test and validation sets. Considering there is not a function in sklearn which lets split in these 3 sets, we can do the following: - split between a train and temporary set with a 0.3 split. - split the temporary set in 2 equal sized test and val sets.\n\n\nCode\nx = df[config.features].values\ny = df[config.labels].values\n\nxtrain, xtemp, ytrain, ytemp = train_test_split(\n  x,\n  y,\n  test_size=config.temp_split, # .3\n  random_state=config.random_state\n  )\nxtest, xval, ytest, yval = train_test_split(\n  xtemp,\n  ytemp,\n  test_size=config.test_split, # .5\n  random_state=config.random_state\n  )\n\n\nxtrain shape: py$xtrain.shape ytrain shape: py$ytrain.shape xtest shape: py$xtest.shape ytest shape: py$ytest.shape xval shape: py$xval.shape yval shape: py$yval.shape\nThe datasets are created using the tf.data.Dataset function. It creates a data input pipeline. The tf.data API makes it possible to handle large amounts of data, read from different data formats, and perform complex transformations. The tf.data.Dataset is an abstraction that represents a sequence of elements, in which each element consists of one or more components. Here each dataset is creates using from_tensor_slices. It create a tf.data.Dataset from a tuple (features, labels). .batch let us work in batches to improve performance, while .prefetch overlaps the preprocessing and model execution of a training step. While the model is executing training step s, the input pipeline is reading the data for step s+1. Check the documentation for further informations.\n\n\nCode\ntrain_ds = (\n    tf.data.Dataset\n    .from_tensor_slices((xtrain, ytrain))\n    .shuffle(xtrain.shape[0])\n    .batch(config.batch_size)\n    .prefetch(tf.data.experimental.AUTOTUNE)\n)\n\ntest_ds = (\n    tf.data.Dataset\n    .from_tensor_slices((xtest, ytest))\n    .batch(config.batch_size)\n    .prefetch(tf.data.experimental.AUTOTUNE)\n)\n\nval_ds = (\n    tf.data.Dataset\n    .from_tensor_slices((xval, yval))\n    .batch(config.batch_size)\n    .prefetch(tf.data.experimental.AUTOTUNE)\n)\n\n\n\n\nCode\nprint(\n  f\"train_ds cardinality: {train_ds.cardinality()}\\n\",\n  f\"val_ds cardinality: {val_ds.cardinality()}\\n\",\n  f\"test_ds cardinality: {test_ds.cardinality()}\\n\"\n  )\n\n\ntrain_ds cardinality: 3491\n val_ds cardinality: 748\n test_ds cardinality: 748\n\n\nCheck the first element of the dataset to be sure that the preprocessing is done correctly.\n\n\nCode\ntrain_ds.as_numpy_iterator().next()\n\n\n(array([b'Neil, as per your advice, I responded to all the charges in detail, and the result is I am blocked for a week ? Can you comment on this ?',\n       b'\"Please stop. If you continue to vandalize pages, }} you will be blocked from editing Wikipedia.   \\xe2\\x99\\xa3   Chat wit\\' me  \\xc2\\xa7  Contributions \\xe2\\x99\\xa3 \"',\n       b\"CAR\\nI don't like re-adding the Central African Republic to this list because it seems to contradict the information in the actual article. If this war really hasn't ended, we should change that article as well. Do you have a source that explicitly states this war hasn't ended?\",\n       b'Thank you Betacommand. Much obliged.',\n       b'\"\\nHi. Sorry I tagged the batch - I should have spotted that. I\\'ll deal with them myself later (if others haven\\'t). Cheers, \\xc2\\xa0\\xe2\\x96\\xba\\xc2\\xa0 \"',\n       b\"OMG, I'm goin' crazy. When I've used IP of anyone o_0 ? Say me it's just a bad dream (o maybe, that I'm just an idiot -))) )... A desperate\",\n       b'same as [NPHS2] time for a merger',\n       b'hello \\n\\ni was interested in this topic sicne i have had it recently but i saw that this is almost the same inforation that is on a real phabdomyolysis site on the internet wrote by medical people and its not verry hepful.\\n\\nthank,',\n       b\"Alright, anime and manga have almost nothing to do with this except actually using this little trick, they never made it, and this is probably some little anime nerd going in and adding useless crap about the series that makes him squeal like a girl the most.\\n\\nWhen i think hammer space, i think of warner brother's cartoons, i don't think of anime.\\n\\nbillions of more people know about warner brothers and know about bugs bunny and company, adding anything to do with anime and manga is unnecessary\",\n       b'I appologise, that comment was made by my facile younger brother',\n       b'Thank you \\n\\nFor the spelling/grammar edits.',\n       b'\"\\n\\n Thanks! \\n\\nThanks very much for the unblock, \\'preciate it.  I actually asked a while ago for this IP to be softblocked for a while so something like this wouldn\\'t happen, \\'cause I kinda knew it was coming.  Personally think it should be longer than it is, \\'cause there\\'s a lot of people that use this IP, and the vandalism is just gonna continue every time the block expires.  Is there such this as a softblock for an indefinite amount of time?   T/C \"',\n       b\"I have a little dreidel, I made it out of clay, And when it's dry and ready, Then dreidel I shall play!\\nOh dreidel, dreidel, dreidel, I made it out of clay; Oh dreidel, dreidel, dreidel, Then dreidel I shall play.\\nIt has a lovely body, With leg so short and thin, And when it gets all tired, It drops and then I win!\\nOh dreidel, dreidel, dreidel, With leg so short and thin, Oh dreidel, dreidel, dreidel, It drops and then I win!\",\n       b\"Please! We are on this together! I haven't been able to have children yet because I want to make sure nothing is going to happen to them in the future. We need to clean-cut the irresponsibility of other peoples mistakes from past history so that we never ever have 1945 again. Remember 1945.  (  )\",\n       b'But there are lots of editors like that.',\n       b'I believe my Article was notable enough this time \\n\\nHello. I believe my Article was notable enough this time however it is deleted without giving me any satisfactory explanation. The procedure i have followed: Make draft, join chat and spend 1 whole day to edit and compose excellent article based on suggestion and edits by experts at the chat, submit draft, draft accepted, draft reviewed and edited by WikiProject_Video_games editor and completely published. Then i ask chat again about isn\\xe2\\x80\\x99t this too much edit? then primefac opens speedy delete then it is deleted without giving me any explanation in matter of minutes. If you check the issue i appreciate ty very much  https://en.wikipedia.org/wiki/MonsterMMORPG . And there were not any discussion it was deleted immediately. One more notice: I checked same genre games articles and majority of them have way more less authority references and even some have 0 references. Thank you very much for your help. I believe at the very least it should have been debated.',\n       b'\"\\n Palmisano playing for Iowa State in the 1970s is a free pass, I think; I\\'m pretty sure they were Division I then.  As far as a list of coaches go, I\\'m not sure that\\'s an articleworthy list at that level of competition (as opposed to it being folded into a general Malone College Athletics article), but I wouldn\\'t file an AfD over it; it\\'s a compromise, anyway.  Seeing as you\\'re digging into uncovering notability for those folks, want a full week for it?   \"',\n       b'Hi, I was talking to the user you believe is a sock puppet and I think he is not a new user. If I were you, I would keep an eye on the user (he/she was recently on the talk page for Freddie Gray). There were also two anonymous  users causing a disruption, but they are probably unrelated to Dracula918. Thanks for your diligence as you already found HydroFerocity on the Gray article.',\n       b'Well, the Olympics are one thing, but world championships seem different, see Medalists at the World Figure Skating Championships, ]]:Category:Medalists at the World Artistic Gymnastics Championships]], Whther they should be or not, there is a disconect between multi-sport event medalists, and single-sport world championships ones.',\n       b'Elysander, I have found good wording. Please, do not change it for previous false version!',\n       b\"Can Someone Lock in the New Links that CIA Keeps Deleting?\\n\\nI have added what I consider, after 20 years of advocacy against fierce opposition from CIA and its FBIS minions,\\na few essential links.  I don't have the time or energy to fight the morons.  If there is an adult with Wiki authority to lock in the links I have added, similar to the manner in which the CIA links are locked in (I have more integrity than they do and would NEVER consider deleting their links), then I think we are all better off for.  If not, www.oss.net will remain up forever, and continues to be *the* reference site for OSINTneither the government nor the vendors are honest on this topic. ~~\",\n       b\"Trolls\\nYou have my email. if this kind of disgusting behaviour continues please let me know but I would urge you to take the PP's to arbcom as they are clearly here on a mission. I have some other ideas too.\",\n       b'\"\\nCalling someone a \"\"cocksucker\"\" is a violation of WP:NPA, and it can get you blocked.  Don\\'t do it, ne? - \"',\n       b\"Interesting edit at Talk:Yoga\\n\\nPlease have a look at this edit by  on Talk:Yoga. Here, he changed the section's title originally created by . As already asked by Sameneguy, is this acceptable? -\",\n       b'\"\\nIt\\'s my pleasure! Sorry if there were a misunderstanding. Pieterse \"',\n       b'waited the requisite six months. I',\n       b\"ps. Almost forgot, Paine don't reply back to this shit, I don't want to see/care what you have to say do your bitching out of my sight, plskthxbai.\",\n       b'Dear Sir any historical discussion on India has to be based India before partition.That is inevitable.\\n\\nV.kothanda Raman',\n       b'\"\\n1. Yoy are a vandal, you have a revert war while your the only one on your side. 2. You are abusing tags, and i\\'m not the first one reverting you over that. You havent given one example. O, you have given one, with canging \"\"Nore then anybody elso\"\" to \"\"big part\"\", and that was made as you offered. It\\'s not like were showing Overys view as the only one, but we have underlined that it\\'s his opinion. So a tag is really not needed, because it was noted in the text that its an opinion.  About what do you want to discuss?? About the fact is it here opinion or no?? The page, and everything, was given to you. \"\"Mister\"\", pans on. Make a table here. Every line you dont agree with, and how do you suggest to change it. It\\'s the 4th time i offer that.   \"',\n       b\"Is it Harmandir or Har Mandir?  \\n\\nI don't understand why sometimes it's one word? I mean if one guru's name is Har Rai. Or Har Krishn. In Hinduism they might say Har Ram. Or Har Krishn. Or Hari OM.....So....Why is the Temple not Har Mandir? Why is it one word like like then Harmandir? I mean im not trying to fight. Im asking. Ive tried figuring this out before to. 71.105.87.54\",\n       b'fuck you \\n\\nfuck you',\n       b\"Hello, and welcome to Wikipedia!\\n\\nI hope not to seem unfriendly or make you feel unwelcome, but I noticed your username, and I am concerned that it might not meet Wikipedia's username policy. After you look over that policy, could we discuss that concern here?\\n\\nI'd appreciate learning your own views, for instance your reasons for wanting this particular name, and what alternative username you might accept that avoids raising this concern.\\n\\nYou have several options freely available to you:\\n If you can relieve my concern through discussing it here, I can stop worrying about it.\\n If the two of us can't agree here, we can ask for help through Wikipedia's dispute resolution process, such as requesting comments from other Wikipedians.  Wikipedia administrators usually abide by agreements reached through this process.\\n You can keep your contributions history under a new username. Visit Wikipedia:Changing username and follow the guidelines there.\"],\n      dtype=object), array([[0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [1, 0, 1, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [1, 0, 1, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [1, 0, 1, 0, 1, 0],\n       [0, 0, 0, 0, 0, 0]]))\n\n\nAnd we check also the shape. We expect a feature of shape (batch, ) and a target of shape (batch, number of labels).\n\n\nCode\nprint(\n  f\"text train shape: {train_ds.as_numpy_iterator().next()[0].shape}\\n\",\n  f\" text train type: {train_ds.as_numpy_iterator().next()[0].dtype}\\n\",\n  f\"label train shape: {train_ds.as_numpy_iterator().next()[1].shape}\\n\",\n  f\"label train type: {train_ds.as_numpy_iterator().next()[1].dtype}\\n\"\n  )\n\n\ntext train shape: (32,)\n  text train type: object\n label train shape: (32, 6)\n label train type: int64"
  },
  {
    "objectID": "posts/toxic_comment_filter/toxic_comment_filter.html#classification-report",
    "href": "posts/toxic_comment_filter/toxic_comment_filter.html#classification-report",
    "title": "Toxic Comment Filter",
    "section": "Classification Report",
    "text": "Classification Report\n\n\nCode\ncr = classification_report(\n  ytrue,\n  ypred,\n  target_names=config.labels,\n  digits=4,\n  output_dict=True\n  )\ndf_cr = pd.DataFrame.from_dict(cr).reset_index()\n\n\n\nCode\nlibrary(reticulate)\ndf_cr &lt;- py$df_cr %&gt;% dplyr::rename(names = index)\ncols &lt;- df_cr %&gt;% colnames()\ndf_cr %&gt;% \n  pivot_longer(\n    cols = -names,\n    names_to = \"metrics\",\n    values_to = \"values\"\n  ) %&gt;% \n  pivot_wider(\n    names_from = names,\n    values_from = values\n  )\n\n\n\n\nTable 5: Classification report\n\n\n\n# A tibble: 10 × 5\n   metrics       precision recall `f1-score` support\n   &lt;chr&gt;             &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;\n 1 toxic            0.562  0.836      0.672     2262\n 2 severe_toxic     0.240  0.896      0.379      240\n 3 obscene          0.504  0.918      0.651     1263\n 4 threat           0.0408 0.203      0.0680      69\n 5 insult           0.428  0.909      0.582     1170\n 6 identity_hate    0.103  0.778      0.183      207\n 7 micro avg        0.411  0.865      0.558     5211\n 8 macro avg        0.313  0.757      0.422     5211\n 9 weighted avg     0.478  0.865      0.606     5211\n10 samples avg      0.0483 0.0799     0.0571    5211"
  }
]