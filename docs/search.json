[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Nice to meet you! My name is Simone Brazzi. I am 34 years old guy from Bologna, Italy.\nI decided to open this blog as a personal journal (what a new idea! Who ever thought about this?) of my data journey. This also means it is an opportunity to display my portfolio and shares my projects.\nAt the date of writing, I am a self made data professional. I am currently working as a Data Scientist for UniCredit Group.\nI have too many hobbies, but I will try to summarize them in a sparse order:\n\nTraining: my 2024 goal is to be able to run at least an half marathon.\nStudy: I am studying data science to further improve my knowledge in the data field. I would like to do a Master Degree in the field, but it is difficult to combine the private life with an international MSc, at least for now.\nGaming, movies and manga: of course! I mean, it is part of being a nerd, right?! For cliche purpose, I love Quentin Tarantino blabbering, Nolan craziness, even though my favorite genre is horror. My favorites directors are Jordan Pelee and Ari Aster. I am currently reading Berserker, My Hero Academia, Jujutsu Kaisen, One Piece, Chainsaw Man and Kagurabachi.\nCooking, I think it is in my vein being italian (joking!) and considering my mother is a chef.\n\nFor now, I think I have annoyed you enough: stay tuned for future posts!"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Spam Detection\n\n\n\n\n\n\ncode\n\n\nNLP\n\n\nNatural Language, Processing\n\n\nPython, R\n\n\n\nNLP\n\n\n\n\n\nSep 5, 2024\n\n\nSimone Brazzi\n\n\n\n\n\n\n\n\n\n\n\n\nToxic Comment Filter\n\n\n\n\n\n\ncode\n\n\nDeep Learning\n\n\nPython, R\n\n\n\nBiLSTM model for multi label classification\n\n\n\n\n\nAug 12, 2024\n\n\nSimone Brazzi\n\n\n\n\n\n\n\n\n\n\n\n\nEurostat Homicide Data\n\n\n\n\n\n\ncode\n\n\nshiny\n\n\nR\n\n\n\nA primer on Shiny to analyze gender differences in homicide rates\n\n\n\n\n\nDec 29, 2023\n\n\nSimone Brazzi\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/toxic_comment_filter/toxic_comment_filter.html",
    "href": "posts/toxic_comment_filter/toxic_comment_filter.html",
    "title": "Toxic Comment Filter",
    "section": "",
    "text": "Build a model that can filter user comments based on the degree of language maliciousness:\n\nPreprocess the text by eliminating the set of tokens that do not make significant contribution at the semantic level.\nTransform the text corpus into sequences.\nBuild a Deep Learning model including recurrent layers for a multilabel classification task.\nAt prediction time, the model should return a vector containing a 1 or a 0 at each label in the dataset (toxic, severe_toxic, obscene, threat, insult, identity_hate). In this way, a non-harmful comment will be classified by a vector of only 0s [0,0,0,0,0]. In contrast, a dangerous comment will exhibit at least a 1 among the 6 labels."
  },
  {
    "objectID": "posts/toxic_comment_filter/toxic_comment_filter.html#import-r-libraries",
    "href": "posts/toxic_comment_filter/toxic_comment_filter.html#import-r-libraries",
    "title": "Toxic Comment Filter",
    "section": "\n2.1 Import R libraries",
    "text": "2.1 Import R libraries\nImport R libraries. These will be used for both the rendering of the document and data analysis. The reason is I prefer ggplot2 over matplotlib. I will also use colorblind safe palettes.\n\nCodelibrary(tidyverse, verbose = FALSE)\nlibrary(tidymodels, verbose = FALSE)\nlibrary(reticulate)\nlibrary(ggplot2)\nlibrary(plotly)\nlibrary(RColorBrewer)\nlibrary(bslib)\nlibrary(Metrics)\nlibrary(gt)\n\nreticulate::use_virtualenv(\"r-tf\")"
  },
  {
    "objectID": "posts/toxic_comment_filter/toxic_comment_filter.html#import-python-packages",
    "href": "posts/toxic_comment_filter/toxic_comment_filter.html#import-python-packages",
    "title": "Toxic Comment Filter",
    "section": "\n2.2 Import Python packages",
    "text": "2.2 Import Python packages\n\nCodeimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport keras\nimport keras_nlp\n\nfrom keras.backend import clear_session\nfrom keras.models import Model, load_model\nfrom keras.layers import TextVectorization, Input, Dense, Embedding, Dropout, GlobalAveragePooling1D, LSTM, Bidirectional, GlobalMaxPool1D, Flatten, Attention\nfrom keras.metrics import Precision, Recall, AUC, SensitivityAtSpecificity, SpecificityAtSensitivity, F1Score\n\n\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.metrics import multilabel_confusion_matrix, classification_report, ConfusionMatrixDisplay, precision_recall_curve, f1_score, recall_score, roc_auc_score\n\n\nCreate a Config class to store all the useful parameters for the model and for the project."
  },
  {
    "objectID": "posts/toxic_comment_filter/toxic_comment_filter.html#class-config",
    "href": "posts/toxic_comment_filter/toxic_comment_filter.html#class-config",
    "title": "Toxic Comment Filter",
    "section": "\n2.3 Class Config",
    "text": "2.3 Class Config\nI created a class with all the basic configuration of the model, to improve the readability.\n\nCodeclass Config():\n    def __init__(self):\n        self.url = \"https://s3.eu-west-3.amazonaws.com/profession.ai/datasets/Filter_Toxic_Comments_dataset.csv\"\n        self.max_tokens = 20000\n        self.output_sequence_length = 911 # check the analysis done to establish this value\n        self.embedding_dim = 128\n        self.batch_size = 32\n        self.epochs = 100\n        self.temp_split = 0.3\n        self.test_split = 0.5\n        self.random_state = 42\n        self.total_samples = 159571 # total train samples\n        self.train_samples = 111699\n        self.val_samples = 23936\n        self.features = 'comment_text'\n        self.labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n        self.new_labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate', \"clean\"]\n        self.label_mapping = {label: i for i, label in enumerate(self.labels)}\n        self.new_label_mapping = {label: i for i, label in enumerate(self.labels)}\n        self.path = \"/Users/simonebrazzi/R/blog/posts/toxic_comment_filter/history/f1score/\"\n        self.model =  self.path + \"model_f1.keras\"\n        self.checkpoint = self.path + \"checkpoint.lstm_model_f1.keras\"\n        self.history = self.path + \"lstm_model_f1.xlsx\"\n        \n        self.metrics = [\n            Precision(name='precision'),\n            Recall(name='recall'),\n            AUC(name='auc', multi_label=True, num_labels=len(self.labels)),\n            F1Score(name=\"f1\", average=\"macro\")\n            \n        ]\n    def get_early_stopping(self):\n        early_stopping = keras.callbacks.EarlyStopping(\n            monitor=\"val_f1\", # \"val_recall\",\n            min_delta=0.2,\n            patience=10,\n            verbose=0,\n            mode=\"max\",\n            restore_best_weights=True,\n            start_from_epoch=3\n        )\n        return early_stopping\n\n    def get_model_checkpoint(self, filepath):\n        model_checkpoint = keras.callbacks.ModelCheckpoint(\n            filepath=filepath,\n            monitor=\"val_f1\", # \"val_recall\",\n            verbose=0,\n            save_best_only=True,\n            save_weights_only=False,\n            mode=\"max\",\n            save_freq=\"epoch\"\n        )\n        return model_checkpoint\n\n    def find_optimal_threshold_cv(self, ytrue, yproba, metric, thresholds=np.arange(.05, .35, .05), n_splits=7):\n\n      # instantiate KFold\n      kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n      threshold_scores = []\n\n      for threshold in thresholds:\n\n        cv_scores = []\n        for train_index, val_index in kf.split(ytrue):\n\n          ytrue_val = ytrue[val_index]\n          yproba_val = yproba[val_index]\n\n          ypred_val = (yproba_val &gt;= threshold).astype(int)\n          score = metric(ytrue_val, ypred_val, average=\"macro\")\n          cv_scores.append(score)\n\n        mean_score = np.mean(cv_scores)\n        threshold_scores.append((threshold, mean_score))\n\n        # Find the threshold with the highest mean score\n        best_threshold, best_score = max(threshold_scores, key=lambda x: x[1])\n      return best_threshold, best_score\n\nconfig = Config()"
  },
  {
    "objectID": "posts/toxic_comment_filter/toxic_comment_filter.html#eda",
    "href": "posts/toxic_comment_filter/toxic_comment_filter.html#eda",
    "title": "Toxic Comment Filter",
    "section": "\n3.1 EDA",
    "text": "3.1 EDA\nFirst a check on the dataset to find possible missing values and imbalances.\n\n3.1.1 Frequency\n\nCodelibrary(reticulate)\ndf_r &lt;- py$df\nnew_labels_r &lt;- py$config$new_labels\n\ndf_r_grouped &lt;- df_r %&gt;% \n  select(all_of(new_labels_r)) %&gt;%\n  pivot_longer(\n    cols = all_of(new_labels_r),\n    names_to = \"label\",\n    values_to = \"value\"\n  ) %&gt;% \n  group_by(label) %&gt;%\n  summarise(count = sum(value)) %&gt;% \n  mutate(freq = round(count / sum(count), 4))\n\ndf_r_grouped %&gt;% \n  gt() %&gt;% \n  tab_header(\n    title = \"Labels frequency\",\n    subtitle = \"Absolute and relative frequency\"\n  ) %&gt;% \n  fmt_number(\n    columns = \"count\",\n    drop_trailing_zeros = TRUE,\n    drop_trailing_dec_mark = TRUE,\n    use_seps = TRUE\n  ) %&gt;% \n  fmt_percent(\n    columns = \"freq\",\n    decimals = 2,\n    drop_trailing_zeros = TRUE,\n    drop_trailing_dec_mark = FALSE\n  ) %&gt;% \n  cols_align(\n    align = \"center\",\n    columns = c(\"count\", \"freq\")\n  ) %&gt;% \n  cols_align(\n    align = \"left\",\n    columns = label\n  ) %&gt;% \n  cols_label(\n    label = \"Label\",\n    count = \"Absolute Frequency\",\n    freq = \"Relative frequency\"\n  )\n\n\nTable 2: Absolute and relative labels frequency\n\n\n\n\n\n\n\nLabels frequency\n\n\nAbsolute and relative frequency\n\n\nLabel\nAbsolute Frequency\nRelative frequency\n\n\n\n\nclean\n143,346\n80.33%\n\n\nidentity_hate\n1,405\n0.79%\n\n\ninsult\n7,877\n4.41%\n\n\nobscene\n8,449\n4.73%\n\n\nsevere_toxic\n1,595\n0.89%\n\n\nthreat\n478\n0.27%\n\n\ntoxic\n15,294\n8.57%\n\n\n\n\n\n\n\n\n\n\n3.1.2 Barchart\n\nCodelibrary(reticulate)\nbarchart &lt;- df_r_grouped %&gt;%\n  ggplot(aes(x = reorder(label, count), y = count, fill = label)) +\n  geom_col() +\n  labs(\n    x = \"Labels\",\n    y = \"Count\"\n  ) +\n  # sort bars in descending order\n  scale_x_discrete(limits = df_r_grouped$label[order(df_r_grouped$count, decreasing = TRUE)]) +\n  scale_fill_brewer(type = \"seq\", palette = \"RdYlBu\") +\n  theme_minimal()\nggplotly(barchart)\n\n\n\n\n\n\nFigure 1: Imbalance in the dataset with clean variable\n\n\n\nIt is visible how much the dataset in imbalanced. This means it could be useful to check for the class weight and use this argument during the training.\nIt is clear that most of our text are clean. We are talking about 0.8033 of the observations which are clean. Only 0.1967 are toxic comments."
  },
  {
    "objectID": "posts/toxic_comment_filter/toxic_comment_filter.html#sequence-lenght-definition",
    "href": "posts/toxic_comment_filter/toxic_comment_filter.html#sequence-lenght-definition",
    "title": "Toxic Comment Filter",
    "section": "\n3.2 Sequence lenght definition",
    "text": "3.2 Sequence lenght definition\nTo convert the text in a useful input for a NN, it is necessary to use a TextVectorization layer. See the Section 4 section.\nOne of the method is output_sequence_length: to better define it, it is useful to analyze our text length. To simulate what the model we do, we are going to remove the punctuation and the new lines from the comments.\n\n3.2.1 Summary\n\nCodelibrary(reticulate)\ndf_r %&gt;% \n  mutate(\n    comment_text_clean = comment_text %&gt;%\n      tolower() %&gt;% \n      str_remove_all(\"[[:punct:]]\") %&gt;% \n      str_replace_all(\"\\n\", \" \"),\n    text_length = comment_text_clean %&gt;% str_count()\n    ) %&gt;% \n  pull(text_length) %&gt;% \n  summary() %&gt;% \n  as.list() %&gt;% \n  as_tibble() %&gt;% \n  gt() %&gt;% \n  tab_header(\n    title = \"Summary Statistics\",\n    subtitle = \"of text length\"\n  ) %&gt;% \n  fmt_number(\n    drop_trailing_zeros = TRUE,\n    drop_trailing_dec_mark = TRUE,\n    use_seps = TRUE\n  ) %&gt;% \n  cols_align(\n    align = \"center\",\n  ) %&gt;% \n  cols_label(\n    Min. = \"Min\",\n    `1st Qu.` = \"Q1\",\n    Median = \"Median\",\n    `3rd Qu.` = \"Q3\",\n    Max. = \"Max\"\n  )\n\n\nTable 3: Summary of text length\n\n\n\n\n\n\n\nSummary Statistics\n\n\nof text length\n\n\nMin\nQ1\nMedian\nMean\nQ3\nMax\n\n\n\n4\n91\n196\n378.4\n419\n5,000\n\n\n\n\n\n\n\n\n\n3.2.2 Boxplot\n\nCodelibrary(reticulate)\nboxplot &lt;- df_r %&gt;% \n  mutate(\n    comment_text_clean = comment_text %&gt;%\n      tolower() %&gt;% \n      str_remove_all(\"[[:punct:]]\") %&gt;% \n      str_replace_all(\"\\n\", \" \"),\n    text_length = comment_text_clean %&gt;% str_count()\n    ) %&gt;% \n  # pull(text_length) %&gt;% \n  ggplot(aes(y = text_length)) +\n  geom_boxplot() +\n  theme_minimal()\nggplotly(boxplot)\n\n\n\n\n\n\nFigure 2: Text length boxplot\n\n\n\n\n3.2.3 Histogram\n\nCodelibrary(reticulate)\ndf_ &lt;- df_r %&gt;% \n  mutate(\n    comment_text_clean = comment_text %&gt;%\n      tolower() %&gt;% \n      str_remove_all(\"[[:punct:]]\") %&gt;% \n      str_replace_all(\"\\n\", \" \"),\n    text_length = comment_text_clean %&gt;% str_count()\n  )\n\nQ1 &lt;- quantile(df_$text_length, 0.25)\nQ3 &lt;- quantile(df_$text_length, 0.75)\nIQR &lt;- Q3 - Q1\nupper_fence &lt;- as.integer(Q3 + 1.5 * IQR)\n\nhistogram &lt;- df_ %&gt;% \n  ggplot(aes(x = text_length)) +\n  geom_histogram(bins = 50) +\n  geom_vline(aes(xintercept = upper_fence), color = \"red\", linetype = \"dashed\", linewidth = 1) +\n  theme_minimal() +\n  xlab(\"Text Length\") +\n  ylab(\"Frequency\") +\n  xlim(0, max(df_$text_length, upper_fence))\nggplotly(histogram)\n\n\n\n\n\n\nFigure 3: Text length histogram with boxplot upper fence\n\n\n\nConsidering all the above analysis, I think a good starting value for the output_sequence_length is 911, the upper fence of the boxplot. In the last plot, it is the dashed red vertical line.. Doing so, we are removing the outliers, which are a small part of our dataset."
  },
  {
    "objectID": "posts/toxic_comment_filter/toxic_comment_filter.html#dataset",
    "href": "posts/toxic_comment_filter/toxic_comment_filter.html#dataset",
    "title": "Toxic Comment Filter",
    "section": "\n3.3 Dataset",
    "text": "3.3 Dataset\nNow we can split the dataset in 3: train, test and validation sets. Considering there is not a function in sklearn which lets split in these 3 sets, we can do the following: - split between a train and temporary set with a 0.3 split. - split the temporary set in 2 equal sized test and val sets.\n\nCodex = df[config.features].values\ny = df[config.labels].values\n\nxtrain, xtemp, ytrain, ytemp = train_test_split(\n  x,\n  y,\n  test_size=config.temp_split, # .3\n  random_state=config.random_state\n  )\nxtest, xval, ytest, yval = train_test_split(\n  xtemp,\n  ytemp,\n  test_size=config.test_split, # .5\n  random_state=config.random_state\n  )\n\n\nxtrain shape: py$xtrain.shape ytrain shape: py$ytrain.shape xtest shape: py$xtest.shape ytest shape: py$ytest.shape xval shape: py$xval.shape yval shape: py$yval.shape\nThe datasets are created using the tf.data.Dataset function. It creates a data input pipeline. The tf.data API makes it possible to handle large amounts of data, read from different data formats, and perform complex transformations. The tf.data.Dataset is an abstraction that represents a sequence of elements, in which each element consists of one or more components. Here each dataset is creates using from_tensor_slices. It create a tf.data.Dataset from a tuple (features, labels). .batch let us work in batches to improve performance, while .prefetch overlaps the preprocessing and model execution of a training step. While the model is executing training step s, the input pipeline is reading the data for step s+1. Check the documentation for further informations.\n\nCodetrain_ds = (\n    tf.data.Dataset\n    .from_tensor_slices((xtrain, ytrain))\n    .shuffle(xtrain.shape[0])\n    .batch(config.batch_size)\n    .prefetch(tf.data.experimental.AUTOTUNE)\n)\n\ntest_ds = (\n    tf.data.Dataset\n    .from_tensor_slices((xtest, ytest))\n    .batch(config.batch_size)\n    .prefetch(tf.data.experimental.AUTOTUNE)\n)\n\nval_ds = (\n    tf.data.Dataset\n    .from_tensor_slices((xval, yval))\n    .batch(config.batch_size)\n    .prefetch(tf.data.experimental.AUTOTUNE)\n)\n\n\n\nCodeprint(\n  f\"train_ds cardinality: {train_ds.cardinality()}\\n\",\n  f\"val_ds cardinality: {val_ds.cardinality()}\\n\",\n  f\"test_ds cardinality: {test_ds.cardinality()}\\n\"\n  )\n\ntrain_ds cardinality: 3491\n val_ds cardinality: 748\n test_ds cardinality: 748\n\n\nCheck the first element of the dataset to be sure that the preprocessing is done correctly.\n\nCodetrain_ds.as_numpy_iterator().next()\n\n(array([b'\"\\nThanks, a separate sentence is an excellent idea, but we will need reliable sources for it. Rather, than tag bomb the sentence, I\\'ll do it here:\\n\"\"typically in education\"\"\\n\"\"everybody knows\"\" (students are included in \"\"everybody\"\", but they don\\'t know \"\"the other kind of numbers\"\")\\n\"\"natural numbers  counting numbers\"\" (natural numbers with or without zero?)\\nWhy was the term \"\"counting numbers\"\" introduced \"\"circa 1965\"\"? (per Merriam-webster.com)\\nAt what educational level is the term \"\"counting numbers\"\" replaced with another term?\\nSourcing ideas include books on pre-algebra and mathematics curriculum standards.\\n50.53.39.110  \"',\n       b',Wikipedia:Articles for deletion/List of countries by ratio of carbon dioxide emissions to GDP per capita',\n       b\"It doesn't like them, but I thought the problem had been fixed. I'll run some more unit tests.\",\n       b\"Amendment of template \\n\\nIs there any reason the Obama template can't be amended with a controversies line, on which we would mention Wright, Ayres, and birthplace?\",\n       b'\"\\nI concur. Outrageous breach of natural justice by an admin....(kiwiexile at DMOZ)  \\xe2\\x80\\xa2 \\n\\n\"',\n       b'\"\\nAnd the entirety of your edits are not related to feminist topics? What a weird comment considering you represent the opposing viewpoint of Memills but still somehow represent this like you\\'d be somehow more neutral. In your earlier comment on this page, you said that Memills \"\"subscribes to a somewhat controversial branch of evolutionary psychology\"\". Well, may I ask, do you subscribe to the somewhat controversial branch women\\'s studies of sociology?\\nRegardless, I suggest everyone reads the following section on KillerChihuahua\\'s talk page: . KillerChihuahua has been giving \"\"admin support\"\" to Slp1\\'s side as pointed out themselves. I suggest you Memills fill a complaint on this. \\' \"',\n       b'\"\\n\\n Your message to me \\n\\nThe page you linked to said \"\"by convention your user page will usually not be edited by others\"\". 122.105.217.56  \"',\n       b'And I see what you meant now, too. -) I completely missed that he changed font sizes. About the discussion, I believe I prefer the 100% standard version as well. Hopefully it will make for less edit warring and a cleaner Wikipedia.  hideliho!',\n       b'Gmaxwell\\nHi, he has said he does want the image here. Cheers',\n       b'\"\\n\\nThis message is regarding the page User:Jpgordon. Please stop.  If you continue to vandalize pages, you will be blocked from editing Wikipedia.  \\xc2\\xab\\xc2\\xbb?\\xc2\\xbf?meta \"',\n       b'\"::::::::::::::I would suggest that it\\'s not necessarily an \"\"anti-vax\"\" site as suggested, and it does cite sources within the body if was read.  Nonetheless, it raises interesting points (such as efficacy being related to age and to a particular strain)and does present the other side of the argument.  With respect to purported anti-vax statements on websites, this is against the standard of practice in most Canadian provinces and the CCO has fined members up to 25K for those shenanigans.  Nonetheless, we should not pick one example and make attributions, for I can go to MD websites and show them endorsing questionable methods such as homeopathy.  We need to be careful with giving fringe POVs too much weight here which seems to be an increasing concern.   \\n\"',\n       b'\"\\n\\nI can say the furlong is a known length in the US and it even sees use on the interstate system as 1/8 mile increments are occasionally signed (although I have never seen 1/8 mile, think it was 3/8 ad 5/8 that I saw).  I\\'ve seen several old British signs that are still in place along roadsigns of public roads in the UK that a buddy of mine there showed me.  Perhaps if he or the author consents, I could put them up here?  Think one was furlongs and there may or may not have been one that was in miles and eighths.\\n\\nAlso, does anyone have some information about the Myanmar furlong sign.  Some internet armchair expert was claiming they were all kilometers \"\"now\"\" which I had assumed correct, until seeing an even newer post, less than 6 mos. old talking about biking there and how \"\"everything was still in miles [and furlongs] there.  If we had a more current picture of the signage there, even if it\\'s the same damned toll plaza, that would go a long way.  Their government website, at least in English, is, shall we say, lacking?\\n  \"',\n       b\"OK, but when you threaten to lock a page because I don't agree with your recommendations, I think calling you a dictator is more of an observation rather than a personal attack.\",\n       b'The problem is no longer there.',\n       b'Thank you for experimenting with Wikipedia. Your test worked, and it has been reverted or removed. Please use the sandbox for any other tests you want to do. Take a look at the welcome page if you would like to learn more about contributing to our encyclopedia.    | Talk',\n       b'\"\\n\\nWelcome\\n\\nHello and welcome to Wikipedia!  We appreciate encyclopedic contributions, but some of your recent contributions do not conform to our policies.  For more information on this, see\\nWikipedia\\'s policies on vandalism\\nPolicies on banned or discouraged content\\n\\nIf you\\'d like to experiment with the wiki\\'s syntax, please do so on Wikipedia:Sandbox rather than in articles.\\n\\nIf you still have questions, there is a new contributor\\'s help page, or you can write {{helpme}} below this message along with a question and someone will be along to answer it shortly.  You may also find the following pages useful for a general introduction to Wikipedia.\\nThe five pillars of Wikipedia\\nHow to edit a page\\nHelp pages\\nTutorial\\nHow to write a great article\\nManual of Style\\nPolicy on neutral point of view\\nGuideline on external links\\nGuideline on conflict of interest\\nI hope you enjoy editing Wikipedia!  Please sign your name on talk pages using four tildes (~~~~); this will automatically produce your name and the date. Feel free to write a note on the bottom of  if you want to get in touch with me. Again, welcome!  - [[User_talk:Bpeps|t@lk]] \"',\n       b'oh i like that one you ask me on the page to state my case i do so and you erase it - in debate was i was a little child i learned that you just forfeited 68.231.15.56',\n       b'Can you unblock my IP? \\n\\nHey pgk, can you plz unblock my ip, cos i have been blocked cos sum1 else was blocked using my AOL IP. Thanks,',\n       b\"Usually, I'd agree, but for two reasons I think not in this case.... A- it would look choppy, and B- it is still the MAIN article, since it was the most pathetic example.  This is, however, just my opinion.  BTW dont sweat it.  I always forget to sign in (KA-BOOOOM!!!!)\",\n       b'Please stop adding nonsense to Wikipedia. It is considered vandalism. If you were just trying to experiment, then use the sandbox instead. Thank you.',\n       b', 20 September 2005 (UTC)\\n\\n ... or (C) their loved one is brought back to life. (B) is by far the most likely though.   |  Talk 00:38',\n       b'Punk\\n\\nthe only reason anyone gives you  trouble is because you cause trouble yourself. You deserve to be attacked',\n       b'. \\n\\nP.s. Victor actually told me himself at the practice. What do I have to do to prove it? Record him?!? LOL yea son',\n       b'\"\\nI do not see any anti-semitism here. If the guy gives lectures in New York synagogues and knocks down his country every single minute, using every opportunity in such influential newspapers as New York Post, then one can state that his interests are not only personal. You can not find any intellectual person among neoconservatives in New York Post. Michelle Malkin, Andrea Peyser, Deborah Orin (may her rest in peace, although I know she is rotting in hell)and other essayists are the ones who make a great company for Amir Taheri. This guy has no mind at all. I agree that Iran\\'s regime is wrong, but the regime of Shah that Amir Taheri is longing for, because he owned Keyhan during that regime was not any better and Islamic Revolution was justified. Look at how they intriduce Amir Taheri in both this article and in New York Post. \"\"Based in Europe memeber of Benador Associates\"\". Is Europe that small? Is that a small town? Why don\\'t they say based in the world journalist. Or may be he is having a breakfast in London with Christian Amanpour and James Rubin, eating lunch in Jerusalem, and having some lectures and dinner with anti Irani Jews in Forest Hills, New York in between emailing his full of hatred articles to New York Post. Isn\\'t this stupid? How can you saysomebody is based in Europe or Asia or Africa?\"',\n       b'Wikileaks article  is written in British English, and some terms used in it are different or absent from American English and other varieties of English. According to the relevant style guide, this should not be changed without broad consensus.',\n       b'\"\\n\\ncriticism section\\n\\nPeople have repeatedly tried to add a criticism section.. and it keeps getting deleted.. If anyone warrents a criticism secion it is alex jones with his nutty theories of black helicopters,secret government detention facilities at amtrak stations retrofitted with gas chambers for american citizens, and so on and so on.. nevermind the self proclaimed crusader for truth admits on his website that you can buy advertisements \"\"disguised as news headlines\"\" in the the police state section.  real thruthfull.. I mean seriously.. an admin needs to add this, ban people who keep censoring the article the protect the page if necessary... this is rediculous. -  \"',\n       b\"I am trying to update my OWN page and you keep undoing it and then accusing me of adding uncourced content, IAM the SOURCE what can I dio? verty fryustrating, please can you leave my page alone, i'm trying to keep my list of publications up to date, and don't block me \\nChristopher Hart\",\n       b'Please stop. If you continue to blank out or delete portions of page content, templates or other materials from Wikipedia, you will be blocked from editing.  -38.116.200.85',\n       b'Hello \\n\\nPlease u deleted the page on Patrick Obahiagbon. I really dont know why although I cited my references and it has be categorised into Nigerian Politicians by another admin. Please why did u delete it? *sad*',\n       b\"The Comedy of Errors\\nSorry, but I've reverted your additions to The Comedy of Errors, as they don't really belong there. Sure, they're about Shakespeare, but you haven't shown how they're in any way relevant to he article about that play, and they don't quite fit in. Thanks.\",\n       b'You are IGNORING my other sources, and only referring to my forum source. Why not put a tag there so I can fix it myself instead of WRECKING hours of work.',\n       b'alreajk not logging in.'], dtype=object), array([[0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [1, 0, 0, 0, 1, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0]]))\n\n\nAnd we check also the shape. We expect a feature of shape (batch, ) and a target of shape (batch, number of labels).\n\nCodeprint(\n  f\"text train shape: {train_ds.as_numpy_iterator().next()[0].shape}\\n\",\n  f\" text train type: {train_ds.as_numpy_iterator().next()[0].dtype}\\n\",\n  f\"label train shape: {train_ds.as_numpy_iterator().next()[1].shape}\\n\",\n  f\"label train type: {train_ds.as_numpy_iterator().next()[1].dtype}\\n\"\n  )\n\ntext train shape: (32,)\n  text train type: object\n label train shape: (32, 6)\n label train type: int64"
  },
  {
    "objectID": "posts/toxic_comment_filter/toxic_comment_filter.html#definition",
    "href": "posts/toxic_comment_filter/toxic_comment_filter.html#definition",
    "title": "Toxic Comment Filter",
    "section": "\n5.1 Definition",
    "text": "5.1 Definition\nDefine the model using the Functional API.\n\nCodedef get_deeper_lstm_model():\n    clear_session()\n    inputs = Input(shape=(None,), dtype=tf.int64, name=\"inputs\")\n    embedding = Embedding(\n        input_dim=config.max_tokens,\n        output_dim=config.embedding_dim,\n        mask_zero=True,\n        name=\"embedding\"\n    )(inputs)\n    x = Bidirectional(LSTM(256, return_sequences=True, name=\"bilstm_1\"))(embedding)\n    x = Bidirectional(LSTM(128, return_sequences=True, name=\"bilstm_2\"))(x)\n    # Global average pooling\n    x = GlobalAveragePooling1D()(x)\n    # Add regularization\n    x = Dropout(0.3)(x)\n    x = Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n    x = LayerNormalization()(x)\n    outputs = Dense(len(config.labels), activation='sigmoid', name=\"outputs\")(x)\n    model = Model(inputs, outputs)\n    model.compile(optimizer='adam', loss=\"binary_crossentropy\", metrics=config.metrics, steps_per_execution=32)\n    \n    return model\n\nlstm_model = get_deeper_lstm_model()\nlstm_model.summary()"
  },
  {
    "objectID": "posts/toxic_comment_filter/toxic_comment_filter.html#callbacks",
    "href": "posts/toxic_comment_filter/toxic_comment_filter.html#callbacks",
    "title": "Toxic Comment Filter",
    "section": "\n5.2 Callbacks",
    "text": "5.2 Callbacks\nFinally, the model has been trained using 2 callbacks: - Early Stopping, to avoid to consume the kaggle GPU time. - Model Checkpoint, to retrieve the best model training information.\n\nCodemy_es = config.get_early_stopping()\nmy_mc = config.get_model_checkpoint(filepath=\"/checkpoint.keras\")\ncallbacks = [my_es, my_mc]"
  },
  {
    "objectID": "posts/toxic_comment_filter/toxic_comment_filter.html#final-preparation-before-fit",
    "href": "posts/toxic_comment_filter/toxic_comment_filter.html#final-preparation-before-fit",
    "title": "Toxic Comment Filter",
    "section": "\n5.3 Final preparation before fit",
    "text": "5.3 Final preparation before fit\nConsidering the dataset is imbalanced, to increase the performance we need to calculate the class weight. This will be passed during the training of the model.\n\nCodelab = pd.DataFrame(columns=config.labels, data=ytrain)\nr = lab.sum() / len(ytrain)\nclass_weight = dict(zip(range(len(config.labels)), r))\ndf_class_weight = pd.DataFrame.from_dict(\n  data=class_weight,\n  orient='index',\n  columns=['class_weight']\n  )\ndf_class_weight.index = config.labels\n\n\n\nCodelibrary(reticulate)\npy$df_class_weight %&gt;% \n  gt() %&gt;% \n  fmt_percent(\n    decimals = 2,\n    drop_trailing_zeros = TRUE,\n    drop_trailing_dec_mark = TRUE\n  )\n\n\nTable 4: Class weight\n\n\n\n\n\n\nclass_weight\n\n\n\n9.59%\n\n\n0.99%\n\n\n5.28%\n\n\n0.31%\n\n\n4.91%\n\n\n0.87%\n\n\n\n\n\n\n\n\n\nIt is also useful to define the steps per epoch for train and validation dataset. This step is required to avoid to not consume entirely the dataset during the fit, which happened to me.\n\nCodesteps_per_epoch = config.train_samples // config.batch_size\nvalidation_steps = config.val_samples // config.batch_size"
  },
  {
    "objectID": "posts/toxic_comment_filter/toxic_comment_filter.html#fit",
    "href": "posts/toxic_comment_filter/toxic_comment_filter.html#fit",
    "title": "Toxic Comment Filter",
    "section": "\n5.4 Fit",
    "text": "5.4 Fit\nThe fit has been done on Kaggle to levarage the GPU. Some considerations about the model:\n\n\n.repeat() ensure the model sees all the dataset.\n\nepocs is set to 100.\n\nvalidation_data has the same repeat.\n\ncallbacks are the one defined before.\n\nclass_weight ensure the model is trained using the frequency of each class, because our dataset is imbalanced.\n\nsteps_per_epoch and validation_steps depend on the use of repeat.\n\n\nCodehistory = model.fit(\n  processed_train_ds.repeat(),\n  epochs=config.epochs,\n  validation_data=processed_val_ds.repeat(),\n  callbacks=callbacks,\n  class_weight=class_weight,\n  steps_per_epoch=steps_per_epoch,\n  validation_steps=validation_steps\n  )\n\n\nNow we can import the model and the history trained on Kaggle.\n\nCodemodel = load_model(filepath=config.model)\nhistory = pd.read_excel(config.history)"
  },
  {
    "objectID": "posts/toxic_comment_filter/toxic_comment_filter.html#evaluate",
    "href": "posts/toxic_comment_filter/toxic_comment_filter.html#evaluate",
    "title": "Toxic Comment Filter",
    "section": "\n5.5 Evaluate",
    "text": "5.5 Evaluate\n\nCodevalidation = model.evaluate(\n  processed_val_ds.repeat(),\n  steps=validation_steps, # 748\n  verbose=0\n  )\n\n\n\nCodeval_metrics &lt;- tibble(\n  metric = c(\"loss\", \"precision\", \"recall\", \"auc\", \"f1_score\"),\n  value = py$validation\n  )\nval_metrics %&gt;% \n  gt() %&gt;% \n  fmt_number(\n    columns = c(\"value\"),\n    decimals = 4,\n    drop_trailing_zeros = TRUE,\n    drop_trailing_dec_mark = TRUE\n  ) %&gt;% \n  cols_align(\n    align = \"left\",\n    columns = metric\n  ) %&gt;% \n  cols_align(\n    align = \"center\",\n    columns = value\n  ) %&gt;% \n  cols_label(\n    metric = \"Metric\",\n    value = \"Value\"\n  )\n\n\nTable 5: Model validation metric\n\n\n\n\n\n\nMetric\nValue\n\n\n\nloss\n0.0542\n\n\nprecision\n0.7888\n\n\nrecall\n0.671\n\n\nauc\n0.9572\n\n\nf1_score\n0.0293"
  },
  {
    "objectID": "posts/toxic_comment_filter/toxic_comment_filter.html#predict",
    "href": "posts/toxic_comment_filter/toxic_comment_filter.html#predict",
    "title": "Toxic Comment Filter",
    "section": "\n5.6 Predict",
    "text": "5.6 Predict\nFor the prediction, the model does not need to repeat the dataset, because it has already been trained on all of the train data. Now it has just to consume the new data to make the prediction.\n\nCodepredictions = model.predict(processed_test_ds, verbose=0)"
  },
  {
    "objectID": "posts/toxic_comment_filter/toxic_comment_filter.html#confusion-matrix",
    "href": "posts/toxic_comment_filter/toxic_comment_filter.html#confusion-matrix",
    "title": "Toxic Comment Filter",
    "section": "\n5.7 Confusion Matrix",
    "text": "5.7 Confusion Matrix\nThe best way to assess the performance of a multi label classification is using a confusion matrix. Sklearn has a specific function to create a multi label classification matrix to handle the fact that there could be multiple labels for one prediction.\n\n5.7.1 Grid Search Cross Validation for best threshold\nGrid Search CV is a technique for fine-tuning hyperparameter of a ML model. It systematically search through a set of hyperparamenter values to find the combination which led to the best model performance. In this case, I am using a KFold Cross Validation is a resempling technique to split the data into k consecutive folds. Each fold is used once as a validation while the k - 1 remaining folds are the training set. See the documentation for more information.\nThe model is trained to optimize the recall. The decision was made because the cost of missing a True Positive is greater than a False Positive. In this case, missing a injurious observation is worst than classifying a clean one as bad.\n\n5.7.2 Confidence threshold and Precision-Recall trade off\nWhilst the KFold GDCV technique is usefull to test multiple hyperparameter, it is important to understand the problem we are facing. A multi label deep learning classifier outputs a vector of per-class probabilities. These need to be converted to a binary vector using a confidence threshold.\n\nThe higher the threshold, the less classes the model predicts, increasing model confidence [higher Precision] and increasing missed classes [lower Recall].\nThe lower the threshold, the more classes the model predicts, decreasing model confidence [lower Precision] and decreasing missed classes [higher Recall].\n\nThreshold selection mean we have to decide which metric to prioritize, based on the problem we are facing and the relative cost of misduging. We can consider the toxic comment filtering a problem similiar to cancer diagnostic. It is better to predict cancer in people who do not have it [False Positive] and perform further analysis than do not predict cancer when the patient has the disease [False Negative].\nI decide to train the model on the F1 score to have a balanced model in both precision and recall and leave to the threshold selection to increase the recall performance.\nMoreover, the model has been trained on the macro avarage F1 score, which is a single performance indicator obtained by the mean of the Precision and Recall scores of individual classses.\n\\[\nF1\\ macro\\ avg = \\frac{\\sum_{i=1}^{n} F1_i}{n}\n\\]\nIt is useful with imbalanced classes, because it weights each classes equally. It is not influenced by the number of samples of each classes. This is sette both in the config.metrics and find_optimal_threshold_cv.\nf1_score\n\nCodeytrue = ytest.astype(int)\ny_pred_proba = predictions\noptimal_threshold_f1, best_score_f1 = config.find_optimal_threshold_cv(ytrue, y_pred_proba, f1_score)\n\nprint(f\"Optimal threshold: {optimal_threshold_f1}\")\n\nOptimal threshold: 0.15000000000000002\n\nCodeprint(f\"Best score: {best_score_f1}\")\n\nBest score: 0.4788653077945807\n\nCode\n# Use the optimal threshold to make predictions\nfinal_predictions_f1 = (y_pred_proba &gt;= optimal_threshold_f1).astype(int)\n\n\nOptimal threshold f1 score: 0.15. Best score: 0.4788653.\nrecall_score\n\nCodeytrue = ytest.astype(int)\ny_pred_proba = predictions\noptimal_threshold_recall, best_score_recall = config.find_optimal_threshold_cv(ytrue, y_pred_proba, recall_score)\n\n# Use the optimal threshold to make predictions\nfinal_predictions_recall = (y_pred_proba &gt;= optimal_threshold_recall).astype(int)\n\n\nOptimal threshold recall: 0.05. Best score: 0.8095814.\nroc_auc_score\n\nCodeytrue = ytest.astype(int)\ny_pred_proba = predictions\noptimal_threshold_roc, best_score_roc = config.find_optimal_threshold_cv(ytrue, y_pred_proba, roc_auc_score)\n\nprint(f\"Optimal threshold: {optimal_threshold_roc}\")\n\nOptimal threshold: 0.05\n\nCodeprint(f\"Best score: {best_score_roc}\")\n\nBest score: 0.8809499649742268\n\nCode\n# Use the optimal threshold to make predictions\nfinal_predictions_roc = (y_pred_proba &gt;= optimal_threshold_roc).astype(int)\n\n\nOptimal threshold roc: 0.05. Best score: 0.88095.\n\n5.7.3 Confusion Matrix Plot\n\nCode# convert probability predictions to predictions\nypred = predictions &gt;=  optimal_threshold_recall # .05\nypred = ypred.astype(int)\n\n# create a plot with 3 by 2 subplots\nfig, axes = plt.subplots(3, 2, figsize=(15, 15))\naxes = axes.flatten()\nmcm = multilabel_confusion_matrix(ytrue, ypred)\n# plot the confusion matrices for each label\nfor i, (cm, label) in enumerate(zip(mcm, config.labels)):\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n    disp.plot(ax=axes[i], colorbar=False)\n    axes[i].set_title(f\"Confusion matrix for label: {label}\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\nFigure 4: Multi Label Confusion matrix"
  },
  {
    "objectID": "posts/toxic_comment_filter/toxic_comment_filter.html#classification-report",
    "href": "posts/toxic_comment_filter/toxic_comment_filter.html#classification-report",
    "title": "Toxic Comment Filter",
    "section": "\n5.8 Classification Report",
    "text": "5.8 Classification Report\n\nCodecr = classification_report(\n  ytrue,\n  ypred,\n  target_names=config.labels,\n  digits=4,\n  output_dict=True\n  )\ndf_cr = pd.DataFrame.from_dict(cr).reset_index()\n\n\n\nCodelibrary(reticulate)\ndf_cr &lt;- py$df_cr %&gt;% dplyr::rename(names = index)\ncols &lt;- df_cr %&gt;% colnames()\ndf_cr %&gt;% \n  pivot_longer(\n    cols = -names,\n    names_to = \"metrics\",\n    values_to = \"values\"\n  ) %&gt;% \n  pivot_wider(\n    names_from = names,\n    values_from = values\n  ) %&gt;% \n  gt() %&gt;%\n  tab_header(\n    title = \"Confusion Matrix\",\n    subtitle = \"Threshold optimization favoring recall\"\n  ) %&gt;% \n  fmt_number(\n    columns = c(\"precision\", \"recall\", \"f1-score\", \"support\"),\n    decimals = 2,\n    drop_trailing_zeros = TRUE,\n    drop_trailing_dec_mark = FALSE\n  ) %&gt;% \n  cols_align(\n    align = \"center\",\n    columns = c(\"precision\", \"recall\", \"f1-score\", \"support\")\n  ) %&gt;% \n  cols_align(\n    align = \"left\",\n    columns = metrics\n  ) %&gt;% \n  cols_label(\n    metrics = \"Metrics\",\n    precision = \"Precision\",\n    recall = \"Recall\",\n    `f1-score` = \"F1-Score\",\n    support = \"Support\"\n  )\n\n\nTable 6: Classification report\n\n\n\n\n\n\n\nConfusion Matrix\n\n\nThreshold optimization favoring recall\n\n\nMetrics\nPrecision\nRecall\nF1-Score\nSupport\n\n\n\n\ntoxic\n0.55\n0.89\n0.68\n2,262.\n\n\nsevere_toxic\n0.24\n0.92\n0.37\n240.\n\n\nobscene\n0.55\n0.94\n0.69\n1,263.\n\n\nthreat\n0.04\n0.49\n0.07\n69.\n\n\ninsult\n0.47\n0.91\n0.62\n1,170.\n\n\nidentity_hate\n0.12\n0.72\n0.2\n207.\n\n\nmicro avg\n0.42\n0.9\n0.57\n5,211.\n\n\nmacro avg\n0.33\n0.81\n0.44\n5,211.\n\n\nweighted avg\n0.49\n0.9\n0.63\n5,211.\n\n\nsamples avg\n0.05\n0.08\n0.06\n5,211."
  },
  {
    "objectID": "posts/eurostat_homicide_rate/eurostat_homicide_rate.html",
    "href": "posts/eurostat_homicide_rate/eurostat_homicide_rate.html",
    "title": "Eurostat Homicide Data",
    "section": "",
    "text": "Important\n\n\n\nThe post is not finished. Stay tuned for updates!\n\n\nHi there and welcome to my first project for the blog! The topic is a sad one, but I would like to explain why I decided to start with this. I was trying Shiny for different dashboards, but I wasn’t satisfied to learn using the classic examples. Unfortunately, italian crime news suffocated the public debate with a case of homicide, in which the victim is a young women. The public debate was focusing so baaaaaadly on the concept of femicide and the data, that I decided to clear the situation with a simple dashboard.\nFirst of all, at this link you can find the dashboard published using shinyapps.io. Also at this link you can find the Github repo. As you can see, even if it is the main branch, there are some details which are not uber perfect, but that don’t interfere with the code.\nNow lets jump into the detail of how to create a Shiny dashboard!"
  },
  {
    "objectID": "posts/eurostat_homicide_rate/eurostat_homicide_rate.html#global.r",
    "href": "posts/eurostat_homicide_rate/eurostat_homicide_rate.html#global.r",
    "title": "Eurostat Homicide Data",
    "section": "\n3.1 global.R",
    "text": "3.1 global.R\nAs said, this file is our usual R Script file. First thing first, we import our libraries:\n\nCode# wrangling\nlibrary(\"tidyverse\")\nlibrary(\"readr\")\nlibrary(\"stringr\")\nlibrary(\"dplyr\")\nlibrary(\"magrittr\")\nlibrary(\"forcats\")\nlibrary(\"lubridate\")\nlibrary(\"writexl\")\nlibrary(\"eurostat\")\n# plotting and dashboarding\nlibrary(\"shiny\")\nlibrary(\"shinythemes\")\nlibrary(\"ggplot2\")\nlibrary(\"plotly\")\nlibrary(\"scales\")\nlibrary(\"RColorBrewer\")\nlibrary(\"waiter\")\n# connecting and other\nlibrary(\"rsconnect\")\nlibrary(\"markdown\")\n\n\nLots of packages! The division is merely to remember how everything is managed and because I have OCD for this type of things.\nI want to focus on some packages:\n\ntidyverse, we all know it. As you can see, I also imported lots of single packages which compose the tidyverse, because I was getting errors of missing methods.\neurostat, which lets data flows from the eurostat website to my dashboard. This also lets the dashboard automatically updates when new data is available.\nscales, to nicely scaling my x and y axis.\nRColorBrewer, because I wanted to have a colorblind safe dashboard, even tough I am not.\nwaiter, for nice waiting images while the dashboard is loading.\n\nNow we can focus on the data importing and wrangling. For this, the eurostat library does the job. Lets focus on the crim_hom_vrel dataset.\n\nCode# search in eurostat db\nhomicide &lt;- search_eurostat(\"homicide\")\n\n# import data to variable\ncrim_hom_vrel &lt;- get_eurostat(\"crim_hom_vrel\", time_format = \"date\")\n\n# convert all observations to understandable data\ncrim_hom_vrel &lt;- label_eurostat(crim_hom_vrel)\n\n# label_eurostat_vars(crim_hom_vrel)\n\n# order data by country and date for time series purpose\ncrim_hom_vrel &lt;- crim_hom_vrel %&gt;% \n  arrange(geo, time)\n\ncrim_hom_vrel_grouped &lt;- crim_hom_vrel %&gt;% \n  dplyr::group_by(geo, time, sex, pers_cat, unit) %&gt;% \n  dplyr::summarise(values_grouped = sum(values), .groups = \"drop\") %&gt;% \n  filter(unit == \"Number\") %&gt;% \n  arrange(geo, time, sex)\n\n\nEverything pretty simple. I would like to highlight something about the dplyr::group_by and dplyr::summarise. As you can see, after having grouped and summarized, I need to drop the groups with the method .groups = \"drop\". With dplyr v.1.1.0 we can do the same with the help of the .by method in summarise.\n\nCodecrim_hom_vrel_grouped &lt;- crim_hom_vrel %&gt;% \n  dplyr::summarise(\n    values_grouped = sum(values),\n    .by = c(geo, time, sex, pers_cat, unit)\n    ) %&gt;% \n  filter(unit == \"Number\") %&gt;% \n  arrange(geo, time, sex)\n\n\nCopying from the dplyr website the differences between .by and group_by() are:\n\n\n.by\ngroup_by()\n\n\n\nGrouping only affects a single verb\nGrouping is persistent across multiple verbs\n\n\nSelects variables with tidy-select\n\nComputes expressions with data-masking\n\n\n\nSummaries use existing order of group keys\nSummaries sort group keys in ascending order\n\n\n\nLast part is all about colors.\n\nCode# brewer.pal(11, \"RdYlBu\")\npalette &lt;- c(\"#A50026\", \"#D73027\", \"#F46D43\", \"#FDAE61\", \"#FEE090\", \"#FFFFBF\", \"#E0F3F8\", \"#ABD9E9\", \"#74ADD1\", \"#4575B4\", \"#313695\")\n\npalette_crim_hom_vrel_grouped &lt;- rep(\n  palette,\n  length.out = crim_hom_vrel_grouped$geo %&gt;% str_unique() %&gt;% length()\n  )\n\n\nHere I defined the palette using ColorBrewer. Using rep I replicated the 11 colours for the length of the unique geo values."
  },
  {
    "objectID": "posts/eurostat_homicide_rate/eurostat_homicide_rate.html#ui.r",
    "href": "posts/eurostat_homicide_rate/eurostat_homicide_rate.html#ui.r",
    "title": "Eurostat Homicide Data",
    "section": "\n3.2 ui.R",
    "text": "3.2 ui.R"
  },
  {
    "objectID": "posts/spam_detection/spam_detection.html",
    "href": "posts/spam_detection/spam_detection.html",
    "title": "Spam Detection",
    "section": "",
    "text": "Tasks:\n\nTrain a model to classify spam.\nFind the main topic in the spam emails.\nCalculate semantic distance of the spam topics.\nExtract the ORG from ham emilas."
  },
  {
    "objectID": "posts/spam_detection/spam_detection.html#r-libraries",
    "href": "posts/spam_detection/spam_detection.html#r-libraries",
    "title": "Spam Detection",
    "section": "\n2.1 R libraries",
    "text": "2.1 R libraries\n\nCodelibrary(tidyverse, verbose = FALSE)\nlibrary(ggplot2)\nlibrary(gt)\nlibrary(reticulate)\nlibrary(plotly)\n\n# renv::use_python(\"/Users/simonebrazzi/venv/blog/bin/python3\")"
  },
  {
    "objectID": "posts/spam_detection/spam_detection.html#python-packages",
    "href": "posts/spam_detection/spam_detection.html#python-packages",
    "title": "Spam Detection",
    "section": "\n2.2 Python packages",
    "text": "2.2 Python packages\n\nCodeimport pandas as pd\nimport numpy as np\nimport spacy\nimport nltk\nimport string\nimport gensim\nimport gensim.corpora as corpora\nimport gensim.downloader\n\nfrom nltk.corpus import stopwords\nfrom gensim.models import Word2Vec\nfrom scipy.spatial.distance import cosine\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\nfrom collections import Counter\n# glove vector\nglove_vector = gensim.downloader.load(\"glove-wiki-gigaword-300\")\n# nltk stopwords\nnltk.download('stopwords')\n\nTrue\n\nCodestop_words = stopwords.words('english')"
  },
  {
    "objectID": "posts/spam_detection/spam_detection.html#config-class",
    "href": "posts/spam_detection/spam_detection.html#config-class",
    "title": "Spam Detection",
    "section": "\n2.3 Config class",
    "text": "2.3 Config class\n\nCodeclass Config():\n  def __init__(self):\n    \n    \"\"\"\n    CLass initialize function.\n    \"\"\"\n    \n    self.path=\"/Users/simonebrazzi/R/blog/posts/spam_detection/spam_dataset.csv\"\n    self.random_state=42\n  \n  def get_lemmas(self, doc):\n    \n    \"\"\"\n    List comprehension to get lemmas. It performs:\\n\n      1. Lowercase.\\n\n      2. Stop words removal\\n.\n      3. Whitespaces removal.\\n\n      4. Remove the word 'subject'.\\n\n      5. Digits removal.\\n\n      6. Get only words with length &gt;= 5.\\n\n    \"\"\"\n    \n    lemmas = [\n      [\n        t.lemma_ for t in d \n        if (text := t.text.lower()) not in punctuation \n        and text not in stop_words \n        and not t.is_space \n        and text != \"subject\" \n        and not t.is_digit\n        and len(text) &gt;=5\n        ]\n        for d in doc\n        ]\n    return lemmas\n  \n  def get_entities(self, doc):\n    \n    \"\"\"\n    List comprehension to get the lemmas which have entity type 'ORG'.\n    \"\"\"\n    \n    entities = [\n    [\n      t.lemma_ for t in d \n      if (text := t.text.lower()) not in punctuation \n      and text not in stop_words \n      and not t.is_space \n      and text != \"subject\" \n      and not t.is_digit\n      and len(text) &gt;=5\n      and t.ent_type_ == \"ORG\"\n      ]\n      for d in doc\n      ]\n    return entities\n  \n  def get_sklearn_mlp(self, activation, solver, max_iter, hidden_layer_sizes, tol):\n    \n    \"\"\"\n    It initialize the sklearn MLPClassifier.\n    \"\"\"\n    \n    mlp = mlp = MLPClassifier(\n      activation=activation,\n      solver=solver,\n      max_iter=max_iter,\n      hidden_layer_sizes=hidden_layer_sizes,\n      tol=tol,\n      verbose=True\n      )\n    return mlp\n  \n  def get_lda(self, corpus, id2word, num_topics, passes, workers, random_state):\n    \n    \"\"\"\n    Initialize LDA.\n    \"\"\"\n    \n    lda = gensim.models.LdaMulticore(\n      corpus=corpus,\n      id2word=id2word,\n      num_topics=num_topics,\n      passes=passes,\n      workers=workers,\n      random_state=self.random_state\n      )\n    return lda\n  \nconfig = Config()"
  },
  {
    "objectID": "posts/spam_detection/spam_detection.html#eda",
    "href": "posts/spam_detection/spam_detection.html#eda",
    "title": "Spam Detection",
    "section": "\n3.1 EDA",
    "text": "3.1 EDA\nFirst things first, Exploratory Data Analysis. Considering we are going to perform a classification, is interesting to check if our dataset is unbalanced.\n\nCodedf_g &lt;- df %&gt;% \n  summarise(\n    freq_abs = n(),\n    freq_rel = n() / nrow(df),\n    .by = label\n    )\n\ndf_g %&gt;% \n  gt() %&gt;% \n  fmt_auto() %&gt;% \n  cols_width(\n    label ~ pct(20),\n    freq_abs ~ pct(35),\n    freq_rel ~ pct(35)\n    ) %&gt;% \n  cols_align(\n    align = \"center\",\n    columns = c(freq_abs, freq_rel)\n  ) %&gt;% \n  tab_header(\n    title = \"Label frequency\",\n    subtitle = \"Absolute and relative frequencies\"\n  ) %&gt;% \n  cols_label(\n    label = \"Label\",\n    freq_abs = \"Absolute frequency\",\n    freq_rel = \"Relative frequency\"\n  ) %&gt;% \n  tab_options(\n    table.width = pct(100)\n    )\n\n\nTable 1: Absolute and relative frequencies\n\n\n\n\n\n\n\n\n\n\n\n\nLabel frequency\n\n\nAbsolute and relative frequencies\n\n\nLabel\nAbsolute frequency\nRelative frequency\n\n\n\n\nham\n3,672 \n0.71\n\n\nspam\n1,499 \n0.29\n\n\n\n\n\n\n\n\n\nIt is useful also a visual cue.\n\nCodeg &lt;- ggplot(df_g) +\n  geom_col(aes(x = freq_abs, y = label, fill = label)) +\n  scale_fill_brewer(palette = \"Set1\", direction = -1) +\n  theme_minimal() +\n  ggtitle(\"Absolute frequency barchart\") +\n  xlab(\"Absolute frequency\") +\n  ylab(\"Label\") +\n  labs(fill = \"Label\")\nggplotly(g)\n\n\n\n\n\n\nFigure 1: Absolute frequency barchart\n\n\n\nThe dataset is not balanced, so it could be relevant when training the model for classification. Depending on the model performance, we know what we could investigate first."
  },
  {
    "objectID": "posts/spam_detection/spam_detection.html#classification",
    "href": "posts/spam_detection/spam_detection.html#classification",
    "title": "Spam Detection",
    "section": "\n5.1 Classification",
    "text": "5.1 Classification\nThe text is already preprocessed as list of lemmas. For the classification task, it is necessary to convert it as a string.\n\nCodedf[\"feature\"] = df.lemmas.apply(lambda x : \" \".join(x))\n\n\n\n5.1.1 Features\nAs said, the machine does not understand human readable text. It has to be transformed. The best approach is to vectorize it with TfidfVectorizer(). It is a tool for converting text into a matrix of TF-IDF features. The TermFrequency-InverseDocumentFrequency is a statistical method. It is a measure of importance of a word in a document, part of a corpus, adjusted for the frequency in the corpus. The model vectorize a word by multiplying the word Term Frequency\n\\[\nTF = \\frac{word\\ frequency\\ in\\ document}{total\\ words\\ in\\ document}\n\\] with the Inverse Document Frequency\n\\[\nIDF = log(\\frac{total\\ number\\ documents}{documents\\ containing\\ the\\ word})\n\\] The final result is\n\\[\nTF-IDF = TF * IDF\n\\]\nThe resulting score represents the importance of a word. It dependes on the word frequency both in a specific document and in the corpus.\n::: {.callout-note collapse:“true”} An example can be useful. If a word t appears 20 times in a document of 100 words, we have\n\\[\nTF = \\frac{20}{100}=0.2\n\\]\nIf there are 10.000 documents in the corpus and 100 documents contains the term t\n\\[\nIDF = log(\\frac{10000}{100})=2\n\\]\nThis means the score is\n\\[\nTF-IDF=0.2*2=0.4\n\\] :::\n\nCodevectorizer = TfidfVectorizer()\nX = vectorizer.fit_transform(df['feature'])\ny = df.label"
  },
  {
    "objectID": "posts/spam_detection/spam_detection.html#split",
    "href": "posts/spam_detection/spam_detection.html#split",
    "title": "Spam Detection",
    "section": "\n5.2 Split",
    "text": "5.2 Split\nNot much to say about this: a best practice which let evaluate the performance of our model on new data.\n\nCodextrain, xtest, ytrain, ytest = train_test_split(X, y, test_size=.2, random_state=42)\n\n\n\n5.2.1 Model\nThe model is the MLPClassifier(). It is a Multi Perceptron Layer Classifier.\n\n\nMLPClassifier\n\nIt is an Artificial Neural Network used for classification. It consists of multiple layers of nodes, called perceptrons. For further reading, see the documentation.\n\nCodemlp = config.get_sklearn_mlp(\n  activation=\"logistic\",\n  solver=\"adam\",\n  max_iter=100,\n  hidden_layer_sizes=(100,),\n  tol=.005\n  )\n\n\n\n5.2.2 Fit\n\nCodemlp.fit(xtrain, ytrain)\n\n\n\n\nMLPClassifier(activation='logistic', max_iter=100, tol=0.005, verbose=True)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n  MLPClassifier?Documentation for MLPClassifieriFittedMLPClassifier(activation='logistic', max_iter=100, tol=0.005, verbose=True) \n\n\n\n\n5.2.3 Predict\n\nCodeypred = mlp.predict(xtest)\n\n\n\n5.2.4 Classification report\nConsidering we are doing a classificatoin, one method to evaluate the performance is the classification report. It summarize the performance of the model comparing true and predicted labels, showing not only the metrics (precision, recall and F1-score) but also the support.\n\nCodecr = classification_report(\n  ytest,\n  ypred,\n  target_names=[\"spam\", \"ham\"],\n  digits=4,\n  output_dict=True\n  )\ndf_cr = pd.DataFrame.from_dict(cr).reset_index()\n\n\n\nCodelibrary(reticulate)\n\ndf_cr &lt;- py$df_cr %&gt;% dplyr::rename(names = index)\ncols &lt;- df_cr %&gt;% colnames()\ndf_cr %&gt;% \n  pivot_longer(\n    cols = -names,\n    names_to = \"metrics\",\n    values_to = \"values\"\n  ) %&gt;% \n  pivot_wider(\n    names_from = names,\n    values_from = values\n  ) %&gt;% \n  gt() %&gt;%\n  tab_header(\n    title = \"Confusion Matrix\",\n    subtitle = \"Sklearn MLPClassifier\"\n  ) %&gt;% \n  fmt_number(\n    columns = c(\"precision\", \"recall\", \"f1-score\", \"support\"),\n    decimals = 2,\n    drop_trailing_zeros = TRUE,\n    drop_trailing_dec_mark = FALSE\n  ) %&gt;% \n  cols_align(\n    align = \"center\",\n    columns = c(\"precision\", \"recall\", \"f1-score\", \"support\")\n  ) %&gt;% \n  cols_align(\n    align = \"left\",\n    columns = metrics\n  ) %&gt;% \n  cols_label(\n    metrics = \"Metrics\",\n    precision = \"Precision\",\n    recall = \"Recall\",\n    `f1-score` = \"F1-Score\",\n    support = \"Support\"\n  )\n\n\n\n\n\n\nConfusion Matrix\n\n\nSklearn MLPClassifier\n\n\nMetrics\nPrecision\nRecall\nF1-Score\nSupport\n\n\n\n\nspam\n0.98\n0.98\n0.98\n742.\n\n\nham\n0.96\n0.94\n0.95\n293.\n\n\naccuracy\n0.97\n0.97\n0.97\n0.97\n\n\nmacro avg\n0.97\n0.96\n0.97\n1,035.\n\n\nweighted avg\n0.97\n0.97\n0.97\n1,035.\n\n\n\n\n\n\nEven if the model is not fitted for an unbalanced dataset, it is not affecting the performance. Precision and Recall are high, so much that is could seems to be overfitted."
  },
  {
    "objectID": "posts/spam_detection/spam_detection.html#topic-modeling-for-spam-content",
    "href": "posts/spam_detection/spam_detection.html#topic-modeling-for-spam-content",
    "title": "Spam Detection",
    "section": "\n5.3 Topic Modeling for spam content",
    "text": "5.3 Topic Modeling for spam content\nTopic modeling in nlp can count on the Latent Dirilicht Model. It is a generative model used to get the topics which occur in a set of documents.\nThe LDA model has:\n\nInput: a corpus of text documents, preprocessed as tokenized and cleaned words. We have this in the lemmas column.\nOutput: a distribution of topics for each document and one of words for each topic.\n\nFor further reading, you can find the paper in the Table of Contents or at this link.\n\n5.3.1 Dataset\nFilter data to have a spam dataframe and create a variable with the lemmas column to work with.\n\nCodespam = df[df.label == \"spam\"]\nx = spam.lemmas\n\n\n\n5.3.2 Create corpus\nLDA algortihm needs the corpus as a bag of word.\n\nCodeid2word = corpora.Dictionary(x)\ncorpus = [id2word.doc2bow(t) for t in x]\n\n\n\n5.3.3 Model\nThe model will return a user defined number of topics. For each of it, it will return a user defined number of words ad the probability of each of them.\n\nCodelda = config.get_lda(\n  corpus=corpus,\n  id2word=id2word,\n  num_topics=10,\n  passes=10, # number of times the algorithm see the corpus\n  workers=4, # parellalize\n  random_state=42\n  )\ntopic_words = lda.show_topics(num_topics=10, num_words=5, formatted=False)\n\n# Iterate over topic_words to extract the data\ndata = []\ndata = [\n    (topic, w, p)\n    for topic, words in topic_words\n    for w, p in words\n    ]\ntopics_df = pd.DataFrame(data, columns=['topic', 'word', 'proba'])\n\n\n\nCodepy$topics_df %&gt;% \n  gt() %&gt;% \n  tab_header(\n    title = \"Words and probabilities by topics\"\n  ) %&gt;% \n  fmt_auto() %&gt;% \n  cols_width(\n    topic ~ pct(33),\n    word ~ pct(33),\n    proba ~ pct(33)\n    ) %&gt;% \n  cols_align(\n    align = \"center\",\n    columns = c(topic, word, proba)\n  ) %&gt;% \n  cols_label(\n    topic = \"Topic\",\n    word = \"Word\",\n    proba = \"Probability\"\n  ) %&gt;% \n  tab_options(\n    table.width = pct(100)\n  )\n\n\nTable 2: Topics id, words and probabilities\n\n\n\n\n\n\n\n\n\n\n\n\nWords and probabilities by topics\n\n\nTopic\nWord\nProbability\n\n\n\n\n0 \naccount\n0.009\n\n\n0 \nmoney\n0.007\n\n\n0 \nemail\n0.006\n\n\n0 \nplease\n0.006\n\n\n0 \nclaim\n0.006\n\n\n1 \ncompany\n0.023\n\n\n1 \nstatement\n0.014\n\n\n1 \nstock\n0.012\n\n\n1 \ninformation\n0.009\n\n\n1 \nreport\n0.008\n\n\n2 \nadobe\n0.01 \n\n\n2 \nprice\n0.009\n\n\n2 \nprofessional\n0.008\n\n\n2 \nmicrosoft\n0.008\n\n\n2 \noffice\n0.007\n\n\n3 \nmoopid\n0.007\n\n\n3 \nhotlist\n0.007\n\n\n3 \nimage\n0.005\n\n\n3 \nwidth\n0.005\n\n\n3 \nemail\n0.005\n\n\n4 \nemail\n0.005\n\n\n4 \ninternational\n0.005\n\n\n4 \ncall\n0.004\n\n\n4 \nbusiness\n0.004\n\n\n4 \nservice\n0.004\n\n\n5 \nprice\n0.008\n\n\n5 \noffer\n0.007\n\n\n5 \nmoney\n0.005\n\n\n5 \nemail\n0.004\n\n\n5 \nmillion\n0.003\n\n\n6 \nbingo\n0.003\n\n\n6 \nplease\n0.002\n\n\n6 \nrefurb\n0.002\n\n\n6 \nprice\n0.002\n\n\n6 \nremove\n0.002\n\n\n7 \nheight\n0.017\n\n\n7 \nwidth\n0.013\n\n\n7 \ncomputron\n0.012\n\n\n7 \nalign\n0.009\n\n\n7 \ncolor\n0.009\n\n\n8 \ncialis\n0.007\n\n\n8 \nhour\n0.004\n\n\n8 \nbrand\n0.003\n\n\n8 \nnormally\n0.002\n\n\n8 \nviagra\n0.002\n\n\n9 \nonline\n0.007\n\n\n9 \nprescription\n0.007\n\n\n9 \nclick\n0.005\n\n\n9 \norder\n0.005\n\n\n9 \nviagra\n0.005"
  },
  {
    "objectID": "posts/spam_detection/spam_detection.html#semantic-distance-between-topics",
    "href": "posts/spam_detection/spam_detection.html#semantic-distance-between-topics",
    "title": "Spam Detection",
    "section": "\n5.4 Semantic distance between topics",
    "text": "5.4 Semantic distance between topics\nThe semantic distance requires a documents made of strings. Using a dict, we can extract the topics and the words. The dict has the topics as keys and the words as a list of words. The documents can be created using the .join().\n\nCodetopics_dict = {}\nfor topics, words in topic_words:\n  topics_dict[topics] = [w[0] for w in words]\n\ndocuments = [\" \".join(words) for words in topics_dict.values()]\n\n\n\nCode\nvectorizer = TfidfVectorizer()\ntfidf_matrix = vectorizer.fit_transform(documents)\ncosine_sim_matrix = cosine_similarity(tfidf_matrix, dense_output=True)\ntopics = list(topics_dict.keys())\ncosine_sim_df = pd.DataFrame(cosine_sim_matrix, index=topics, columns=topics)\n\n\n\nCodepy$cosine_sim_df %&gt;% \n  gt() %&gt;% \n  fmt_auto() %&gt;% \n  cols_align(\n    align = \"center\",\n    columns = c(`0`, `1`, `2`, `3`, `4`, `5`, `6`, `7`, `8`, `9`)\n  ) %&gt;% \n  cols_label(\n    `0` = \"Topic 0\",\n    `1` = \"Topic 1\",\n    `2` = \"Topic 2\",\n    `3` = \"Topic 3\",\n    `4` = \"Topic 4\",\n    `5` = \"Topic 5\",\n    `6` = \"Topic 6\",\n    `7` = \"Topic 7\",\n    `8` = \"Topic 8\",\n    `9` = \"Topic 9\",\n  ) %&gt;% \n  tab_options(\n    table.width = pct(100)\n    )\n\n\nTable 3: Cosine similarity matrix\n\n\n\n\n\n\nTopic 0\nTopic 1\nTopic 2\nTopic 3\nTopic 4\nTopic 5\nTopic 6\nTopic 7\nTopic 8\nTopic 9\n\n\n\n1    \n0 \n0    \n0.109\n0.105\n0.305\n0.177\n0    \n0    \n0    \n\n\n0    \n1 \n0    \n0    \n0    \n0    \n0    \n0    \n0    \n0    \n\n\n0    \n0 \n1    \n0    \n0    \n0.135\n0.125\n0    \n0    \n0    \n\n\n0.109\n0 \n0    \n1    \n0.102\n0.111\n0    \n0.163\n0    \n0    \n\n\n0.105\n0 \n0    \n0.102\n1    \n0.108\n0    \n0    \n0    \n0    \n\n\n0.305\n0 \n0.135\n0.111\n0.108\n1    \n0.139\n0    \n0    \n0    \n\n\n0.177\n0 \n0.125\n0    \n0    \n0.139\n1    \n0    \n0    \n0    \n\n\n0    \n0 \n0    \n0.163\n0    \n0    \n0    \n1    \n0    \n0    \n\n\n0    \n0 \n0    \n0    \n0    \n0    \n0    \n0    \n1    \n0.153\n\n\n0    \n0 \n0    \n0    \n0    \n0    \n0    \n0    \n0.153\n1"
  },
  {
    "objectID": "posts/spam_detection/spam_detection.html#organization-of-ham-mails",
    "href": "posts/spam_detection/spam_detection.html#organization-of-ham-mails",
    "title": "Spam Detection",
    "section": "\n5.5 Organization of “HAM” mails",
    "text": "5.5 Organization of “HAM” mails\n\n5.5.1 Create “HAM” df\n\n5.5.2 Get ham lemmas which have ORG entity\n\nCodeham = df[df.label == \"ham\"]\nx = ham.entities\n\n\n\nCodefrom collections import Counter\n\n# Flatten the list of lists and create a Counter object\nd = Counter([i for e in x for i in e])\n\nfreq_df = pd.DataFrame(d.items(), columns=[\"word\", \"freq\"])\n\n\n\nCodelibrary(\"wordcloud\")\n\nLoading required package: RColorBrewer\n\nCodelibrary(\"RColorBrewer\")\n\nset.seed(42)\nword_freqs_df &lt;- py$freq_df\n\nwordcloud(\n  words = word_freqs_df$word,\n  freq = word_freqs_df$freq,\n  min.freq = 1,\n  max.words = 100, # nrow(word_freqs_df),\n  random.order = FALSE,\n  rot.per = .3,\n  colors = brewer.pal(n = 8, name = \"Dark2\")\n  )\n\n\n\n\n\n\nFigure 2: Wordcloud\n\n\n\n\n\nCodeword_freqs_df %&gt;% \n  arrange(desc(freq)) %&gt;% \n  head(10) %&gt;% \n  gt() %&gt;%                                        \n  tab_header(\n    title = \"Top 10 ham words \",\n    subtitle = \"by frequency\"\n  ) %&gt;% \n  fmt_auto() %&gt;% \n  cols_width(\n    word ~ pct(50),\n    freq ~ pct(50)\n    ) %&gt;% \n  cols_align(\n    align = \"center\",\n    columns = c(word, freq)\n  ) %&gt;% \n  cols_label(\n    word = \"Word\",\n    freq = \"Frequency\"\n  ) %&gt;% \n  tab_options(\n    table.width = pct(100)\n  )\n\n\nTable 4: Top 10 words by frequency\n\n\n\n\n\n\n\n\n\n\n\nTop 10 ham words\n\n\nby frequency\n\n\nWord\nFrequency\n\n\n\n\nenron\n4,257 \n\n\nnorth\n  285 \n\n\namerica\n  284 \n\n\ntexas\n  196 \n\n\nclynes\n  194 \n\n\nenergy\n  148 \n\n\naimee\n  131 \n\n\nexxon\n  113 \n\n\nyahoo\n  102 \n\n\nlannou\n  101"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Simone Brazzi",
    "section": "",
    "text": "I am a (data) nerd, with lot of passion and some mistake along the way. Data analyst by day, aspiring data wizard by night! I love using data to tell stories and drive business decisions. But I’m not content with stopping there. My passion for data and desire to expand my skill set has led me on a quest to become a data scientist. I’m a fearless problem solver with an insatiable curiosity, and I’m always seeking new challenges and opportunities to learn and grow. Let’s make some magic with data! When I don’t stare to lines of code or spreadsheet, I like to read and play video game or spend quality time with my dog."
  },
  {
    "objectID": "posts/credit_score/credit_score_kaggle.html",
    "href": "posts/credit_score/credit_score_kaggle.html",
    "title": "Import",
    "section": "",
    "text": "Code\nimport pandas as pdimport numpy as npimport matplotlib.pyplot as pltimport pickleimport shapimport timefrom datetime import datefrom dateutil.relativedelta import relativedeltafrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCVfrom sklearn.compose import ColumnTransformerfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler, LabelBinarizerfrom sklearn.pipeline import Pipelinefrom sklearn.tree import DecisionTreeClassifierfrom xgboost import XGBClassifierfrom sklearn.ensemble import RandomForestClassifierfrom sklearn.metrics import classification_report\nCode\nclass Config():  def __init__(self):    \"\"\"    Initialization calss.    \"\"\"    self.path=\"/kaggle/working/\"    self.file=\"/kaggle/input/credit-scoring/credit_scoring.csv\"    self.model_result_path=\"/kaggle/working/\"    self.random_state=42    self.col_bi = ['code_gender', 'flag_own_car', 'flag_own_realty', 'flag_mobil', 'flag_work_phone', 'flag_phone', 'flag_email']    self.col_ohe = [\"name_income_type\", \"name_education_type\", \"name_family_status\", \"name_housing_type\", \"occupation_type\"]    self.col_ss = ['cnt_children', 'amt_income_total', 'cnt_fam_members']    self.features = self.col_bi + self.col_ohe + self.col_ssconfig = Config()\nCode\ndf = pd.read_csv(config.file)df.columns = df.columns.str.lower()df[\"id\"] = df.id.astype(int)\nCode\ndf = df[df.id != 6392180]df = df[~df.occupation_type.isna()]"
  },
  {
    "objectID": "posts/credit_score/credit_score_kaggle.html#fit",
    "href": "posts/credit_score/credit_score_kaggle.html#fit",
    "title": "Import",
    "section": "Fit",
    "text": "Fit\n\n\nCode\ngs_dtc = grid_search_dtc.fit(xtrain, ytrain)"
  },
  {
    "objectID": "posts/credit_score/credit_score_kaggle.html#pickle",
    "href": "posts/credit_score/credit_score_kaggle.html#pickle",
    "title": "Import",
    "section": "Pickle",
    "text": "Pickle\n\n\nCode\nwith open(config.model_result_path + \"credit_score_grid_search_dtc.pkl\", \"wb\") as f:  pickle.dump(gs_dtc, f)\n\n\n\n\nCode\nwith open(config.model_result_path + \"credit_score_grid_search_dtc.pkl\", \"rb\") as f:  gs_dtc = pickle.load(f)dtc_model = gs_dtc.best_estimator_"
  },
  {
    "objectID": "posts/credit_score/credit_score_kaggle.html#predict-classification-report",
    "href": "posts/credit_score/credit_score_kaggle.html#predict-classification-report",
    "title": "Import",
    "section": "Predict & Classification Report",
    "text": "Predict & Classification Report\n\n\nCode\nypred_dtc = dtc_model.predict(xtest)cr = classification_report(  ytest,  ypred_dtc,  # target_names=config.labels,  digits=4,  output_dict=True  )df_cr = pd.DataFrame.from_dict(cr).reset_index()df_cr\n\n\n\n\n\n\n\n\n\nindex\n0\n1\naccuracy\nmacro avg\nweighted avg\n\n\n\n\n0\nprecision\n0.971134\n0.123114\n0.268073\n0.547124\n0.880392\n\n\n1\nrecall\n0.185895\n0.953886\n0.268073\n0.569891\n0.268073\n\n\n2\nf1-score\n0.312056\n0.218082\n0.268073\n0.265069\n0.302001\n\n\n3\nsupport\n41986.000000\n5031.000000\n0.268073\n47017.000000\n47017.000000"
  },
  {
    "objectID": "posts/credit_score/credit_score_kaggle.html#fit-1",
    "href": "posts/credit_score/credit_score_kaggle.html#fit-1",
    "title": "Import",
    "section": "Fit",
    "text": "Fit\n\n\nCode\nstart_time = time.time()rs_rfc = random_search_rfc.fit(xtrain, ytrain)end_time = time.time()elapsed_time = end_time - start_timeprint(f\"Elapsed time: {elapsed_time} seconds\")print(f\"Elapsed time: {elapsed_time / 60} minutes\")\n\n\nElapsed time: 1071.030502319336 seconds"
  },
  {
    "objectID": "posts/credit_score/credit_score_kaggle.html#pickle-1",
    "href": "posts/credit_score/credit_score_kaggle.html#pickle-1",
    "title": "Import",
    "section": "Pickle",
    "text": "Pickle\n\n\nCode\nwith open(config.model_result_path + \"credit_score_grid_search_rfc.pkl\", \"wb\") as f:  pickle.dump(rs_rfc, f)\n\n\n\n\nCode\nwith open(config.model_result_path + \"credit_score_grid_search_rfc.pkl\", \"rb\") as f:  gs_rfc = pickle.load(f)rfc_model = gs_rfc.best_estimator_"
  },
  {
    "objectID": "posts/credit_score/credit_score_kaggle.html#predict",
    "href": "posts/credit_score/credit_score_kaggle.html#predict",
    "title": "Import",
    "section": "Predict",
    "text": "Predict\n\n\nCode\nypred_rfc = rfc_model.predict(xtest)cr_rfc = classification_report(  ytest,  ypred_rfc,  # target_names=config.labels,  digits=4,  output_dict=True  )df_cr_rfc = pd.DataFrame.from_dict(cr_rfc).reset_index()df_cr_rfc\n\n\n\n\n\n\n\n\n\nindex\n0\n1\naccuracy\nmacro avg\nweighted avg\n\n\n\n\n0\nprecision\n0.992840\n0.316549\n0.774592\n0.654695\n0.920474\n\n\n1\nrecall\n0.753013\n0.954681\n0.774592\n0.853847\n0.774592\n\n\n2\nf1-score\n0.856454\n0.475450\n0.774592\n0.665952\n0.815685\n\n\n3\nsupport\n41986.000000\n5031.000000\n0.774592\n47017.000000\n47017.000000"
  },
  {
    "objectID": "posts/credit_score/credit_score_kaggle.html#pipeline",
    "href": "posts/credit_score/credit_score_kaggle.html#pipeline",
    "title": "Import",
    "section": "Pipeline",
    "text": "Pipeline\n\n\nCode\n# classifierxgb = XGBClassifier(n_jobs=-1)# preprocessorpreprocessor = ColumnTransformer(  transformers=[    (\"binary_encoder\", ohe_bi, config.col_bi),    (\"categorical_encoder\", ohe, config.col_ohe),    (\"scaler\", ss, config.col_ss)    ],    remainder='passthrough'    )# pipelinepipe_xgb = Pipeline(  steps=[    (\"preprocessor\", preprocessor),    (\"xgb\", xgb)    ]    )# parametersparam_dist_xgb = {  \"xgb__n_estimators\" : [100, 150, 200, 300],  \"xgb__max_depth\" : [3, 5, 7, 10],  \"xgb__learning_rate\" : [0.1, 0.1, 0.01, 0.0002],  \"xgb__subsample\": [0.7, 0.8, 0.9],  \"xgb__colsample_bytree\": [0.7, 0.8, 0.9],  \"xgb__gamma\": [0, 0.1],  \"xgb__alpha\": [0, 0.1],  # Adding slight L1 regularization for simplicity  \"xgb__lambda\": [1, 2]    # Adding slight L2 regularization for stability}# scoringscoring = {    \"accuracy\": \"accuracy\",    \"f1\": \"f1\",    \"roc_auc\": \"roc_auc\"}# random search cvrandom_search_xgb = RandomizedSearchCV(    estimator=pipe_xgb,    param_distributions=param_dist_xgb,    n_iter=20,  # Set the number of parameter combinations to try    cv=7,    scoring=scoring,    refit=\"accuracy\",    n_jobs=-1,    random_state=42    )"
  },
  {
    "objectID": "posts/credit_score/credit_score_kaggle.html#fit-2",
    "href": "posts/credit_score/credit_score_kaggle.html#fit-2",
    "title": "Import",
    "section": "Fit",
    "text": "Fit\n\n\nCode\nstart_time = time.time()rs_xgb = random_search_xgb.fit(xtrain, ytrain)end_time = time.time()elapsed_time = end_time - start_timeprint(f\"Elapsed time: {elapsed_time} seconds\")print(f\"Elapsed time: {elapsed_time / 60} minutes\")\n\n\nElapsed time: 336.31454849243164 seconds\nElapsed time: 5.605242474873861 minutes"
  },
  {
    "objectID": "posts/credit_score/credit_score_kaggle.html#pickle-2",
    "href": "posts/credit_score/credit_score_kaggle.html#pickle-2",
    "title": "Import",
    "section": "Pickle",
    "text": "Pickle\n\n\nCode\nwith open(config.model_result_path + \"credit_score_random_search_xgb.pkl\", \"wb\") as f:  pickle.dump(rs_xgb, f)\n\n\n\n\nCode\nwith open(config.model_result_path + \"credit_score_random_search_xgb.pkl\", \"rb\") as f:  rs_xgb = pickle.load(f)xgb_model = rs_xgb.best_estimator_"
  },
  {
    "objectID": "posts/credit_score/credit_score_kaggle.html#predict-1",
    "href": "posts/credit_score/credit_score_kaggle.html#predict-1",
    "title": "Import",
    "section": "Predict",
    "text": "Predict\n\n\nCode\nypred_xgb = xgb_model.predict(xtest)cr_xgb = classification_report(  ytest,  ypred_xgb,  # target_names=config.labels,  digits=4,  output_dict=True  )df_cr_xgb = pd.DataFrame.from_dict(cr_xgb).reset_index()df_cr_xgb\n\n\n\n\n\n\n\n\n\nindex\n0\n1\naccuracy\nmacro avg\nweighted avg\n\n\n\n\n0\nprecision\n0.919670\n0.663104\n0.907927\n0.791387\n0.892217\n\n\n1\nrecall\n0.982732\n0.283641\n0.907927\n0.633187\n0.907927\n\n\n2\nf1-score\n0.950156\n0.397327\n0.907927\n0.673742\n0.891001\n\n\n3\nsupport\n41986.000000\n5031.000000\n0.907927\n47017.000000\n47017.000000"
  },
  {
    "objectID": "posts/credit_score/credit_score.html",
    "href": "posts/credit_score/credit_score.html",
    "title": "Credit Score",
    "section": "",
    "text": "Create a model capable of estimating the creditworthiness of customers, in order to help the dedicated team understand whether or not to accept a credit card application.\n\n\nID: customer identification number\nCODE_GENDER: gender of the customer\nFLAGOWNCAR: indicator of car ownership\nFLAGOWNREALTY: indicator of house ownership\nCNT_CHILDREN: number of children\nAMTINCOMETOTAL: annual income\nNAMEINCOMETYPE: type of income\nNAMEEDUCATIONTYPE: level of education\nNAMEFAMILYSTATUS: marital status\nNAMEHOUSINGTYPE: type of dwelling\nDAYS_BIRTH: number of days since birth\nDAYS_EMPLOYED: number of days since employment (if positive, indicates the number of days since being unemployed)\nFLAG_MOBIL: indicator for the presence of a mobile phone number\nFLAGWORKPHONE: indicator of the presence of a work phone number\nFLAG_PHONE: indicator of the presence of a telephone number\nFLAG_EMAIL: indicator for the presence of an email address\nOCCUPATION_TYPE: type of occupation\nCNTFAMMEMBERS: number of family members\nTARGET: variable which is worth 1 if the customer has a high creditworthiness (constant payment of installments), 0 otherwise.\n\nIf a customer is denied a credit card, the team must be able to give a reason. This means that your model must provide indications that are easy to interpret.\nIt is a binary classification which needs a good explainability."
  },
  {
    "objectID": "posts/credit_score/credit_score.html#features",
    "href": "posts/credit_score/credit_score.html#features",
    "title": "Credit Score",
    "section": "",
    "text": "ID: customer identification number\nCODE_GENDER: gender of the customer\nFLAGOWNCAR: indicator of car ownership\nFLAGOWNREALTY: indicator of house ownership\nCNT_CHILDREN: number of children\nAMTINCOMETOTAL: annual income\nNAMEINCOMETYPE: type of income\nNAMEEDUCATIONTYPE: level of education\nNAMEFAMILYSTATUS: marital status\nNAMEHOUSINGTYPE: type of dwelling\nDAYS_BIRTH: number of days since birth\nDAYS_EMPLOYED: number of days since employment (if positive, indicates the number of days since being unemployed)\nFLAG_MOBIL: indicator for the presence of a mobile phone number\nFLAGWORKPHONE: indicator of the presence of a work phone number\nFLAG_PHONE: indicator of the presence of a telephone number\nFLAG_EMAIL: indicator for the presence of an email address\nOCCUPATION_TYPE: type of occupation\nCNTFAMMEMBERS: number of family members\nTARGET: variable which is worth 1 if the customer has a high creditworthiness (constant payment of installments), 0 otherwise.\n\nIf a customer is denied a credit card, the team must be able to give a reason. This means that your model must provide indications that are easy to interpret.\nIt is a binary classification which needs a good explainability."
  },
  {
    "objectID": "posts/credit_score/credit_score.html#r",
    "href": "posts/credit_score/credit_score.html#r",
    "title": "Credit Score",
    "section": "\n2.1 R",
    "text": "2.1 R\n\nCodelibrary(tidyverse, verbose = FALSE)\nlibrary(ggplot2)\nlibrary(plotly)\nlibrary(reticulate)\nlibrary(gt)\nlibrary(scales)"
  },
  {
    "objectID": "posts/credit_score/credit_score.html#python",
    "href": "posts/credit_score/credit_score.html#python",
    "title": "Credit Score",
    "section": "\n2.2 Python",
    "text": "2.2 Python\n\nCodeimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pickle\n\nfrom datetime import date\nfrom dateutil.relativedelta import relativedelta\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler, LabelBinarizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report"
  },
  {
    "objectID": "posts/credit_score/credit_score.html#config-class",
    "href": "posts/credit_score/credit_score.html#config-class",
    "title": "Credit Score",
    "section": "\n2.3 Config class",
    "text": "2.3 Config class\n\nCodeclass Config():\n  \n  def __init__(self):\n    \"\"\"\n    Initialization calss.\n    \"\"\"\n    \n    self.path=\"/Users/simonebrazzi/R/blog/posts/credit_score/\"\n    self.file=\"/Users/simonebrazzi/R/blog/posts/credit_score/credit_scoring.csv\"\n    self.random_state=42\n    self.col_bi = ['code_gender', 'flag_own_car', 'flag_own_realty', 'flag_mobil', 'flag_work_phone', 'flag_phone', 'flag_email']\n    self.col_ohe = [\"name_income_type\", \"name_education_type\", \"name_family_status\", \"name_housing_type\", \"occupation_type\"]\n    self.col_ss = ['cnt_children', 'amt_income_total', 'cnt_fam_members']\n    self.features = self.col_bi + self.col_ohe + self.col_ss\n\n  \nconfig = Config()"
  },
  {
    "objectID": "posts/credit_score/credit_score.html#target",
    "href": "posts/credit_score/credit_score.html#target",
    "title": "Credit Score",
    "section": "\n4.1 Target",
    "text": "4.1 Target\nIt is always important to perform an EDA. In this case I am expecting to have less cases of high creditworthiness than low leveraging personal knowledge. Xonsidering it is a binary classification, it is important to be sure if data are balanced to make a better model selection.\nThe variable:\n\n0 has 308,705.\n1 has 29,722.\n\n\nCodedf &lt;- py$df\n\ndf_g &lt;- df %&gt;% \n  rename(label = target) %&gt;% \n  mutate(label = as.factor(label)) %&gt;% \n  summarize(\n    freq_abs = n(),\n    freq_rel = n() / nrow(df),\n    .by = label\n  )\ndf_g %&gt;% \n  gt() %&gt;% \n  fmt_auto() %&gt;% \n  cols_width(\n    label ~ pct(20),\n    freq_abs ~ pct(35),\n    freq_rel ~ pct(35)\n    ) %&gt;% \n  cols_align(\n    align = \"center\",\n    columns = c(freq_abs, freq_rel)\n  ) %&gt;% \n  tab_header(\n    title = \"Label frequency\",\n    subtitle = \"Absolute and relative frequencies\"\n  ) %&gt;% \n  cols_label(\n    label = \"Label\",\n    freq_abs = \"Absolute frequency\",\n    freq_rel = \"Relative frequency\"\n  ) %&gt;% \n  tab_options(\n    table.width = pct(100)\n    )\n\n\nTable 1: Absolute and relative frequencies\n\n\n\n\n\n\n\n\n\n\n\n\nLabel frequency\n\n\nAbsolute and relative frequencies\n\n\nLabel\nAbsolute frequency\nRelative frequency\n\n\n\n\n0\n308,705 \n0.912\n\n\n1\n 29,722 \n0.088\n\n\n\n\n\n\n\n\n\n\nCodeg &lt;- ggplot(df_g) +\n  geom_bar(aes(x = label, y = freq_abs, fill = label), stat = \"identity\") +\n  ggtitle(\"Absolute frequency barchart\") +\n  xlab(\"Label\") +\n  ylab(\"Absolute frequency\") +\n  labs(fill = \"Label\") +\n  scale_y_continuous(breaks = scales::breaks_pretty(n = 5)) +\n  theme_minimal()\nggplotly(g)\n\n\n\n\n\n\nFigure 1: Absolute frequency barchart\n\n\n\nRemember: 0. stands for low creditworthiness, 1 for high. Our dataset is strongly unbalanced towards the 0 label. So, my first idea was right."
  },
  {
    "objectID": "posts/credit_score/credit_score.html#features-1",
    "href": "posts/credit_score/credit_score.html#features-1",
    "title": "Credit Score",
    "section": "\n4.2 Features",
    "text": "4.2 Features\nLets check the NA values in each feature.\n\nCodedf_na &lt;- df %&gt;%\n  summarize(across(everything(), ~ sum(is.na(.)))) %&gt;%\n  pivot_longer(\n    cols = everything(),\n    names_to = \"columns\",\n    values_to = \"value\"\n  )\ndf_na %&gt;% \n  gt() %&gt;% \n  tab_options(\n    table.width = pct(100)\n    ) %&gt;% \n  cols_align(align = \"center\") %&gt;% \n  cols_label(\n    columns = \"Columns\",\n    value = \"NA Value\"\n  ) %&gt;% \n  tab_header(\n    title = \"NA values\",\n    subtitle = \"Number of NA value for each feature\"\n  ) %&gt;% \n  fmt_number(\n    columns = c(\"value\"),\n    decimals = 2,\n    drop_trailing_zeros = TRUE,\n    #drop_trailing_dec_mark = FALSE\n  ) \n\n\nTable 2: Number of NA values for each feature\n\n\n\n\n\n\n\nNA values\n\n\nNumber of NA value for each feature\n\n\nColumns\nNA Value\n\n\n\n\nid\n0\n\n\ncode_gender\n0\n\n\nflag_own_car\n0\n\n\nflag_own_realty\n0\n\n\ncnt_children\n0\n\n\namt_income_total\n0\n\n\nname_income_type\n0\n\n\nname_education_type\n0\n\n\nname_family_status\n1\n\n\nname_housing_type\n1\n\n\ndays_birth\n1\n\n\ndays_employed\n1\n\n\nflag_mobil\n1\n\n\nflag_work_phone\n1\n\n\nflag_phone\n1\n\n\nflag_email\n1\n\n\noccupation_type\n103,342\n\n\ncnt_fam_members\n1\n\n\ntarget\n0\n\n\n\n\n\n\n\n\n\nThere are 2 interesting things to notice:\nThe column occupation_type has lots of NA value. How much in proportion to the total? Should we drop the NAs or keep them?\n\nCodedf %&gt;%\n  summarize(\n    freq_abs = sum(is.na(occupation_type)),\n    freq_rel = sum(is.na(occupation_type)) / nrow(df)\n    ) %&gt;% \n  gt() %&gt;% \n  tab_options(\n    table.width = pct(100)\n    ) %&gt;% \n  cols_align(align = \"center\") %&gt;% \n  cols_label(\n    freq_abs = \"Absolute Frequency\",\n    freq_rel = \"Relative Frequency\"\n  ) %&gt;% \n  tab_header(\n    title = \"Absolute and Relative Frequency\",\n    subtitle = \"NA frequency for occupation_type\"\n  ) %&gt;% \n  fmt_number(\n    columns = c(\"freq_abs\", \"freq_rel\"),\n    decimals = 2,\n    drop_trailing_zeros = TRUE\n  ) \n\n\nTable 3: NA frequency for occupation_type\n\n\n\n\n\n\n\nAbsolute and Relative Frequency\n\n\nNA frequency for occupation_type\n\n\nAbsolute Frequency\nRelative Frequency\n\n\n\n103,342\n0.31\n\n\n\n\n\n\n\n\n30.54% of occupation_type are NA. This could be a big concern: how to handle it? Drop it means losing lots of data, keeping it could lead to impute wrong data, which could be worse.\nLets check how the occupation_type labels frequency.\n\nCodedf %&gt;% \n  summarise(\n    n = n(),\n    .by = occupation_type\n  ) %&gt;% \n  arrange(desc(n)) %&gt;% \n  gt() %&gt;% \n  tab_options(\n    table.width = pct(100)\n    ) %&gt;% \n  cols_align(align = \"center\") %&gt;% \n  cols_label(\n    occupation_type = \"Occupation Type\",\n    n = \"Frequency\"\n  ) %&gt;% \n  tab_header(\n    title = \"Occupation Type Label Frequency\"\n  ) %&gt;% \n  fmt_number(\n    columns = c(\"n\"),\n    decimals = 2,\n    drop_trailing_zeros = TRUE\n  ) \n\n\nTable 4: Occupation type label frequency\n\n\n\n\n\n\n\nOccupation Type Label Frequency\n\n\nOccupation Type\nFrequency\n\n\n\n\nNA\n103,342\n\n\nLaborers\n60,146\n\n\nCore staff\n33,527\n\n\nSales staff\n31,652\n\n\nManagers\n27,384\n\n\nDrivers\n20,020\n\n\nHigh skill tech staff\n13,399\n\n\nAccountants\n12,281\n\n\nMedicine staff\n10,438\n\n\nCooking staff\n6,248\n\n\nSecurity staff\n6,218\n\n\nCleaning staff\n4,594\n\n\nPrivate service staff\n2,787\n\n\nLow-skill Laborers\n1,714\n\n\nSecretaries\n1,577\n\n\nWaiters/barmen staff\n1,245\n\n\nRealty agents\n852\n\n\nHR staff\n567\n\n\nIT staff\n436\n\n\n\n\n\n\n\n\n\nSo, here is the situation: we could impute the NA respecting the proportion of the other labels in the dataset or we can drop the NA values. I want to drop the NA. Even if we are losing a third of the data, we are not making assumption about the rest. Also, we still have 235085, which are plenty enough.\n\nCodedf_occ_type &lt;- df %&gt;% \n  filter(!is.na(occupation_type)) %&gt;% \n  summarise(\n    n = n(),\n    .by = occupation_type\n  ) %&gt;% \n  arrange(desc(n))\n\n# to extend the palette to the number of labels\nextended_palette &lt;- colorRampPalette(RColorBrewer::brewer.pal(9, \"Purples\"))(df_occ_type %&gt;% nrow())\n\ng &lt;- ggplot(df_occ_type) +\n  geom_col(aes(x = n, y = reorder(occupation_type, n), fill = reorder(occupation_type, n))) +\n  theme_minimal() +\n  labs(\n    x = \"Count\",\n    y = \"Labels\",\n    title = \"Occupation Type Label Frequency\"\n  ) +\n  scale_fill_manual(values = extended_palette)\n\nggplotly(g)\n\n\n\n\n\n\nFigure 2: Barplot Occupation Type Label frequency\n\n\n\nThere is a NA value in at least all the columns: is it possible it is the same observation?\nFirst of all, we can create a list of the columns name which have only 1 NA, using the df_na tibble.\n\nCodeone_cols &lt;- df_na %&gt;% \n  filter(value == 1) %&gt;% \n  pull(columns)\none_cols\n\n[1] \"name_family_status\" \"name_housing_type\"  \"days_birth\"        \n[4] \"days_employed\"      \"flag_mobil\"         \"flag_work_phone\"   \n[7] \"flag_phone\"         \"flag_email\"         \"cnt_fam_members\"   \n\n\nThen, to check multiple OR condition, we can use if_any. In this case, we are checking if multiple columns are NA, with the previous filter on only one NA in each column.\n\nCodelibrary(reticulate)\n\ndf %&gt;% \n  filter(\n    if_any(all_of(one_cols), ~ . %&gt;% is.na())\n  ) %&gt;% \n  gt()  %&gt;% \n  tab_options(\n    table.width = pct(100)\n    )\n\n\n\n\n\nid\ncode_gender\nflag_own_car\nflag_own_realty\ncnt_children\namt_income_total\nname_income_type\nname_education_type\nname_family_status\nname_housing_type\ndays_birth\ndays_employed\nflag_mobil\nflag_work_phone\nflag_phone\nflag_email\noccupation_type\ncnt_fam_members\ntarget\n\n\n6392180\nF\nN\nN\n0\n67500\nWorking\nSecondary / se\nNA\nNA\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNA\nNaN\n0\n\n\n\n\nCodepy$id_to_remove &lt;- df %&gt;% \n  filter(\n    if_any(one_cols, ~ . %&gt;% is.na())\n  ) %&gt;% \n  pull(id)\n\nWarning: Using an external vector in selections was deprecated in tidyselect 1.1.0.\nℹ Please use `all_of()` or `any_of()` instead.\n  # Was:\n  data %&gt;% select(one_cols)\n\n  # Now:\n  data %&gt;% select(all_of(one_cols))\n\nSee &lt;https://tidyselect.r-lib.org/reference/faq-external-vector.html&gt;.\n\n\nInteresting features\nLets also look at some interesting variable: click on the different tabs to see the frequency table for each of them.\n\nCode# features to check\ncols &lt;- c(\n  \"cnt_children\",\n  \"name_education_type\",\n  \"name_family_status\",\n  \"name_housing_type\",\n  \"cnt_fam_members\",\n  \"flag_own_car\",\n  \"code_gender\"\n  )\n# \ncount_results &lt;- map(cols, ~ df %&gt;% filter(!is.na(occupation_type)) %&gt;% count(.data[[.x]]))\nnames(count_results) &lt;- cols\n\n\n\n\nCount children\nEducation type\nFamily status\nHousing type\nCount family members\nOwn car\nGender\n\n\n\n\nCodecount_results$cnt_children %&gt;% \n  arrange(desc(n)) %&gt;% \n  gt() %&gt;% \n  tab_options(\n    table.width = pct(100)\n    ) %&gt;% \n  cols_align(align = \"center\") %&gt;% \n  cols_label(\n    cnt_children = \"# Children\",\n    n = \"Count\"\n  ) %&gt;% \n  tab_header(\n    title = \"Count of Number of Children\",\n  ) %&gt;% \n  fmt_number(\n    columns = c(\"n\"),\n    decimals = 2,\n    drop_trailing_zeros = TRUE,\n    #drop_trailing_dec_mark = FALSE\n  ) \n\n\nTable 5: Count of Number of Children\n\n\n\n\n\n\n\nCount of Number of Children\n\n\n# Children\nCount\n\n\n\n\n0\n148,909\n\n\n1\n56,238\n\n\n2\n26,196\n\n\n3\n3,319\n\n\n4\n300\n\n\n5\n109\n\n\n9\n4\n\n\n12\n4\n\n\n14\n3\n\n\n7\n2\n\n\n19\n1\n\n\n\n\n\n\n\n\n\n\n\n\nCodecount_results$name_education_type %&gt;% \n  arrange(desc(n)) %&gt;% \n  gt()  %&gt;% \n  tab_options(\n    table.width = pct(100)\n    ) %&gt;% \n  cols_align(align = \"center\") %&gt;% \n  cols_label(\n    name_education_type = \"Education Type\",\n    n = \"Count\"\n  ) %&gt;% \n  tab_header(\n    title = \"Count by Education Type\",\n  ) %&gt;% \n  fmt_number(\n    columns = c(\"n\"),\n    decimals = 2,\n    drop_trailing_zeros = TRUE,\n    #drop_trailing_dec_mark = FALSE\n  ) \n\n\nTable 6: Count by Education Type\n\n\n\n\n\n\n\nCount by Education Type\n\n\nEducation Type\nCount\n\n\n\n\nSecondary / secondary special\n157,822\n\n\nHigher education\n66,693\n\n\nIncomplete higher\n8,799\n\n\nLower secondary\n1,609\n\n\nAcademic degree\n162\n\n\n\n\n\n\n\n\n\n\n\n\nCodecount_results$name_family_status %&gt;% \n  arrange(desc(n)) %&gt;% \n  gt()  %&gt;% \n  tab_options(\n    table.width = pct(100)\n    ) %&gt;% \n  cols_align(align = \"center\") %&gt;% \n  cols_label(\n    name_family_status = \"Family Status\",\n    n = \"Count\"\n  ) %&gt;% \n  tab_header(\n    title = \"Count by Family Status\",\n  ) %&gt;% \n  fmt_number(\n    columns = c(\"n\"),\n    decimals = 2,\n    drop_trailing_zeros = TRUE,\n    #drop_trailing_dec_mark = FALSE\n  ) \n\n\nTable 7: Count by Family Status\n\n\n\n\n\n\n\nCount by Family Status\n\n\nFamily Status\nCount\n\n\n\n\nMarried\n164,310\n\n\nSingle / not married\n30,551\n\n\nCivil marriage\n20,968\n\n\nSeparated\n14,108\n\n\nWidow\n5,148\n\n\n\n\n\n\n\n\n\n\n\n\nCodecount_results$name_housing_type %&gt;% \n  arrange(desc(n)) %&gt;% \n  gt()  %&gt;% \n  tab_options(\n    table.width = pct(100)\n    ) %&gt;% \n  cols_align(align = \"center\") %&gt;% \n  cols_label(\n    name_housing_type = \"House Type\",\n    n = \"Count\"\n  ) %&gt;% \n  tab_header(\n    title = \"Count by House Type\",\n  ) %&gt;% \n  fmt_number(\n    columns = c(\"n\"),\n    decimals = 2,\n    drop_trailing_zeros = TRUE,\n    #drop_trailing_dec_mark = FALSE\n  ) \n\n\nTable 8: Count by House Type\n\n\n\n\n\n\n\nCount by House Type\n\n\nHouse Type\nCount\n\n\n\n\nHouse / apartment\n208,980\n\n\nWith parents\n11,938\n\n\nMunicipal apartment\n7,411\n\n\nRented apartment\n3,549\n\n\nOffice apartment\n2,270\n\n\nCo-op apartment\n937\n\n\n\n\n\n\n\n\n\n\n\n\nCodecount_results$cnt_fam_members %&gt;% \n  arrange(desc(n)) %&gt;% \n  gt()  %&gt;% \n  tab_options(\n    table.width = pct(100)\n    ) %&gt;% \n  cols_align(align = \"center\") %&gt;% \n  cols_label(\n    cnt_fam_members = \"Number of Family Members\",\n    n = \"Count\"\n  ) %&gt;% \n  tab_header(\n    title = \"Count by Number of Family Members\",\n  ) %&gt;% \n  fmt_number(\n    columns = c(\"n\"),\n    decimals = 2,\n    drop_trailing_zeros = TRUE,\n    #drop_trailing_dec_mark = FALSE\n  ) \n\n\nTable 9: Count by Number of Family Members\n\n\n\n\n\n\n\nCount by Number of Family Members\n\n\nNumber of Family Members\nCount\n\n\n\n\n2\n118,852\n\n\n3\n49,232\n\n\n1\n38,902\n\n\n4\n24,556\n\n\n5\n3,138\n\n\n6\n282\n\n\n7\n109\n\n\n11\n4\n\n\n14\n4\n\n\n15\n3\n\n\n9\n2\n\n\n20\n1\n\n\n\n\n\n\n\n\n\n\n\n\nCodecount_results$flag_own_car %&gt;% \n  arrange(desc(n)) %&gt;% \n  gt()  %&gt;% \n  tab_options(\n    table.width = pct(100)\n    ) %&gt;% \n  cols_align(align = \"center\") %&gt;% \n  cols_label(\n    flag_own_car = \"Own Car\",\n    n = \"Count\"\n  ) %&gt;% \n  tab_header(\n    title = \"Count by Own a Car\",\n  ) %&gt;% \n  fmt_number(\n    columns = c(\"n\"),\n    decimals = 2,\n    drop_trailing_zeros = TRUE,\n    #drop_trailing_dec_mark = FALSE\n  ) \n\n\nTable 10: Count by Own a Car\n\n\n\n\n\n\n\nCount by Own a Car\n\n\nOwn Car\nCount\n\n\n\n\nN\n138,203\n\n\nY\n96,882\n\n\n\n\n\n\n\n\n\n\n\n\nCodecount_results$code_gender %&gt;% \n  arrange(desc(n)) %&gt;% \n  gt() %&gt;% \n  tab_options(\n    table.width = pct(100)\n    ) %&gt;% \n  cols_align(align = \"center\") %&gt;% \n  cols_label(\n    code_gender = \"Gender\",\n    n = \"Count\"\n  ) %&gt;% \n  tab_header(\n    title = \"Count by Gender\",\n  ) %&gt;% \n  fmt_number(\n    columns = c(\"n\"),\n    decimals = 2,\n    drop_trailing_zeros = TRUE,\n    #drop_trailing_dec_mark = FALSE\n  ) \n\n\nTable 11: Count by Gender\n\n\n\n\n\n\n\nCount by Gender\n\n\nGender\nCount\n\n\n\n\nF\n147,505\n\n\nM\n87,580"
  },
  {
    "objectID": "posts/credit_score/credit_score.html#decisiontreeclassifier",
    "href": "posts/credit_score/credit_score.html#decisiontreeclassifier",
    "title": "Credit Score",
    "section": "\n7.1 DecisionTreeClassifier",
    "text": "7.1 DecisionTreeClassifier\nTo address the model explainability, we can choose a CART model. This selection has 2 interesting point for our task:\n\n\nExplainability, as already mentioned. This will help us explain the reason of rejection.\n\nFeature selection. At this link you can find a paper which shows how embedded method as the tree models can have the same performance of traditional feature selection method.\n\n\n7.1.1 GridSearchCV\nEven if the result of the model are relatively easy to understand, the hyperparameter tuning has far too many possibility. This is where GridSearchCV() comes handy.\n\nCodeparam_grid = {\n  \"dtc__criterion\" : [\"gini\", \"entropy\", \"log_loss\"],\n  \"dtc__splitter\" : [\"best\", \"random\"],\n  \"dtc__max_depth\" : [2, 5, 10],\n  \"dtc__max_features\" : [len(config.features)]\n}\n\nscoring = {\n    \"accuracy\": \"accuracy\",\n    \"f1\": \"f1\",\n    \"roc_auc\": \"roc_auc\"\n}\n\n\n\n7.1.2 Pipeline\nOne of the many nice sklearn features is the Pipeline(). It has lots of important benefits:\n\n\nCode redundancy and reproducibility : it is like a recipe you can tune and run without worrying about to put togheter each step in the correct order.\n\nAvoid data leakege: the preprocessing steps applies only to the training dataset.\n\nHyperparameter tuning: the Pipeline() integrates with GridSearchCV() and RandomizedSearchCV().\n\nModularity: it enables tuning the various steps, removing or adding new one.\n\nFirst, initialize the various preprocessor and the CART model.\n\nCodeohe = OneHotEncoder()\nohe_bi = OneHotEncoder(drop=\"if_binary\")\nss = StandardScaler()\nmms = MinMaxScaler()\ndtc = DecisionTreeClassifier(class_weight=\"balanced\")\n\n\nUsing the ColumnTransformer(), we can apply the different preprocessing to the specific columns. The preprocessing is divided between:\n\nEncoding the binary labels to 0-1 using OneHotEncoding(drop=\"if_binary)\".\nEncoding the remaining labels using OneHotEncoding(). I choose this over other categorical variables encoding because it avoid imputing a hierarchy.\n\nStandardize the numerical variables using StandardScaler(). I also have instantiated the MinMaxScaler(), but I am not using it.\n\nThe classifier is a DecisionTreeClassifier(), which the GridSearchCV() will tune.\nAll of these steps are putted togheter with the Pipeline().\n\nCodepreprocessor = ColumnTransformer(\n  transformers=[\n    (\"binary_encoder\", ohe_bi, config.col_bi),\n    (\"categorical_encoder\", ohe, config.col_ohe),\n    (\"scaler\", ss, config.col_ss)\n    ],\n    remainder='passthrough'\n    )\n\npipe_dtc = Pipeline(\n  steps=[\n    (\"preprocessor\", preprocessor),\n    (\"dtc\", dtc)\n    ]\n    )\n\n\n\n\n\n\n\n\nImportant\n\n\n\nFor the binary labels I had to use the OneHotEncoding(drop=\"if_binary\"), otherwise other preprocessor would not work with the ColumnTransformer().\n\n\n\n7.1.3 Fit\n\nCodegrid_search_dtc = GridSearchCV(\n    estimator=pipe_dtc,\n    param_grid=param_grid,\n    cv=7,\n    scoring=scoring,\n    refit=\"accuracy\",  # This will refit the model using the accuracy metric\n    n_jobs=-1 # use all the available CPU\n    )\n\ngrid_search_dtc.fit(xtrain, ytrain)\n# save the model as pkl file\nwith open(config.path + \"grid_search_dtc.pkl\", \"wb\") as file:\n  pickle.dump(grid_search_dtc, file)\n\n\n\nCodewith open(config.path + \"grid_search_dtc.pkl\", \"rb\") as file:\n    dtc_model = pickle.load(file)\n\n\ncv_results = pd.DataFrame(dtc_model.cv_results_)\nmodel = dtc_model.best_estimator_\n\n\n\nCodepy$cv_results %&gt;% \n  arrange(across(contains(\"rank\"))) %&gt;% \n  select(params, mean_test_accuracy, rank_test_accuracy, mean_test_f1, rank_test_f1, mean_test_roc_auc, rank_test_roc_auc) %&gt;% \n  gt() %&gt;%\n  tab_header(\n    title = \"Cross Validation Results\",\n    subtitle = \"Main metrics\"\n  ) %&gt;% \n  fmt_number(\n   columns = c(\"mean_test_accuracy\", \"mean_test_f1\", \"mean_test_roc_auc\"),\n   decimals = 3,\n   drop_trailing_zeros = TRUE,\n   drop_trailing_dec_mark = FALSE\n  ) %&gt;% \n  cols_align(\n    align = \"center\"\n  ) %&gt;% \n  cols_label(\n    params = \"criterion - max_depth - max_features - splitter\",\n    mean_test_accuracy = \"Mean Test Accuracy\",\n    rank_test_accuracy = \"Rank Test Accuracy\",\n    mean_test_f1 = \"Mean Test F1\",\n    rank_test_f1 = \"Rank Test F1\",\n    mean_test_roc_auc = \"Mean Test ROC AUC\",\n    rank_test_roc_auc = \"Rank Test ROC AUC\",\n  ) %&gt;% \n  tab_options(\n    table.width = pct(100)\n    )\n\n\nTable 12: Cross Validation Results\n\n\n\n\n\n\n\nCross Validation Results\n\n\nMain metrics\n\n\ncriterion - max_depth - max_features - splitter\nMean Test Accuracy\nRank Test Accuracy\nMean Test F1\nRank Test F1\nMean Test ROC AUC\nRank Test ROC AUC\n\n\n\n\nlog_loss, 2, 15, random\n0.727\n1\n0.2\n18\n0.56\n17\n\n\ngini, 10, 15, best\n0.707\n2\n0.403\n1\n0.863\n2\n\n\nlog_loss, 10, 15, best\n0.704\n3\n0.4\n2\n0.863\n1\n\n\nentropy, 10, 15, best\n0.701\n4\n0.394\n3\n0.853\n3\n\n\ngini, 2, 15, best\n0.677\n5\n0.31\n8\n0.705\n11\n\n\nentropy, 5, 15, best\n0.667\n6\n0.368\n4\n0.82\n4\n\n\nlog_loss, 5, 15, best\n0.656\n7\n0.365\n5\n0.819\n5\n\n\ngini, 5, 15, best\n0.651\n8\n0.348\n6\n0.797\n6\n\n\nlog_loss, 10, 15, random\n0.605\n9\n0.296\n9\n0.741\n7\n\n\nentropy, 10, 15, random\n0.602\n10\n0.274\n12\n0.706\n10\n\n\nlog_loss, 2, 15, best\n0.591\n11\n0.31\n7\n0.715\n9\n\n\ngini, 10, 15, random\n0.583\n12\n0.277\n11\n0.722\n8\n\n\nentropy, 5, 15, random\n0.56\n13\n0.231\n15\n0.622\n15\n\n\nentropy, 2, 15, best\n0.543\n14\n0.285\n10\n0.677\n12\n\n\ngini, 5, 15, random\n0.541\n15\n0.234\n14\n0.633\n14\n\n\nlog_loss, 5, 15, random\n0.515\n16\n0.246\n13\n0.65\n13\n\n\ngini, 2, 15, random\n0.513\n17\n0.207\n16\n0.567\n16\n\n\nentropy, 2, 15, random\n0.341\n18\n0.203\n17\n0.549\n18\n\n\n\n\n\n\n\n\n\nThe grid search returns a CART with a best score of best_score with the following parameters:\n\ncriterion:\nmax_depth:\nsplitter:\n\nNow, we can save and load the model. This is useful to avoid to run the grid_search.\n\n7.1.4 Predict\n\nCodeypred = model.predict(xtest)\n\n\n\n7.1.5 Classification Report\n\nCodecr = classification_report(\n  ytest,\n  ypred,\n  # target_names=config.labels,\n  digits=4,\n  output_dict=True\n  )\ndf_cr = pd.DataFrame.from_dict(cr).reset_index()\n\n\n\nCodelibrary(reticulate)\ndf_cr &lt;- py$df_cr %&gt;% dplyr::rename(names = index)\ncols &lt;- df_cr %&gt;% colnames()\ndf_cr %&gt;% \n  pivot_longer(\n    cols = -names,\n    names_to = \"metrics\",\n    values_to = \"values\"\n  ) %&gt;% \n  pivot_wider(\n    names_from = names,\n    values_from = values\n  ) %&gt;% \n  gt() %&gt;%\n  tab_header(\n    title = \"Confusion Matrix\",\n    subtitle = \"Threshold optimization favoring recall\"\n  ) %&gt;% \n  fmt_number(\n    columns = c(\"precision\", \"recall\", \"f1-score\", \"support\"),\n    decimals = 2,\n    drop_trailing_zeros = TRUE,\n    drop_trailing_dec_mark = FALSE\n  ) %&gt;% \n  cols_align(\n    align = \"center\",\n    columns = c(\"precision\", \"recall\", \"f1-score\", \"support\")\n  ) %&gt;% \n  cols_align(\n    align = \"left\",\n    columns = metrics\n  ) %&gt;% \n  cols_label(\n    metrics = \"Metrics\",\n    precision = \"Precision\",\n    recall = \"Recall\",\n    `f1-score` = \"F1-Score\",\n    support = \"Support\"\n  ) %&gt;% \n  tab_options(\n    table.width = pct(100)\n    )\n\n\nTable 13: Classification report\n\n\n\n\n\n\n\nConfusion Matrix\n\n\nThreshold optimization favoring recall\n\n\nMetrics\nPrecision\nRecall\nF1-Score\nSupport\n\n\n\n\n0\n0.94\n0.48\n0.63\n41,986.\n\n\n1\n0.15\n0.76\n0.25\n5,031.\n\n\naccuracy\n0.51\n0.51\n0.51\n0.51\n\n\nmacro avg\n0.55\n0.62\n0.44\n47,017.\n\n\nweighted avg\n0.86\n0.51\n0.59\n47,017.\n\n\n\n\n\n\n\n\n\nCLASSIFICATION ANALYSIS"
  },
  {
    "objectID": "posts/credit_score/credit_score.html#random-forest-classifier",
    "href": "posts/credit_score/credit_score.html#random-forest-classifier",
    "title": "Credit Score",
    "section": "\n7.2 Random Forest Classifier",
    "text": "7.2 Random Forest Classifier"
  },
  {
    "objectID": "posts/credit_score/credit_score.html#xgboost-classifier",
    "href": "posts/credit_score/credit_score.html#xgboost-classifier",
    "title": "Credit Score",
    "section": "\n7.3 XGBoost Classifier",
    "text": "7.3 XGBoost Classifier"
  }
]