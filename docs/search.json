[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Nice to meet you! My name is Simone Brazzi. I am 33 years old guy from Bologna, Italy.\nI decided to open this blog as a personal journal (what a new idea! Who ever thought about this?) of my data journey. This also means it is an opportunity to display my portfolio and shares my projects.\nAt the date of writing, I am a self made data professional. I am currently working as a Data Analyst/Engineer for UniCredit Group.\nI have too many hobbies, but I will try to summarize them in a sparse order:\n\nTraining: my 2024 goal is to be able to run at least an half marathon.\nStudy: I am studying data science to further improve my knowledge in the data field. I would like to do a Master Degree in the field, but it is difficult to combine the private life with an international MSc, at least for now.\nGaming, movies and manga: of course! I mean, it is part of being a nerd, right?! For cliche purpose, I love Quentin Tarantino blabbering, Nolan craziness, even though my favorite genre is horror. My favorites directors are Jordan Pelee and Ari Aster. I am currently reading Berserker, My Hero Academia, Jujutsu Kaisen, One Piece, Chainsaw Man and Kagurabachi.\nCooking, I think it is in my vein being italian (joking!) and considering my mother is a chef.\n\nFor now, I think I have annoyed you enough: stay tuned for future posts!"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Toxic Comment Filter\n\n\n\n\n\n\ncode\n\n\nDeep Learning\n\n\nPython, R\n\n\n\nBiLSTM model to make a multi label classification for a toxic comment filter\n\n\n\n\n\nAug 2, 2024\n\n\nSimone Brazzi\n\n\n\n\n\n\n\n\n\n\n\n\nEurostat Homicide Data\n\n\n\n\n\n\ncode\n\n\nshiny\n\n\nR\n\n\n\nA primer on Shiny to analyze gender differences in homicide rates\n\n\n\n\n\nDec 29, 2023\n\n\nSimone Brazzi\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/eurostat_homicide_rate/eurostat_homicide_rate.html",
    "href": "posts/eurostat_homicide_rate/eurostat_homicide_rate.html",
    "title": "Eurostat Homicide Data",
    "section": "",
    "text": "Hi there and welcome to my first project for the blog! The topic is a sad one, but I would like to explain why I decided to start with this. I was trying Shiny for different dashboards, but I wasn’t satisfied to learn using the classic examples. Unfortunately, italian crime news suffocated the public debate with a case of homicide, in which the victim is a young women. The public debate was focusing so baaaaaadly on the concept of femicide and the data, that I decided to clear the situation with a simple dashboard.\nFirst of all, at this link you can find the dashboard published using shinyapps.io. Also at this link you can find the Github repo. As you can see, even if it is the main branch, there are some details which are not uber perfect, but that don’t interfere with the code.\nNow lets jump into the detail of how to create a Shiny dashboard!"
  },
  {
    "objectID": "posts/eurostat_homicide_rate/eurostat_homicide_rate.html#global.r",
    "href": "posts/eurostat_homicide_rate/eurostat_homicide_rate.html#global.r",
    "title": "Eurostat Homicide Data",
    "section": "global.R",
    "text": "global.R\nAs said, this file is our usual R Script file. First thing first, we import our libraries:\n\nCode# wrangling\nlibrary(\"tidyverse\")\nlibrary(\"readr\")\nlibrary(\"stringr\")\nlibrary(\"dplyr\")\nlibrary(\"magrittr\")\nlibrary(\"forcats\")\nlibrary(\"lubridate\")\nlibrary(\"writexl\")\nlibrary(\"eurostat\")\n# plotting and dashboarding\nlibrary(\"shiny\")\nlibrary(\"shinythemes\")\nlibrary(\"ggplot2\")\nlibrary(\"plotly\")\nlibrary(\"scales\")\nlibrary(\"RColorBrewer\")\nlibrary(\"waiter\")\n# connecting and other\nlibrary(\"rsconnect\")\nlibrary(\"markdown\")\n\n\nLots of packages! The division is merely to remember how everything is managed and because I have OCD for this type of things.\nI want to focus on some packages:\n\ntidyverse, we all know it. As you can see, I also imported lots of single packages which compose the tidyverse, because I was getting errors of missing methods.\neurostat, which lets data flows from the eurostat website to my dashboard. This also lets the dashboard automatically updates when new data is available.\nscales, to nicely scaling my x and y axis.\nRColorBrewer, because I wanted to have a colorblind safe dashboard, even tough I am not.\nwaiter, for nice waiting images while the dashboard is loading.\n\nNow we can focus on the data importing and wrangling. For this, the eurostat library does the job. Lets focus on the crim_hom_vrel dataset.\n\nCode# search in eurostat db\nhomicide &lt;- search_eurostat(\"homicide\")\n\n# import data to variable\ncrim_hom_vrel &lt;- get_eurostat(\"crim_hom_vrel\", time_format = \"date\")\n\n# convert all observations to understandable data\ncrim_hom_vrel &lt;- label_eurostat(crim_hom_vrel)\n\n# label_eurostat_vars(crim_hom_vrel)\n\n# order data by country and date for time series purpose\ncrim_hom_vrel &lt;- crim_hom_vrel %&gt;% \n  arrange(geo, time)\n\ncrim_hom_vrel_grouped &lt;- crim_hom_vrel %&gt;% \n  dplyr::group_by(geo, time, sex, pers_cat, unit) %&gt;% \n  dplyr::summarise(values_grouped = sum(values), .groups = \"drop\") %&gt;% \n  filter(unit == \"Number\") %&gt;% \n  arrange(geo, time, sex)\n\n\nEverything pretty simple. I would like to highlight something about the dplyr::group_by and dplyr::summarise. As you can see, after having grouped and summarized, I need to drop the groups with the method .groups = \"drop\". With dplyr v.1.1.0 we can do the same with the help of the .by method in summarise.\n\nCodecrim_hom_vrel_grouped &lt;- crim_hom_vrel %&gt;% \n  dplyr::summarise(\n    values_grouped = sum(values),\n    .by = c(geo, time, sex, pers_cat, unit)\n    ) %&gt;% \n  filter(unit == \"Number\") %&gt;% \n  arrange(geo, time, sex)\n\n\nCopying from the dplyr website the differences between .by and group_by() are:\n\n\n.by\ngroup_by()\n\n\n\nGrouping only affects a single verb\nGrouping is persistent across multiple verbs\n\n\nSelects variables with tidy-select\n\nComputes expressions with data-masking\n\n\n\nSummaries use existing order of group keys\nSummaries sort group keys in ascending order\n\n\n\nLast part is all about colors.\n\nCode# brewer.pal(11, \"RdYlBu\")\npalette &lt;- c(\"#A50026\", \"#D73027\", \"#F46D43\", \"#FDAE61\", \"#FEE090\", \"#FFFFBF\", \"#E0F3F8\", \"#ABD9E9\", \"#74ADD1\", \"#4575B4\", \"#313695\")\n\npalette_crim_hom_vrel_grouped &lt;- rep(\n  palette,\n  length.out = crim_hom_vrel_grouped$geo %&gt;% str_unique() %&gt;% length()\n  )\n\n\nHere I defined the palette using ColorBrewer. Using rep I replicated the 11 colours for the length of the unique geo values."
  },
  {
    "objectID": "posts/eurostat_homicide_rate/eurostat_homicide_rate.html#ui.r",
    "href": "posts/eurostat_homicide_rate/eurostat_homicide_rate.html#ui.r",
    "title": "Eurostat Homicide Data",
    "section": "ui.R",
    "text": "ui.R"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Simone Brazzi",
    "section": "",
    "text": "I am a (data) nerd, with lot of passion and some mistake along the way. Data analyst by day, aspiring data wizard by night! I love using data to tell stories and drive business decisions. But I’m not content with stopping there. My passion for data and desire to expand my skillset has led me on a quest to become a data scientist. I’m a fearless problem solver with an insatiable curiosity, and I’m always seeking new challenges and opportunities to learn and grow. Let’s make some magic with data! When I don’t stare tolines of code or spreadsheet, I like to read and play video game or spend quality time with my dog."
  },
  {
    "objectID": "posts/toxic_comment_filter/toxic_comment_filter.html",
    "href": "posts/toxic_comment_filter/toxic_comment_filter.html",
    "title": "Toxic Comment Filter",
    "section": "",
    "text": "Costruire un modello in grado di filtrare i commenti degli utenti in base al grado di dannosità del linguaggio.\nPreprocessare il testo eliminando l’insieme di token che non danno contributo significativo a livello semantico.\nTrasformare il corpus testuale in sequenze.\nCostruire un modello di Deep Learning comprendente dei layer ricorrenti per un task di classificazione multilabel.\n\nIn prediction time, il modello deve ritornare un vettore contenente un 1 o uno 0 in corrispondenza di ogni label presente nel dataset (toxic, severe_toxic, obscene, threat, insult, identity_hate). In questo modo, un commento non dannoso sarà classificato da un vettore di soli 0 [0,0,0,0,0,0]. Al contrario, un commento pericoloso presenterà almeno un 1 tra le 6 labels."
  },
  {
    "objectID": "posts/toxic_comment_filter/toxic_comment_filter.html#import-r-libraries",
    "href": "posts/toxic_comment_filter/toxic_comment_filter.html#import-r-libraries",
    "title": "Toxic Comment Filter",
    "section": "Import R libraries",
    "text": "Import R libraries\nImport R libraries. These will be used for both the rendering of the document and data analysis. The reason is I prefer ggplot2 over matplotlib. I will also use colorblind safe palettes.\n\n\nCode\nlibrary(tidyverse, verbose = FALSE)\nlibrary(tidymodels, verbose = FALSE)\nlibrary(reticulate)\nlibrary(ggplot2)\nlibrary(plotly)\nlibrary(RColorBrewer)\nlibrary(bslib)\nlibrary(Metrics)\n\nreticulate::use_virtualenv(\"r-tf\")"
  },
  {
    "objectID": "posts/toxic_comment_filter/toxic_comment_filter.html#class-config",
    "href": "posts/toxic_comment_filter/toxic_comment_filter.html#class-config",
    "title": "Toxic Comment Filter",
    "section": "Class Config",
    "text": "Class Config\nI created a class with all the basic configuration of the model, to improve the readability.\n\n\nCode\nclass Config():\n    def __init__(self):\n        self.url = \"https://s3.eu-west-3.amazonaws.com/profession.ai/datasets/Filter_Toxic_Comments_dataset.csv\"\n        self.max_tokens = 20000\n        self.output_sequence_length = 911 # check the analysis done to establish this value\n        self.embedding_dim = 128\n        self.batch_size = 32\n        self.epochs = 100\n        self.temp_split = 0.3\n        self.test_split = 0.5\n        self.random_state = 42\n        self.total_samples = 159571 # total train samples\n        self.train_samples = 111699\n        self.val_samples = 23936\n        self.features = 'comment_text'\n        self.labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n        self.new_labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate', \"clean\"]\n        self.label_mapping = {label: i for i, label in enumerate(self.labels)}\n        self.new_label_mapping = {label: i for i, label in enumerate(self.labels)}\n        self.path = \"/Users/simonebrazzi/R/blog/posts/toxic_comment_filter/history/f1score/\"\n        self.model =  self.path + \"model_f1.keras\"\n        self.checkpoint = self.path + \"checkpoint.lstm_model_f1.keras\"\n        self.history = self.path + \"lstm_model_f1.xlsx\"\n        \n        self.metrics = [\n            Precision(name='precision'),\n            Recall(name='recall'),\n            AUC(name='auc', multi_label=True, num_labels=len(self.labels)),\n            F1Score(name=\"f1\", average=\"macro\")\n            \n        ]\n    def get_early_stopping(self):\n        early_stopping = keras.callbacks.EarlyStopping(\n            monitor=\"val_f1\", # \"val_recall\",\n            min_delta=0.2,\n            patience=10,\n            verbose=0,\n            mode=\"max\",\n            restore_best_weights=True,\n            start_from_epoch=3\n        )\n        return early_stopping\n\n    def get_model_checkpoint(self, filepath):\n        model_checkpoint = keras.callbacks.ModelCheckpoint(\n            filepath=filepath,\n            monitor=\"val_f1\", # \"val_recall\",\n            verbose=0,\n            save_best_only=True,\n            save_weights_only=False,\n            mode=\"max\",\n            save_freq=\"epoch\"\n        )\n        return model_checkpoint\n\n    def find_optimal_threshold_cv(self, ytrue, yproba, metric, thresholds=np.arange(.05, .35, .05), n_splits=7):\n\n      # instantiate KFold\n      kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n      threshold_scores = []\n\n      for threshold in thresholds:\n\n        cv_scores = []\n        for train_index, val_index in kf.split(ytrue):\n\n          ytrue_val = ytrue[val_index]\n          yproba_val = yproba[val_index]\n\n          ypred_val = (yproba_val &gt;= threshold).astype(int)\n          score = metric(ytrue_val, ypred_val, average=\"macro\")\n          cv_scores.append(score)\n\n        mean_score = np.mean(cv_scores)\n        threshold_scores.append((threshold, mean_score))\n\n        # Find the threshold with the highest mean score\n        best_threshold, best_score = max(threshold_scores, key=lambda x: x[1])\n      return best_threshold, best_score\n\nconfig = Config()"
  },
  {
    "objectID": "posts/toxic_comment_filter/toxic_comment_filter.html#import-python-packages",
    "href": "posts/toxic_comment_filter/toxic_comment_filter.html#import-python-packages",
    "title": "Toxic Comment Filter",
    "section": "Import Python packages",
    "text": "Import Python packages\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport keras\nimport keras_nlp\n\nfrom keras.backend import clear_session\nfrom keras.models import Model, load_model\nfrom keras.layers import TextVectorization, Input, Dense, Embedding, Dropout, GlobalAveragePooling1D, LSTM, Bidirectional, GlobalMaxPool1D, Flatten, Attention\nfrom keras.metrics import Precision, Recall, AUC, SensitivityAtSpecificity, SpecificityAtSensitivity, F1Score\n\n\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.metrics import multilabel_confusion_matrix, classification_report, ConfusionMatrixDisplay, precision_recall_curve, f1_score, recall_score, roc_auc_score\n\n\nCreate a Config class to store all the useful parameters for the model and for the project."
  },
  {
    "objectID": "posts/toxic_comment_filter/toxic_comment_filter.html#eda",
    "href": "posts/toxic_comment_filter/toxic_comment_filter.html#eda",
    "title": "Toxic Comment Filter",
    "section": "EDA",
    "text": "EDA\nFirst a check on the dataset to find possible missing values and imbalances.\n\nFrequency\n\nCode\nlibrary(reticulate)\ndf_r &lt;- py$df\nnew_labels_r &lt;- py$config$new_labels\n\ndf_r_grouped &lt;- df_r %&gt;% \n  select(all_of(new_labels_r)) %&gt;%\n  pivot_longer(\n    cols = all_of(new_labels_r),\n    names_to = \"label\",\n    values_to = \"value\"\n  ) %&gt;% \n  group_by(label) %&gt;%\n  summarise(count = sum(value)) %&gt;% \n  mutate(freq = round(count / sum(count), 4))\n\ndf_r_grouped\n\n\n\n\nTable 2: Absolute and relative labels frequency\n\n\n\n# A tibble: 7 × 3\n  label          count   freq\n  &lt;chr&gt;          &lt;dbl&gt;  &lt;dbl&gt;\n1 clean         143346 0.803 \n2 identity_hate   1405 0.0079\n3 insult          7877 0.0441\n4 obscene         8449 0.0473\n5 severe_toxic    1595 0.0089\n6 threat           478 0.0027\n7 toxic          15294 0.0857\n\n\n\n\n\n\nBarchart\n\n\nCode\nlibrary(reticulate)\nbarchart &lt;- df_r_grouped %&gt;%\n  ggplot(aes(x = reorder(label, count), y = count, fill = label)) +\n  geom_col() +\n  labs(\n    x = \"Labels\",\n    y = \"Count\"\n  ) +\n  # sort bars in descending order\n  scale_x_discrete(limits = df_r_grouped$label[order(df_r_grouped$count, decreasing = TRUE)]) +\n  scale_fill_brewer(type = \"seq\", palette = \"RdYlBu\")\nggplotly(barchart)\n\n\n\n\n\n\n\n\nFigure 1: Imbalance in the dataset with clean variable\n\n\n\n\nIt is visible how much the dataset in imbalanced. This means it could be useful to check for the class weight and use this argument during the training.\nIt is clear that most of our text are clean. We are talking about 0.8033 of the observations which are clean. Only 0.1967 are toxic comments."
  },
  {
    "objectID": "posts/toxic_comment_filter/toxic_comment_filter.html#sequence-lenght-definition",
    "href": "posts/toxic_comment_filter/toxic_comment_filter.html#sequence-lenght-definition",
    "title": "Toxic Comment Filter",
    "section": "Sequence lenght definition",
    "text": "Sequence lenght definition\nTo convert the text in a useful input for a NN, it is necessary to use a TextVectorization layer. See the Section 4 section.\nOne of the method is output_sequence_length: to better define it, it is useful to analyze our text length. To simulate what the model we do, we are going to remove the punctuation and the new lines from the comments.\n\nSummary\n\nCode\nlibrary(reticulate)\ndf_r %&gt;% \n  mutate(\n    comment_text_clean = comment_text %&gt;%\n      tolower() %&gt;% \n      str_remove_all(\"[[:punct:]]\") %&gt;% \n      str_replace_all(\"\\n\", \" \"),\n    text_length = comment_text_clean %&gt;% str_count()\n    ) %&gt;% \n  pull(text_length) %&gt;% \n  summary() %&gt;% \n  as.list() %&gt;% \n  as_tibble()\n\n\n\n\nTable 3: Summary of text length\n\n\n\n# A tibble: 1 × 6\n   Min. `1st Qu.` Median  Mean `3rd Qu.`  Max.\n  &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1     4        91    196  378.       419  5000\n\n\n\n\n\n\nBoxplot\n\n\nCode\nlibrary(reticulate)\nboxplot &lt;- df_r %&gt;% \n  mutate(\n    comment_text_clean = comment_text %&gt;%\n      tolower() %&gt;% \n      str_remove_all(\"[[:punct:]]\") %&gt;% \n      str_replace_all(\"\\n\", \" \"),\n    text_length = comment_text_clean %&gt;% str_count()\n    ) %&gt;% \n  # pull(text_length) %&gt;% \n  ggplot(aes(y = text_length)) +\n  geom_boxplot() +\n  theme_minimal()\nggplotly(boxplot)\n\n\n\n\n\n\n\n\nFigure 2: Text length boxplot\n\n\n\n\n\n\nHistogram\n\n\nCode\nlibrary(reticulate)\ndf_ &lt;- df_r %&gt;% \n  mutate(\n    comment_text_clean = comment_text %&gt;%\n      tolower() %&gt;% \n      str_remove_all(\"[[:punct:]]\") %&gt;% \n      str_replace_all(\"\\n\", \" \"),\n    text_length = comment_text_clean %&gt;% str_count()\n  )\n\nQ1 &lt;- quantile(df_$text_length, 0.25)\nQ3 &lt;- quantile(df_$text_length, 0.75)\nIQR &lt;- Q3 - Q1\nupper_fence &lt;- as.integer(Q3 + 1.5 * IQR)\n\nhistogram &lt;- df_ %&gt;% \n  ggplot(aes(x = text_length)) +\n  geom_histogram(bins = 50) +\n  geom_vline(aes(xintercept = upper_fence), color = \"red\", linetype = \"dashed\", linewidth = 1) +\n  theme_minimal() +\n  xlab(\"Text Length\") +\n  ylab(\"Frequency\") +\n  xlim(0, max(df_$text_length, upper_fence))\nggplotly(histogram)\n\n\n\n\n\n\n\n\nFigure 3: Text length histogram with boxplot upper fence\n\n\n\n\nConsidering all the above analysis, I think a good starting value for the output_sequence_length is 911, the upper fence of the boxplot. In the last plot, it is the dashed red vertical line.. Doing so, we are removing the outliers, which are a small part of our dataset."
  },
  {
    "objectID": "posts/toxic_comment_filter/toxic_comment_filter.html#split-dataset",
    "href": "posts/toxic_comment_filter/toxic_comment_filter.html#split-dataset",
    "title": "Toxic Comment Filter",
    "section": "Split Dataset",
    "text": "Split Dataset\nNow we can split the dataset in 3: train, test and validation sets. Considering there is not a function in sklearn which lets split in these 3 sets, we can do the following: - split between a train and temporary set with a 0.3 split. - split the temporary set in 2 equal sized test and val sets.\n\n\nCode\nx = df[config.features].values\ny = df[config.labels].values\n\nxtrain, xtemp, ytrain, ytemp = train_test_split(\n  x,\n  y,\n  test_size=config.temp_split, # .3\n  random_state=config.random_state\n  )\nxtest, xval, ytest, yval = train_test_split(\n  xtemp,\n  ytemp,\n  test_size=config.test_split, # .5\n  random_state=config.random_state\n  )\n\n\nxtrain shape: py$xtrain.shape ytrain shape: py$ytrain.shape xtest shape: py$xtest.shape ytest shape: py$ytest.shape xval shape: py$xval.shape yval shape: py$yval.shape\nThe datasets are created using the tf.data.Dataset function. It creates a data input pipeline. The tf.data API makes it possible to handle large amounts of data, read from different data formats, and perform complex transformations. The tf.data.Dataset is an abstraction that represents a sequence of elements, in which each element consists of one or more components. Here each dataset is creates using from_tensor_slices. It create a tf.data.Dataset from a tuple (features, labels). .batch let us work in batches to improve performance, while .prefetch overlaps the preprocessing and model execution of a training step. While the model is executing training step s, the input pipeline is reading the data for step s+1. Check the documentation for further informations.\n\n\nCode\ntrain_ds = (\n    tf.data.Dataset\n    .from_tensor_slices((xtrain, ytrain))\n    .shuffle(xtrain.shape[0])\n    .batch(config.batch_size)\n    .prefetch(tf.data.experimental.AUTOTUNE)\n)\n\ntest_ds = (\n    tf.data.Dataset\n    .from_tensor_slices((xtest, ytest))\n    .batch(config.batch_size)\n    .prefetch(tf.data.experimental.AUTOTUNE)\n)\n\nval_ds = (\n    tf.data.Dataset\n    .from_tensor_slices((xval, yval))\n    .batch(config.batch_size)\n    .prefetch(tf.data.experimental.AUTOTUNE)\n)\n\n\n\n\nCode\nprint(\n  f\"train_ds cardinality: {train_ds.cardinality()}\\n\",\n  f\"val_ds cardinality: {val_ds.cardinality()}\\n\",\n  f\"test_ds cardinality: {test_ds.cardinality()}\\n\"\n  )\n\n\ntrain_ds cardinality: 3491\n val_ds cardinality: 748\n test_ds cardinality: 748\n\n\nCheck the first element of the dataset to be sure that the preprocessing is done correctly.\n\n\nCode\ntrain_ds.as_numpy_iterator().next()\n\n\n(array([b\"You see, they are colorcoded. Pink, for example, are animals and plants.  I din't know till now.\",\n       b\"Discussion is finished \\n\\nI'm done discussing the matter at hand. I am also done discussing things with the other user. I've been threatened by you twice about receiving sanctions, and yet another user can beat me down within the guidelines of personal attacks and nothing happens to them.  It's bad enough the other user puts high levels of stress and anxiety on me, but now the mediators are doing the same, by taking sides. This whole situation is very discouraging and disheartening!\",\n       b'\"The notion that people are actively trying to suppress this story from gaining national attention is absurd.  Sad as it may be, the story simply got filed to the later pages of the papers and small blurbs because other issues seemed more... \"\"newsworthy.\"\"\"',\n       b'. I will not engage in battleground attitude, personal attacks, long-term tendentious editing. I am willing to accept content and subject blocks as a condition that my block be lifted.',\n       b\"On reflection, you're right.\",\n       b\"Hold on a second here.  The link to e-ariana.com is not even original content.  This is the heading of the source article reads:\\n\\n International Herald Tribune\\n07/23/2007\\nBy Barry Bearak \\n\\nIf I'm not mistaken, that makes your makes your objection to the source void.  In addition, this Zahir Shah article itself cites a Barry Bearak article published in the New York Times.\",\n       b'\"\\nLet\\'s stop and think about how the Jimbo gif thing has gone since it started and maybe those of you who are voicing that I\\'m \"\"stubborn\"\" could rethink that as an unfair characterization: a admin comes here, tells me that he/she was going to review my block but now won\\'t because the Jimbo animation makes him/her nauseated and that I will never be taken seriously because it\\'s there.  Then, the same admin states after my reply that it\\'s essentially my fault that he/she didn\\'t review the block.  Think about how *you* would react to an attitude like that.  Now, consider the statements made by those who are obviously better at communicating what they really mean: \"\"it\\'s hard for me to read the page with it there\"\".  If that had been said in the first place, how would you have reacted to that instead?  I know that if that had been stated to me initially, I would have understood - because it\\'s a reasonable statement and observation, and I don\\'t want people to have problems reading the page - and had no problem removing the Jimbo gif from my page.  Snarkily saying that the Jimbo gif makes one either annoyed or \"\"nauseated\"\" and will keep others from taking me seriously, is neither helpful nor clear as to the real issue, is it?    \"',\n       b'(a) Because they reflect the last known positions of editors based on their actual participation in a related discussion, and (b) so that the scorecard reflects this historical debate.  You would prefer that I ignore the previous discussion on this topic?  Remember, WP:CCC but it seems appropriate to use as a starting point the previous level of consensus?',\n       b'\"\\n\\nHi Pterantula,\\n\\nThe encyclopedia entry requires attributions and references.  For example, the studies of the \"\"effects of Landmark Education\"\" is from the third-party surveys.  If, instead, the encyclopedia entry were to have customer testimonials instead, they could be struck on grounds they are not verifiable.  (We\\'ve gone around in circles on that many times as well.)\\n\\nAs well, the 20 points of critique are all against Landmark Education (with rebuttals in many cases).\\n\\nAs for litigation, I am aware of five outbound cases:\\n\\nPre-emptive warning\\n\\n1) Traci Hukill \"\"The Est of Friends\"\" (1998, pre-emptive warning, no suit filed)\\n\\nActual litigation, outbound\\n\\n Elle Magazine   (1998, LE achieved no result)\\n Cynthia Kisser  (1997, LE won retraction)\\n Margaret Singer (1997, LE won retraction)\\n Rick Ross       (2005, LE cancelled suit for product disparagement)\\n\\nThere are about three inbound cases (against LE):\\n Stefanie Ney (court ruled in LE\\'s favor)\\n Been versus Weed (2002; 10 claims, 3 settled, 7 dropped;  LE was a cross-defendent)\\n Tracy Neff (1997, sexual harrassment, settled out of court;  LE lacked a \"\"sexual harrassment\"\" policy at the time and got pulled in on this)\\n\\nDoes anyone have anything more than this from LE\\'s inception in 1991 (14 years ago)?  Litigation regarding \"\"est\"\" should be on the \"\"est\"\" page.   For a company with $70 million in revenue and over 800,000 customers of the Landmark Forum, this is a tiny amount of litigation.\\n\\n \"',\n       b'\"\\n\\n Challenge/Response and Backscatter \\n\\nOne anti-spam proposal has been the challenge/response type of system, which sends challenge messages to unknown senders of incoming mail and awaits a response before delivering a message.  Such systems are generally declared as failures and backscatter is the reason.  Concerning a legitimate message, the challenge would typically be sent to the sender.  However, for spam, this is often a forged address, so such a challenge \"\"raises the noise floor\"\" for its innocent recipient.  Although such a recipient will fail to reply, thus causing the originating spam to be discarded, one is effectively placing the decision into the hands of an unknown third party, who is annoyed by the unsolicited challenge message.\\n\\nFurthermore, for a challenge to be useful, it must somehow refer to the original message sent, and even include a (partial) copy of such.  When a C/R system does so, effectively it becomes a spam relay.  A spammer, knowing that a certain C/R system includes a copy, will design his message so that the spam payload is among the copied portion, and forge his desired recipient\\'s address as if he were the sender, thus allowing the C/R system to be the actual delivering server of the spam (as a challenge message payload) to his desired target.\\n\\nThe definition of backscatter as used by some individuals very well includes such challenges from C/R systems, especially those which can be tricked by spammers to deliver their message payloads.  Such challenges are not technically \"\"bounce messages\"\" as they are not NDRs but otherwise satisfy all the other common requirements to be considered backscatter (if not spam in its own right).\\n\\n- 71.106.213.194  \"',\n       b\"Ravana Sri Lanka and Sinahalese , neutrality \\nHe was a Sinhalese. Sri Lankan and  North Indian (Hindi), who wrote orginal Ramayana believes Ravan was Sinhalese ( The Hindu reference). \\nAs per Sri Lankan myths Ravana is very powerful , Raksha king lived in Sri Lanka who had two children named meganand and other but not mentioned any of Sita or Indian invasion. This page is needed to rewritten in neutral manner. Some Sri Lankan myths says Ravana was very powerful capable ruler and others couldn't fight with Ravana so they wrote a book in which they made him as a antagonist and reverted his real world victories by writing a novel. \\nI doesn't mean to remove any of myths written against him. But to mention Ravana's cruelty, raping women stories are taken from the hindu myth story Ramayanaya. Otherwise stating them as real facts will harm the image of a famous Sri Lankan hero. I will put neutrality tag for heavily based on Ramayanaya.\",\n       b'\":::::::::::Relevant policies: Failure or refusal to \"\"get the point\"\" and competence is required. Go argue with US Library of Congress Country Profile source as to why they have not included \"\"Cimerians, Persians, Assyrians, Akkadians, Phrygians (who were not Thracians), etc, etc.\"\" It\\'s also funny you want Persians mentioned in the lead since their invasion was like a blink of an eye, given the scope of history.   \\n\\n\"',\n       b'\"\\nI find it hard to believe that you didn\\'t know you write a verbatim copy of the website, but the content was \\n{{Unreferenced|date=December 2012}}\\n{{Infobox university\\n|name           = Darbhanga Medical College and Hospital \\n|native_name    = \\xe0\\xa4\\xa6\\xe0\\xa4\\xb0\\xe0\\xa4\\xad\\xe0\\xa4\\x82\\xe0\\xa4\\x97\\xe0\\xa4\\xbe \\xe0\\xa4\\xae\\xe0\\xa5\\x87\\xe0\\xa4\\xa1\\xe0\\xa4\\xbf\\xe0\\xa4\\x95\\xe0\\xa4\\xb2 \\xe0\\xa4\\x95\\xe0\\xa5\\x89\\xe0\\xa4\\xb2\\xe0\\xa5\\x87\\xe0\\xa4\\x9c \\xe0\\xa4\\x94\\xe0\\xa4\\xb0 \\xe0\\xa4\\x85\\xe0\\xa4\\xb8\\xe0\\xa5\\x8d\\xe0\\xa4\\xaa\\xe0\\xa4\\xa4\\xe0\\xa4\\xbe\\xe0\\xa4\\xb2\\n|image          = | \\n|established    = 1946\\n|type           = [[Public university|Public]] \\n|city           = [[Darbhanga]] \\n|country        = [[India]] \\n|students       =  \\n|staff          = \\n|campus         = [[urban area|Urban]]\\n}}\\'\\'\\'History\\'\\'\\' \\'\\'\\' http://www.darbhangamedicalcollege.in/aboutus.php{{dead link|date=December 2012}}\\n\\nthen the exact content of the body of http://www.darbhangamedicalcollege.in/aboutus.php and then\\n\\n==Courses Offered==\\nFollowing courses are offered:{{cite | url = http://mohfw.nic.in/ubihar.html{{dead link|date=December 2012}} | title = List of Recognised Medical Qualifications}}\\n* MBBS (90 seats)\\n* MD - Biochem, Forensic Med, Microbiol, Med, Paediatrics, Pathol, Pharmacol, Physiology, Preventive & Social Med, Radio-diagnosis, \\n* MS - Anaesthesiology, Anatomy, ENT Surgery, Obst & Gynae, Ophthal, Orthopaedics\\n* Diplomas - DA, DCH, DCP, DGO, DMRD, DOMS, DTMH, DLO \\n\\n==Address==\\nLaheriasarai, Bihar, India - 846003\\n\\n==References==\\n{{Reflist}}\\n\\n==External links==\\n*[http://institutions.education4india.com/3649/darbhanga-medical-college-laheriasarai-846003-darbhanga-bihar/ Educational Institutes of India ]\\n*[http://gov.bih.nic.in/Profile/Institutions.htm Educational Institutes of Bihar]\\n\\n{{coord missing|Bihar}}\\n\\n[[Category:Universities and colleges in Bihar]]\\n[[Category:Medical colleges in Bihar]]\"',\n       b'\"\\n\\nYour young age\\n\\nI\\'ve figured out what makes you so \"\"interested\"\" in that donkey punch article.  It\\'s your age.  You\\'re a minor.  A child, in other words.  You\\'re under one or both ages of majority in the United States, 18 or 21.  I suspect you are a high school student under 18.  Earlier, you claimed to be \"\"Steven Andrew Miller, a student living in Printer\\'s Row, Chicago.\"\"  Most university students advertise their university affiliations.  You didn\\'t.  Makes me think you\\'re a child.  So, just how young are you?  People should know whom their dealing with, so they don\\'t get in trouble for using profanity to a child or insulting a child or saying negative things to a child or whatever is illegal with children nowadays.  It\\'s problematic to deal with children on the internet, and you should put up a userbox indicating your age to avoid those problems.  \"',\n       b\"Well, it's now at FoxNews and the New York Post, and his publicist has made a statement concerning it, so is it now ready for the Article page, or are we going to play games that is doesn't exist? Naaaanaaanaaa I can't hear you, my fingers are in my ears! Honestly folks, it's a sad day when Wikipedia skews an article even when established respectable news outlets are publishing stories about it. Sad day indeed.\",\n       b'\":I wish you had done as I asked, when I said, \"\"...please just go ahead and block me for a week or two now\"\".  Thank you for at least doing so now; better late than never.  I won\\'t respond to the rest of your post, as clearly the 5 \"\"bad phrase\"\" examples and the block show I am not allowed to defend my actions, or point out improper actions of others, as when I do, I am blocked for it; you\\'ve clearly shown that I can\\'t do that so I won\\'t attempt it.  PS I am sure you don\\'t see this block as being done at my request; no need to point it out. (t\\xe2\\x80\\xa2c) \\n\\n\"',\n       b'So what? I mentioned private family holding and it is already well known as the picture Browne produces in his materials. You want to play this game. Very well. Watch this.',\n       b'\"\\n\\nWell, we now have a copyvio in the history of the temp page. The current version could still profit from some more rephrasing (and from including reformulated stuff from the second page from Medicinenet - click on the small \"\"2\"\" at the bottom, it\\'ll show you some info on treatments). It\\'s not ideal, but since we usually tolerate copyvios in the history, I think we might be fine. Alternatively, you could just copy your temp version offline, delete the temp page, and recreate it anew with the saved content. After all, you\\'re an admin, and I don\\'t think anybody would object to your temporarily deleting a temp page to get rid of copyvio in ther history.  22:42, 20 Dec 2004 (UTC)\"',\n       b'\"\\n\\n Troubling aspects of the artiicle as it exists right now \\n\\n1) I believe if Eric Altermann is to be accorded the prestige of being associated with msnbc.com, it should be noted that he is a blogger for msnbc.com. I say that because I found conservative columnist and author Michelle Malkin referred to as a blogger in another article and I do believe that\\'s meant to diminish the value of her statement. So, again, yes. We HAVE to see the forest for the trees. No one article in Wikipedia exists in a vaccum. There has to be a baseline standard and I reject any argumentation that suggests implies or directs that if \\'I don\\'t like that article, go change it.\\' Instead, one way or the other (and I don\\'t care which way) liberals and conservatives have to be characterized by the same fair measuring stick.\\n\\n2) This is new. It\\'s from Conason: \"\"Conason goes on to point out that Coulter\\'s critical nature is blunted by her pre-assumed opinions, making many of the conclusions she draws irrelevant to the actual nature of her arguments.\"\" Wow! That seems a little unfair just hanging out there like that. I propose either eliminating it or...and I hate this approach as much as the next guy...scotch-taping a quote after that which refutes this obvious cheap-shot.\\n\\n3) This is old. The McVeigh section, as should be no surprise to anyone who\\'s followed my work here, is troubling. Let\\'s start with this quote: \"\" She was willing to characterize members of the Branch Davidians at the Waco compound as \"\"harmless American citizens\"\" [6] even after the survivors of the raid and subsequent immolation of the group by their leader were convicted.\"\"\\nWell, some Waco compounders were convicted of something or another. But the fact is that there WERE mostly innocent victims of David Koresh in that compound. It\\'s unbalanced to just add the tag line about conviced Davidians without acknowledging what I GUARANTEE you Ann was referring to, which were the overwhelming number that were innocent. \\n\\nThis quote is also troubling: \"\"Timothy McVeigh made similar criticism as partial justification for his bombing.\"\" Well, McVeigh had LOTS of reasons for his bombing. But who cares what a madman thinks? And if you do, then I understand he also had some uncomplimentary things to say about the Bible. Therefore, if we are to include this in Coulter\\'s section, I say we include it\\'s compelement in Bill Maher\\'s article. It would go something like this:\\n\"\"Naher has frequently criticized the Bible calling it a \\'bunch of fairy tales\\' and claiming \\'God doesn\\'t write books.\\'  Oklahoma City bomber Timothy McVeigh made similar criticism as partial justification for his distrust of Christianity, though the Bible has been widely criticized throughout history.\"\"\\n\\nWhady\\'a say?\\n\\n(Much more on this section later.)\\n\\n4) Racism is not the same as being anti-Islamic terrorists that want to kill Americans for supporting Israel etc. To title Ann\\'s comments on Islamic Radicals as allegations of racism seems dishonest to me.\\n\\n5) Did Ann really say that ALL  women are \"\"not as bright\"\" as men\\'?\\nSome editor in Wikipedia wants us to think so. I say this quote be taken out until we get confirmation of her whole statement in context. Now, I know for a fact that Ann is only half-joking when she argues women should not be allowed to vote so she is far from imune when it comes to suggesting she holds less-than-boilerplate-feminist views. But this quote supposedly out of H & C seems like it\\'s probably another smear. Again, if proven wrong, I\\'ll gladly man up and stand corrected. But, unitl I\\'m proven it wrong, it should be excised in my view. \\n\\nMore later... \"',\n       b'\"\\n\\nGeorge, you are taking the role of the Arbitration Committee, all by your self. The community already decided that my ArbCom case page should stand, you step beyond your powers by \"\"ruling\"\" it disruptive.\\n\\nMy ArbCom elections participation referred to an ANI in which I was admonished, yet my asking \"\"any general concerns by the five days of silence from admins in this ANI\"\" in a question to candidates is taken as, \"\"purpose of causing trouble for people Harry has disagreed with\"\", \"\"veiled personal attack\"\" and \"\"some kind of agenda behind them\"\". It\\'s almost as if the admin community had some vestigial shame on how that ANI was handled, why else suppress it by removing the questions from the candidates pages?\\n\\nI have been advised not to make edits to articles if I wish my ArbCom case page to be regarded as timeous, then you say I\\'m, \"\"not even trying to build articles anymore\"\". However I regarded ArbCom elections, and editing WP:AGF - , ,  as exceptions. As was my WP:AGF talk page participation.\\n\\nIf I was going to re-write the satire,  I would satirise the admin behaviour it was aimed at by comparison to the mindset of the religious police of the Iranian state.\\n\\nThe religious police are to Islam and the Koran as some admins are to Wikipedia and it\\'s Policies and Guidelines.\\n\\nYou can draw further parallels with conservative and liberal wings among admins, but no one likes an extremest.\\n\\nIf you don\\'t like that comparison then compare them with the mindset of US soldiers at Abu Ghraib.\\n\\nOver-the-top comparisons? Remember I\\'m talking about mindset here; but given the chance, how many admins would you expect to behave as abhorrently as the religious police? Now take that answer and multiply by three. If the product is greater than zero, then you must have thought of at least one admin who you would expect to abuse others human rights if they had real powers in the real world. Don\\'t you think that these admins should have the tools gently removed from their hands?\\n\\nPart of the humour of the  was the alternative spelling of Constable to Cunstable - not very subtle humour & a very minor part near the end, but obviously intended as humour, and read with good faith can only be regarded as such - however this was used as a pretext to speedy delete after 5 hours in WP:MFD.  was put up with the Brainstorming template from the first, indicating an appeal for participation in it\\'s production and alteration, as someone did when it was in WP:Namespace. If the cited reason of \"\"calling admins cunts by typographic means\"\" was the only objection, then it would have been quite easy to participate and call for those changes. Instead of participation, admins afraid of satire, suppressed the satire rather than examine the underlying concern the piece was obviously addressing - WP:religious police, another satire anyone?\\n\\nI would remind you of the story of Dickens being sued by some gentlemen over Oliver Twist, citing that the book used their person as the basis for the poorhouse character who refused Oliver\\'s request for more (despite it being politely put; I know how he feels), the judge summed up that anyone who saw themselves in that character wasn\\'t up to much (I paraphrase). If you felt you were being targeted by my satire, then I would ask which character from Oliver you think the general readership would regard you as.\\n\\nI trust that my useful, and still standing, contributions to WP:AGF, and for the other rebuttals of your given reasons for the block, you will change your view to one which I hold myself - I am here to build an encyclopaedia; and lift the block.\\n\\nFinally I would say, all that is required for evil to flourish is for good Wikipedians to do nothing.   18:0',\n       b\"I haven't been following the conversation too much, but I'm assuming he's referring to me and my comments I made on this very talk page to him asking him to cut out the sockpuppet crap? It's difficult to tell...because your comments are the only ones on here, it makes it look very one-sided. At any rate, I'm not who he claims me to be, and I certainly do believe he has exhibited negative behavior both in past and present incidents such as this one.\",\n       b'Hmmm... just my two cents, but I find it quite bizarre that no fewer than three administrators (',\n       b\"HELLO Liberal MAGGOT Part TWO\\nCensorship will not make me go away... thanks to you, Rob, I am now using my influence, to undermine the crediablity of Wikipedidiots, like you, and Wikipedia.  To bad you don't want to acknowledge the fact that you and your kind are ignorant, knuckle dragging,CyberThugs. Your shadow Joe.\",\n       b'\"\\n\\n A barnstar for you! \\n\\n  The Original Barnstar Thanks for all the standardization work you\\'ve been doing recently, on IPA templates. (And from a glance at various archives, thanks also for many years of work improving the templates themselves!) \\xe2\\x80\\x93  \"',\n       b'Simon Boliva over claimed \\n\\nMany historians have noted that Simon boliva Deliberately claimed additional land for this mountain country even though he knew bolivia will always be a land of mountain people, why is this not mention in the article, its historical facts!!!!',\n       b', 22 July 2008 (UTC)\\n\\nYeah, nice to take a back seat for a while.   22:21',\n       b'\"\\n\\nRose\\'s surname should be capitalized in the page title (\"\"Phyllis rose\"\"). I can\\'t figure out how to edit that   \"',\n       b\"Could you help me, perhaps?  I'm the Finkbeiner of the Finkbeiner test.  I'd this article to link to my original post.  Right now, it links to a re-post of my post on another site.  This subject has gotten a lot of attention and I'm happy for Christie Aschwanden and Double XX Science to get most of it.  But would you mind adding this link? http://www.lastwordonnothing.com/2013/01/17/5266/  The title of the original post is What I'm Not Going To Do.\\n\\nI tried to do this myself but my html skills are rudimentary.  As is, not even there.\\n\\nThank you for considering it.\\n\\nAnn Finkbeiner\",\n       b'\"\\n\\n Article Absolutely Neutral \\nThe article is Absolutely Neutral and it is not a \"\"point of view\"\" but absolute facts and references have been cited.User:Vaishnava\"',\n       b\"Doug, \\nto doug\\nYou'll find on a second reading that the edit is not an essay but disproves the Original Research as portrayed in the article attempting to masquerade as fact. The source texts are peer-reviewed external third-party books, if you'd care to re-check. Thus the Disputation stands the test of Wikipedia edit requirements as it not original research, but, the required rebuttal of Original Research in order for an article which does not meet any criteria of POV Neutrality and research that diverges from academic mainstream discourse. Additionally, the links I use add to Wikipedia in the framework of cross-linking to related articles.\\nFrankly, if more articles were written as thoroughly as the Disputation alone- Wikipedia would not be a steaming pile of politicised horse manure and would not be ridiculed a d banned from even primary and secondary school referencing (as per UK, Australia and Indonesia school curriculau)\",\n       b'\"\\n\\noh please, I think my arguments need a better response than just you calling for ceasing accusations.Its just that you dont even have an answer and you know exactly what you meant when you said that I am trying to make someone \"\"LOOK\"\" european. what do you mean by concentrating on the topic? I said enough about the topic. we BOTH agreed that borders are not established yet, that is why putting them in several regions but europe is uaaceptable.  It either needs to be in Asian section as it is now already, or in the western asian section where it is now already as well, or the european section where it is listed as western asian or it does not have to be there at all.it  is everywjere but in europe even though borders are not official yet? WHY do you think it is very easy for you to put them in asia, or western asia and not europe even though the borders are not established ? why do you think those people and me( I can imagine how big you think of you as european)  are trying to \"\"LOOK\"\" and be european? what gives you the right to give that supercilious evaluation ? you are BIASED , thats the only explanation.YES, I am accusing you and I am accusing you of being a racist and being a paranoid that thinks that everyone is trying to be European and HUSOND is a brave person who does everything to protect them from \"\"Unnessesary\"\" people and people who he thinks are LESS than him personally because he is european. Europe is like a priviledged section where countries can be put thats why its hard for them to be there and not hard to be in middle east even though the article itself says that first european homo georgicus was found in georgia.next time if you will bother to write a response, I urge you to adress all these REAL issues instead of calling for ceasing accusations.    \"',\n       b'\"\\nDear , you say that in RWP there are \"\"Many small unsignificant parties are claiming as \"\"the official representative\"\", \"\"president\"\", etc.\"\" It\\'s not right. After the National Congress III (few days ago), some leaders of these parties were jailed by Indonesian government. These organizations were mostly closed after getting their politicians jailed. The only parties left were WPLO (I am representative of it in Russia), KNPB and WPNLA. And all these parties made a coalition at 25th October. So, now West Papuan separatism is much more united and serious than it was before.   \"'],\n      dtype=object), array([[0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [1, 0, 0, 0, 1, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0]]))\n\n\nAnd we check also the shape. We expect a feature of shape (batch, ) and a target of shape (batch, number of labels).\n\n\nCode\nprint(\n  f\"text train shape: {train_ds.as_numpy_iterator().next()[0].shape}\\n\",\n  f\" text train type: {train_ds.as_numpy_iterator().next()[0].dtype}\\n\",\n  f\"label train shape: {train_ds.as_numpy_iterator().next()[1].shape}\\n\",\n  f\"label train type: {train_ds.as_numpy_iterator().next()[1].dtype}\\n\"\n  )\n\n\ntext train shape: (32,)\n  text train type: object\n label train shape: (32, 6)\n label train type: int64"
  },
  {
    "objectID": "posts/toxic_comment_filter/toxic_comment_filter.html#definition",
    "href": "posts/toxic_comment_filter/toxic_comment_filter.html#definition",
    "title": "Toxic Comment Filter",
    "section": "Definition",
    "text": "Definition\nDefine the model using the Functional API.\n\n\nCode\ndef get_deeper_lstm_model():\n    clear_session()\n    inputs = Input(shape=(None,), dtype=tf.int64, name=\"inputs\")\n    embedding = Embedding(\n        input_dim=config.max_tokens,\n        output_dim=config.embedding_dim,\n        mask_zero=True,\n        name=\"embedding\"\n    )(inputs)\n    x = Bidirectional(LSTM(256, return_sequences=True, name=\"bilstm_1\"))(embedding)\n    x = Bidirectional(LSTM(128, return_sequences=True, name=\"bilstm_2\"))(x)\n    # Global average pooling\n    x = GlobalAveragePooling1D()(x)\n    # Add regularization\n    x = Dropout(0.3)(x)\n    x = Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n    x = LayerNormalization()(x)\n    outputs = Dense(len(config.labels), activation='sigmoid', name=\"outputs\")(x)\n    model = Model(inputs, outputs)\n    model.compile(optimizer='adam', loss=\"binary_crossentropy\", metrics=config.metrics, steps_per_execution=32)\n    \n    return model\n\nlstm_model = get_deeper_lstm_model()\nlstm_model.summary()"
  },
  {
    "objectID": "posts/toxic_comment_filter/toxic_comment_filter.html#callbacks",
    "href": "posts/toxic_comment_filter/toxic_comment_filter.html#callbacks",
    "title": "Toxic Comment Filter",
    "section": "Callbacks",
    "text": "Callbacks\nFinally, the model has been trained using 2 callbacks: - Early Stopping, to avoid to consume the kaggle GPU time. - Model Checkpoint, to retrieve model training information.\n\n\nCode\n# callbacks\nmy_es = config.get_early_stopping()\nmy_mc = config.get_model_checkpoint(filepath=\"/checkpoint.keras\")\ncallbacks = [my_es, my_mc]"
  },
  {
    "objectID": "posts/toxic_comment_filter/toxic_comment_filter.html#final-preparation-before-fit",
    "href": "posts/toxic_comment_filter/toxic_comment_filter.html#final-preparation-before-fit",
    "title": "Toxic Comment Filter",
    "section": "Final preparation before fit",
    "text": "Final preparation before fit\nConsidering the dataset is imbalanced, to increase the performance we need to calculate the class weight. This will be passed during the training of the model.\n\n\nCode\nlab = pd.DataFrame(columns=config.labels, data=ytrain)\nr = lab.sum() / len(ytrain)\nclass_weight = dict(zip(range(len(config.labels)), r))\ndf_class_weight = pd.DataFrame.from_dict(\n  data=class_weight,\n  orient='index',\n  columns=['class_weight']\n  )\ndf_class_weight.index = config.labels\n\n\n\nCode\nlibrary(reticulate)\npy$df_class_weight\n\n\n\n\nTable 4: Class weight\n\n\n\n              class_weight\ntoxic          0.095900590\nsevere_toxic   0.009928468\nobscene        0.052757858\nthreat         0.003061800\ninsult         0.049132042\nidentity_hate  0.008710911\n\n\n\n\nIt is also useful to define the steps per epoch for train and validation dataset. This step is required to avoid\n\n\nCode\nsteps_per_epoch = config.train_samples // config.batch_size\nvalidation_steps = config.val_samples // config.batch_size"
  },
  {
    "objectID": "posts/toxic_comment_filter/toxic_comment_filter.html#fit",
    "href": "posts/toxic_comment_filter/toxic_comment_filter.html#fit",
    "title": "Toxic Comment Filter",
    "section": "Fit",
    "text": "Fit\nThe fit has been done on Kaggle to levarage the GPU. Some considerations about the model:\n\n.repeat() ensure the model sees all the daataset.\nepocs is set to 100.\nvalidation_data has the same repeat.\ncallbacks are the one defined before.\nclass_weight ensure the model is trained using the frequency of each class, because our dataset is imbalanced.\nsteps_per_epoch and validation_steps depend on the use of repeat.\n\n\n\nCode\nhistory = model.fit(\n  processed_train_ds.repeat(),\n  epochs=config.epochs,\n  validation_data=processed_val_ds.repeat(),\n  callbacks=callbacks,\n  class_weight=class_weight,\n  steps_per_epoch=steps_per_epoch,\n  validation_steps=validation_steps\n  )\n\n\nNow we can import the model and the history trained on Kaggle.\n\n\nCode\nmodel = load_model(filepath=config.model)\nhistory = pd.read_excel(config.history)"
  },
  {
    "objectID": "posts/toxic_comment_filter/toxic_comment_filter.html#evaluate",
    "href": "posts/toxic_comment_filter/toxic_comment_filter.html#evaluate",
    "title": "Toxic Comment Filter",
    "section": "Evaluate",
    "text": "Evaluate\n\n\nCode\nvalidation = model.evaluate(\n  processed_val_ds.repeat(),\n  steps=validation_steps, # 748\n  verbose=0\n  )\n\n\n\nCode\ntibble(\n  metric = c(\"loss\", \"precision\", \"recall\", \"auc\", \"f1_score\"),\n  value = py$validation\n  )\n\n\n\n\nTable 5: Model validation metric\n\n\n\n# A tibble: 5 × 2\n  metric     value\n  &lt;chr&gt;      &lt;dbl&gt;\n1 loss      0.0542\n2 precision 0.789 \n3 recall    0.671 \n4 auc       0.957 \n5 f1_score  0.0293"
  },
  {
    "objectID": "posts/toxic_comment_filter/toxic_comment_filter.html#predict",
    "href": "posts/toxic_comment_filter/toxic_comment_filter.html#predict",
    "title": "Toxic Comment Filter",
    "section": "Predict",
    "text": "Predict\nFor the prediction, the model does not need to repeat the dataset, because the model has already been trained and now it has just to consume the data to make the prediction.\n\n\nCode\npredictions = model.predict(processed_test_ds, verbose=0)"
  },
  {
    "objectID": "posts/toxic_comment_filter/toxic_comment_filter.html#confusion-matrix",
    "href": "posts/toxic_comment_filter/toxic_comment_filter.html#confusion-matrix",
    "title": "Toxic Comment Filter",
    "section": "Confusion Matrix",
    "text": "Confusion Matrix\nThe best way to assess the performance of a multi label classification is using a confusion matrix. Sklearn has a specific function to create a multi label classification matrix to handle the fact that there could be multiple labels for one prediction.\n\nGrid Search Cross Validation for best threshold\nGrid Search CV is a technique for fine-tuning hyperparameter of a ML model. It systematically search through a set of hyperparamenter values to find the combination which led to the best model performance. In this case, I am using a KFold Cross Validation is a resempling technique to split the data into k consecutive folds. Each fold is used once as a validation while the k - 1 remaining folds are the training set. See the documentation for more information.\nThe model is trained to optimize the recall. The decision was made because the cost of missing a True Positive is greater than a False Positive. In this case, missing a injurious observation is worst than classifying a clean one as bad.\nHaving said this, I still want to test different metrics other than the recall_score to have more possibility of decision of the best threshold.\n\nf1_score\n\n\nCode\nytrue = ytest.astype(int)\ny_pred_proba = predictions\noptimal_threshold_f1, best_score_f1 = config.find_optimal_threshold_cv(ytrue, y_pred_proba, f1_score)\n\nprint(f\"Optimal threshold: {optimal_threshold_f1}\")\n\n\nOptimal threshold: 0.15000000000000002\n\n\nCode\nprint(f\"Best score: {best_score_f1}\")\n\n\nBest score: 0.4788653077945807\n\n\nCode\n\n# Use the optimal threshold to make predictions\nfinal_predictions_f1 = (y_pred_proba &gt;= optimal_threshold_f1).astype(int)\n\n\nOptimal threshold f1 score: 0.15. Best score: 0.4788653.\n\n\nrecall_score\n\n\nCode\nytrue = ytest.astype(int)\ny_pred_proba = predictions\noptimal_threshold_recall, best_score_recall = config.find_optimal_threshold_cv(ytrue, y_pred_proba, recall_score)\n\n# Use the optimal threshold to make predictions\nfinal_predictions_recall = (y_pred_proba &gt;= optimal_threshold_recall).astype(int)\n\n\nOptimal threshold recall: 0.05. Best score: 0.8095814.\n\n\nroc_auc_score\n\n\nCode\nytrue = ytest.astype(int)\ny_pred_proba = predictions\noptimal_threshold_roc, best_score_roc = config.find_optimal_threshold_cv(ytrue, y_pred_proba, roc_auc_score)\n\nprint(f\"Optimal threshold: {optimal_threshold_roc}\")\n\n\nOptimal threshold: 0.05\n\n\nCode\nprint(f\"Best score: {best_score_roc}\")\n\n\nBest score: 0.8809499649742268\n\n\nCode\n\n# Use the optimal threshold to make predictions\nfinal_predictions_roc = (y_pred_proba &gt;= optimal_threshold_roc).astype(int)\n\n\nOptimal threshold roc: 0.05. Best score: 0.88095.\n\n\n\nConfusion Matrix Plot\nThe confusion matrix is plotted using the multilabel_confusion_matrix function in scikit-learn. We have to plot a confusion matrix for each label. To plot the confusion matrix, we need to convert the predicted probability of a label to a proper prediction. To do so, we use the calculated optimal threshold for the recall, which is 0.05. The confusion matrix plotted hete, considering we have a multi label task, is not a big one with all the labels as columns and indices. We plot a confusion matrix for each label with a simple for loop, which extract for each loop the confusion matrix and the associated label.\n\n\nCode\n# convert probability predictions to predictions\nypred = predictions &gt;=  optimal_threshold_recall # .05\nypred = ypred.astype(int)\n\n# create a plot with 3 by 2 subplots\nfig, axes = plt.subplots(3, 2, figsize=(15, 15))\naxes = axes.flatten()\nmcm = multilabel_confusion_matrix(ytrue, ypred)\n# plot the confusion matrices for each label\nfor i, (cm, label) in enumerate(zip(mcm, config.labels)):\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n    disp.plot(ax=axes[i], colorbar=False)\n    axes[i].set_title(f\"Confusion matrix for label: {label}\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 4: Multi Label Confusion matrix"
  },
  {
    "objectID": "posts/toxic_comment_filter/toxic_comment_filter.html#dataset",
    "href": "posts/toxic_comment_filter/toxic_comment_filter.html#dataset",
    "title": "Toxic Comment Filter",
    "section": "Dataset",
    "text": "Dataset\nNow we can split the dataset in 3: train, test and validation sets. Considering there is not a function in sklearn which lets split in these 3 sets, we can do the following: - split between a train and temporary set with a 0.3 split. - split the temporary set in 2 equal sized test and val sets.\n\n\nCode\nx = df[config.features].values\ny = df[config.labels].values\n\nxtrain, xtemp, ytrain, ytemp = train_test_split(\n  x,\n  y,\n  test_size=config.temp_split, # .3\n  random_state=config.random_state\n  )\nxtest, xval, ytest, yval = train_test_split(\n  xtemp,\n  ytemp,\n  test_size=config.test_split, # .5\n  random_state=config.random_state\n  )\n\n\nxtrain shape: py$xtrain.shape ytrain shape: py$ytrain.shape xtest shape: py$xtest.shape ytest shape: py$ytest.shape xval shape: py$xval.shape yval shape: py$yval.shape\nThe datasets are created using the tf.data.Dataset function. It creates a data input pipeline. The tf.data API makes it possible to handle large amounts of data, read from different data formats, and perform complex transformations. The tf.data.Dataset is an abstraction that represents a sequence of elements, in which each element consists of one or more components. Here each dataset is creates using from_tensor_slices. It create a tf.data.Dataset from a tuple (features, labels). .batch let us work in batches to improve performance, while .prefetch overlaps the preprocessing and model execution of a training step. While the model is executing training step s, the input pipeline is reading the data for step s+1. Check the documentation for further informations.\n\n\nCode\ntrain_ds = (\n    tf.data.Dataset\n    .from_tensor_slices((xtrain, ytrain))\n    .shuffle(xtrain.shape[0])\n    .batch(config.batch_size)\n    .prefetch(tf.data.experimental.AUTOTUNE)\n)\n\ntest_ds = (\n    tf.data.Dataset\n    .from_tensor_slices((xtest, ytest))\n    .batch(config.batch_size)\n    .prefetch(tf.data.experimental.AUTOTUNE)\n)\n\nval_ds = (\n    tf.data.Dataset\n    .from_tensor_slices((xval, yval))\n    .batch(config.batch_size)\n    .prefetch(tf.data.experimental.AUTOTUNE)\n)\n\n\n\n\nCode\nprint(\n  f\"train_ds cardinality: {train_ds.cardinality()}\\n\",\n  f\"val_ds cardinality: {val_ds.cardinality()}\\n\",\n  f\"test_ds cardinality: {test_ds.cardinality()}\\n\"\n  )\n\n\ntrain_ds cardinality: 3491\n val_ds cardinality: 748\n test_ds cardinality: 748\n\n\nCheck the first element of the dataset to be sure that the preprocessing is done correctly.\n\n\nCode\ntrain_ds.as_numpy_iterator().next()\n\n\n(array([b\"I've read through the months-long discussion regarding the introduction, and seen some very helpful proposals from each of the main participants. I'd like to plead for at least a conservative interim revision of the unreadable and uninterpretable paragraph that is there now. By conservative, I mean to  retaining the existing terminology, but to open with a positive declarative sentence without asides, exceptions and negatives. Hammer out the exact terminology later, and other contentious issues in continued discussion. That change will at least allow the visitor to orient on first arrival. Thanks!\",\n       b'In addition, you may want to read this as it is explained by author Mike Campbell:\\n\\nThe TIGHAR-Nikumaroro Fiasco\\nTwo years of speculation and hype came to a numbing climax on\\nMarch 16, 1992, at the National Press Club in Washington,\\nD.C. At a crowded news conference receiving national TV\\ncoverage, Richard Gillespie, executive director of The International\\nGroup for Historic Aircraft Recovery [TIGHAR], unabashedly\\nannounced that the Amelia Earhart mystery \\xe2\\x80\\x9cis solved.\\xe2\\x80\\x9d\\nSince early 1990, following the group\\xe2\\x80\\x99s first excursion to Nikumaroro,\\na 3.5-mile long coral atoll in the Phoenix Islands, about 2,100 miles\\nsouthwest of Hawaii and three hundred miles south of Howland Island,\\nthe national media had touted TIGHAR\\xe2\\x80\\x99s theory that Nikumaroro could\\nbe the final resting place of Amelia Earhart and Fred Noonan.\\nThe \\xe2\\x80\\x9cevidence\\xe2\\x80\\x9d Gillespie presented included a battered piece of aluminum,\\na weathered size 9 shoe sole labeled \\xe2\\x80\\x9cCat\\xe2\\x80\\x99s Paw Rubber Co.,\\nUSA,\\xe2\\x80\\x9d a small brass eyelet, and another unlabeled heel the group found\\non Nikumaroro during TIGHAR\\xe2\\x80\\x99s highly publicized return there in\\nOctober 1991. These items, elaborately displayed and labeled in a glass\\ncase, all came from Earhart or her Electra, according to Gillespie.\\n\\xe2\\x80\\x9cThere may be conflicting opinions, but there is no conflicting evidence,\\xe2\\x80\\x9d\\nGillespie said. \\xe2\\x80\\x9cI submit that the case is solved.\\xe2\\x80\\x9d Gillespie then\\nbit his lip and looked down at the floor \\xe2\\x80\\x94 a curious yet revealing display\\nof body language for someone claiming to have solved one of the\\n20th century\\xe2\\x80\\x99s greatest mysteries.\\nGillespie and Patricia R. Thrasher, TIGHAR\\xe2\\x80\\x99s husband-and-wife\\nteam, theorize that through a navigational error, Earhart and Noonan\\noverflew Howland Island and landed on a reef on Nikumaroro during\\nlow tide. There they died of dehydration a short time later. The plane,\\nthey said, was washed over the reef when the tide came in and now lies\\nunder 2,000 feet of ocean.\\nThe April 1992 Life magazine featured a six-page spread penned by\\nGillespie, who cites Navy pilot John Lambrecht\\xe2\\x80\\x99s July 9, 1937 report:\\n\\xe2\\x80\\x9cSigns of recent human habitation were clearly visible [on\\nNikumaroro], but \\xe2\\x80\\x9crepeated circling and zooming failed to elicit any\\nanswering wave from possible inhabitants.\\xe2\\x80\\x9d Gillespie called this report\\n\\xe2\\x80\\x9chugely significant\\xe2\\x80\\x9d and \\xe2\\x80\\x9ctragically inadequate. What had not been\\ndone in 1937 had to be done now. We would have to search\\nNikumaroro.\\xe2\\x80\\x9d\\nGillespie credited two retired military aviators, Tom Gannon and\\nTom Willi, of Fort Walton Beach, Florida, for the theory\\xe2\\x80\\x99s origin.\\n\\xe2\\x80\\x9cUsing celestial tables, Gannon pointed out that on the morning of July\\n2, 1937, the rising sun would have provided the precise line of position\\nEarhart said she was running,\\xe2\\x80\\x9d Gillespie wrote in Life. \\xe2\\x80\\x9cBy flying\\nsoutheast along that line, Noonan could be sure that, even if he missed\\nHowland, he would reach an island in the Phoenix group in about two\\nhours. Clearly it was the safest, sanest course to follow. I traced the line\\non the chart and read the name of the island: Nikumaroro.\\xe2\\x80\\x9d\\nThis supposition was convincing to some, but Devine wasn\\xe2\\x80\\x99t\\nimpressed. \\xe2\\x80\\x9cThere is considerable information in Gillespie\\xe2\\x80\\x99s rendition that\\ncan be faulted,\\xe2\\x80\\x9d Devine writes. \\xe2\\x80\\x9cOne example is his attempt to foster a line\\nbetween Howland Island and Nikumaroro. He presents Earhart\\xe2\\x80\\x99s last message\\nas \\xe2\\x80\\x98We are on line 157/337 ... We are running on line.\\xe2\\x80\\x99\\nEarhart never reached Howland Island. Earhart never\\nsaw Howland Island, therefore Howland Island is not a reference\\npoint for a landing on Nikumaroro. Earhart\\xe2\\x80\\x99s authentic\\nlast message received by Coast Guard personnel on board\\nthe Itasca, and recorded by Commander Thompson, related\\nno reference point: \\xe2\\x80\\x98We are on line of position 157-337 ...\\nWe are running north and south.\\xe2\\x80\\x99 Earhart was not flying east\\ntoward Howland Island or Hawaii. She was on a course, a\\ncompass course or line of position 157-337, which terminates\\non the island of Saipan.1\\nWith Our Own Eyes\\n\\xe2\\x80\\x93 163 \\xe2\\x80\\x93\\n\\xe2\\x80\\x93 164 \\xe2\\x80\\x93\\nEarhart was not adept at navigation, and may have stated\\nher reading from a pocket compass she carried on her person.\\nShe was probably and almost certainly stressed by ensuing\\nevents, especially if Noonan had been rendered unconscious\\nduring the almost disastrous takeoff of the overladen plane.\\nIf Noonan had not been incapacitated, I assure you that he,\\none of the best air-and-sea navigators of that period, would\\nhave been communicating. He would have guided their\\nplane on a pre-arranged course, and calculated for accuracy\\nduring intervals of the flight.2\\nDevine writes that another likely origin of Gill',\n       b'It is 510A , dimwit, the persecuted heroine, a scientific category, recognized by everyone except you. The sources are all there in the Wiki links. And nobody says that the Chinese version must come from the Greek one, all it says is that the Greek was an earlier Cinderella version. Since Windling has based her argument of the oriental origin on the temporal priority of Ye, the existence of the much earlier Greek version, shows her to be wrong. Either we include a part on Rhodopis or we exclude the misinformed Windling reserach. You choose. With your destructive approach, what are you doing anyway at Wikipedia? Regards',\n       b'\"\\n\\nMy apologies, I thought when you mentioned you were related to the \"\"Jewish Giant\"\" you were ironically referring to Dianne Arbus herself, since it is to the JG himself you are related to my point still holds, as someone of at least partial Jewish ancestry why are you aiding the anti semites of the world?\\n\\nAs to my edit, you totally fail to answer the substance of my point: what is the point of identifying Noam Chomsky as \"\"jewish\"\" other than to lend legitimacy to his anti Jewish stance?  And if that description stands, why is it a pov reference to indicate that while he may be of Jewish ancestry, he does not identify \"\"jewishly\"\" in any way.  Look forward to your non ironical answer.  \"',\n       b\"THEIR GOING TO BATH \\n\\nBUT THEY'RE A MUSLIM AND PAKI SO WHAT DO YOU EXPECT? 86.178.49.98\",\n       b'That would make it the flag of the team using it, not the flag of the country. As with all national flag articles, they are about national flags, which the UB is not any more.',\n       b'Some of the 2010 movie My Lai Four by the italian producer Gianni Paolucci can be viewed on You Tube. This film looks like trash. If it is to be mentioned on this page then another movie about My Lai, by a Vietnamese film maker, Le Dan, also released last year, should also be mentioned. This is a film romanticization about William Calley, leader of the first platoon into My Lai.',\n       b'And this is why you should keep your watchlist nuked for a bit (assuming that T Canens meant what I think he meant, and you avoid the ban). Quit thinking about the quality of the encyclopaedia for a bit and focus on your peace of mind.',\n       b'Case settled? \\n\\nThere is currently a footnote saying the case was settled out of court on July 29, 2008 for an undisclosed sum, and that there is a gag order forbidding disclosure of how much the settlement is. However, the link given as reference is broken, and I was unable to find any other reference on the web. Could it really be that NO ONE wrote about the settlement in their blogs, when so much was said earlier about the lawsuit?',\n       b\"Awww, my deepest appologies, here's a Kleenex.\",\n       b'\"\\n\\nThe only other option to \"\"evisceration\"\" might be to take the entire thing back to userspace until it is completed. Would that be better for you, D? Somehow I doubt it.\\nBTW, you will be pleased to hear that I have been developing one of your stubs. Not much because of source limitations, but a bit nonetheless - Green Leaves. -   \"',\n       b\"=Am I disruptive ?\\nSteve, the criticism of Calo's paper is addressed in the two articles by Cloonan and Trell  that are also in the Wikipedia article about Ruggero Santilli. These two articles were there, you ignored them and quickly eliminated the entire section. There was also another article about ongoing experiments that you ignored and cancelled...but you erased everything so nobody knows....in addition you and i are not qualified to take on pros or cons, just to report it and you avoided and prohibited that. I do not know where the idea of law suit came from. You are the one who got the three legal points and said that you were saying everything you wanted since no legal  suit could be won against you . What a fine example of editorial ethics, I already told what I think about your position, but you ignored it.  If you  do not want to add the articles, as are in the Wikipedia page itself , I could not care less, it is the loss of the readers who do not read about ongoing research with its pros and cons. I thought  it was interesting since I am sure there  are more and that shows a vital field of research. However I take issue with associating Santilli, with fraud, kook, fringe  and I am sure that it is against the rules of Wikipedia on libel and reckless behavior with the clear intent to damage since the fringe scientist definition is not supported by anything of value,  except, perhaps some blogs and  tabloids which, according to the rules, have no source value since they are not peer-reviewed articles! Unless you change the rules again. Scuranova\",\n       b'Show up again when you figure out how to rub two brain cells together, and after you lose that filthy ego of yours!71.174.141.4',\n       b'While I believe the situation could have been handled better by everyone, there is nothing to be gained by further discord on this subject.  You have been given an adequate chance to properly cite your additions; I suggest you either do so, or not, and move on.  I have closed the discussion on my talk page and request that no further comment be made there.  Thank you.',\n       b'\"\\nthere is no electron\\nFacts:\\n\\n1) Faradays law of induction cannot be explained by electron theory Source: physics Textbook \\n2) Maxwells electromatetic theory is in direct violation of electron theory. Most notably from his theory displacement current is still taught as mainstream science. Source physics Textbook \\n3) In chemisty the number of electrons leaving a mass is determined by the voltage Source: chemistry textbook \\n4) In induction physics, the current is determined by the number of coils in the winding source: physics textbooks \\n5) In circiut theory the current is determined by the load Source: also physics textbook \\n6) these three things are all different \\n7) In the power industry, there is also something called \"\"current draw\"\" which is a current not determined by the load but the power supply Source: Con Edison training manual \\n8) All of these things with the exception of 7 are excepted theories of science that contflict with the electron. Number 4 and 5 conflict with number 3 which is part of the definition of the electron. \\nKnown people in the scientific community who said that they did not hold electrons to be true without proof(or admitted there existance to be different than that of mainstream science)\\n\\n1) Albert Einstien said that their existance was different \\n2) Max Planck said that he would not hold it to be true without proof \\n3) Faraday was totally against the idea before it ever came out \\n4) Tesla in his patents refers to electricity as a pressure rather than a substance \\n5) Heaviside said that electrons were compressed ether \\n6) JJ Thomson the so called discover of the electron said that he did not agree with the electron theory that was interpreted from his experiment. \\nAnyone else that a circuits textbook fails to mentions. Actually I find that most circuits textbooks don\\'t even talk at all about any scientist. What I demand:\\n\\n1) A scientific presentation of electron theory that uses the scientific method \\n2) A list of people in the scientific community who actually accepted this (and hold electrons to be a fact and more than a theory), and on what basis of the scientific method did they accept it.\"',\n       b\"This is not your average not-many-people-know-about biography or that like article. It shouldn't be to much to ask that you allow a few days, or even a few hours, for everyone who has this article on his watchlist to comment or, in my case, to prepare a more elaborate comment. It certainly is the only way to avoid an edit war. Now I don't mind a full protection of this article due to another edit war - actually I think it would help since we do have a lot of argument to catch up on.\",\n       b'Gov. Scott \\n\\nI recall shaking hands with the Governor in 1952 at the age of 5. I looked him in the eye and he looked me in the eye but I do not recall what he said. However, Concern about GMO and nutrition continues to be a major part of my life as well as technologies which can help anyone get to the bottom of an issue and completely understand it. I am also deeply committed to equality among races perhaps because my own race has the benefit of 4,000 years of cultural evolution which is not always shared but withheld from those who might improve it. I am truly a better person and  fortunate that my uncle introduced me to Gov. Scott. ~',\n       b'Response - Tephrachronology is a precise and well established procedure. The Laarcher See Volcano is dated to within 40 years, 200 years before the Younger Dryas, and for you to imply that anyone in the geological community subscribes to your personal hypothesis that the Laarcher See volcanic eruption was related in any way to the Younger Dryas Chronozone - is simply false. I have no more plans to attempt to either educate you or edit your ramblings here.',\n       b\"lonely\\n\\nGo outside and make a friend.  You've seen all your mom's basement has to offer.\",\n       b'fuck u little bitch. im the porn king',\n       b'Mr.Kumar rao! Thank you for your support about Rajus article,i have seen your message in my talk page.All the Best. -abcde',\n       b'Your messages.  \\n\\nI am sorry but what are you talking about?',\n       b\"By the way, if you don't think MacDonald qualifies as an anthropologist, you might want to examine what sort of Ph. D.s and professorships anthropologists usually have or have had. Here is MacDonald's quite impressive curriculum vitae: http://www.csulb.edu/~kmacd/VITA2005.pdf\",\n       b'Visit http://www.WebNoeSys.com\\nMail us at support@WebNoeSys.com, contact@WebNoeSys.com',\n       b'\"\\n\\n Powers and Abilities \\n\\nMy addition to the Hulk page was removed by a chap named Cameron Scott, but so far he has failed to explain exactly why he did so. My addition to the P+A section simply stated that Hulk\\'s one \"\"vulnerability\"\" is that he can be made weaker when calmed down. This has happened in various comics and so is accurate information. Any thoughts?   \\xe2\\x80\\x94Preceding unsigned comment added by  (talk \\xe2\\x80\\xa2 contribs)  \\n\\nIt\\'s been removed for a few reasons. One, you\\'re violating WP:SOCK, using multiple accounts to try to make it looks like there\\'s more support for your edit than there is. Two, Using one instance of an idea, citing a comic book for it, is not enough. Hulk has been stopped in other ways - Magic, Drugs, being knocked out cold. That his opponents have used the tactic of defusing his anger to allow other methods of defeat to work is also different than that alone being the defeat itself.   \"',\n       b\"Deletion discussion about Afif Chaya \\nHello, Ammar Alwaeli, \\n\\nI wanted to let you know that there's a discussion about whether Afif Chaya should be deleted. Your comments are welcome at  Wikipedia:Articles for deletion/Afif Chaya . \\n\\nIf you're new to the process, articles for deletion is a group discussion (not a vote!) that usually lasts seven days. If you need it, there is a guide on how to contribute. Last but not least, you are highly encouraged to continue improving the article; just be sure not to remove the tag about the deletion nomination from the top. \\n\\nThanks, \\xe2\\x80\\x94 \\xc2\\xbb\\xc2\\xbb (talk to me)\",\n       b'\"\\n\\nThis  is better, but it does talk about \"\"simultaneous values\"\".   \"',\n       b'yeah \\n\\nbut scientology is still a fucking joke.  70.92.103.13',\n       b\"your atricle demonstrates absolutely nothing. first, it nowhere says Fangio was the greatest. Second, it rests its arguemnt on the grounds that he won 5 C in 8 years, which is supposedly better than Shumacher's. But wait it's not!. Schumacher won 5 WC in FIVE YEARS. Gone.\",\n       b'\" May 2009 (UTC)\\n\\nDo you mean you can read about those work conditions & not see that it\\'s slavery?  Prepare yourself for earthshaking news: the wage system is slavery but most people didn\\'t think so.  Slavery means \"\"someone who is controlled by someone or something.\"\"   A boss controls a slave, but a slave also enslaves a boss.   Being offered a wage you can\\'t live on is slavery.   All jobs are being eliminated by machine-slaves,  which makes it more obvious than ever before that all people must have a Guaranteed Minimum Income, although I prefer the word \\'residual\\' to \\'minimum\\'.  Read child labour & try to see it was child slavery into the 20th century, & it still is today since many children work in their parents\\' business.  When they \"\"ended child labour\"\" that didn\\'t set the children free; they were \"\"free to starve\"\" because what they need/ed is a GMI.  Having children is a form of slavery too: parents become slaves to their children (when they need to eat, bathe, etc) but the children are also slaves to their parents.  So \"\"no control\"\" also means we\\'ll always be slaves to needing food, & we can find ways to eliminate all the work with machines which again means all people need a GMI.  (Actually all people should own all things, because when a few rich people own everything it\\'s obviously slavery; how did every person not know that?  America refused to let any attempts at Socialism & Communism succeed; instead they forced it to fail by starting many wars, teaching torture, & you must read Wm Blum\\'s \"\"Rogue State\"\".  Fidel Castro was right & America was wrong.)  And the BLS is wrong; they always underestimate, because most of those people are probably illegal immigrants from many generations back.  They say every year 50,000 people are smuggled into the US to work as prostitutes (that\\'s slavery), so can you imagine how many illegals there must be in America hidden behind walls, & all supporting a few rich slave masters who travel the world & raise their families on the backs of SLAVES in America.  How can you read about multinational corporations & not see they cause poverty in those other countries, as well as in America?    If Liberia has a population of 3 million & one corporation goes there & \"\"creates only 2000 jobs\"\" then can\\'t anyone see the wage is causing their poverty??  That\\'s just an example, but do you see what I\\'m saying about the wage is the problem worldwide, not the solution, like America thinks?  When 3,000 people show up to apply for 20 job openings, you know the wage is wrong & it\\'s slavery.  I know I\\'m a slave, & I know every person is a slave & I\\'m right & everyone is wrong.  Gerry Spence also says we\\'re slaves in Give Me Liberty.  That\\'s all for now.    02:08, 31\"',\n       b\"It's not about some obscure song. Rather, it's about a very famous song that purportedly was written about her. That's more than trivia. It's a part of her life that she has talked about on many occasions.\",\n       b'possible racism of user rodhullandemu - as this is my talk page, i can of course decide the headings. this is not my last warning. i do like to eat pineapples.'],\n      dtype=object), array([[0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [1, 0, 0, 0, 1, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0],\n       [1, 1, 1, 0, 1, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [1, 0, 1, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0]]))\n\n\nAnd we check also the shape. We expect a feature of shape (batch, ) and a target of shape (batch, number of labels).\n\n\nCode\nprint(\n  f\"text train shape: {train_ds.as_numpy_iterator().next()[0].shape}\\n\",\n  f\" text train type: {train_ds.as_numpy_iterator().next()[0].dtype}\\n\",\n  f\"label train shape: {train_ds.as_numpy_iterator().next()[1].shape}\\n\",\n  f\"label train type: {train_ds.as_numpy_iterator().next()[1].dtype}\\n\"\n  )\n\n\ntext train shape: (32,)\n  text train type: object\n label train shape: (32, 6)\n label train type: int64"
  },
  {
    "objectID": "posts/toxic_comment_filter/toxic_comment_filter.html#classification-report",
    "href": "posts/toxic_comment_filter/toxic_comment_filter.html#classification-report",
    "title": "Toxic Comment Filter",
    "section": "Classification Report",
    "text": "Classification Report\n\n\nCode\ncr = classification_report(\n  ytrue,\n  ypred,\n  target_names=config.labels,\n  digits=4,\n  output_dict=True\n  )\ndf_cr = pd.DataFrame.from_dict(cr).reset_index()\n\n\n\nCode\nlibrary(reticulate)\ndf_cr &lt;- py$df_cr %&gt;% dplyr::rename(names = index)\ncols &lt;- df_cr %&gt;% colnames()\ndf_cr %&gt;% \n  pivot_longer(\n    cols = -names,\n    names_to = \"metrics\",\n    values_to = \"values\"\n  ) %&gt;% \n  pivot_wider(\n    names_from = names,\n    values_from = values\n  )\n\n\n\n\nTable 6: Classification report\n\n\n\n# A tibble: 10 × 5\n   metrics       precision recall `f1-score` support\n   &lt;chr&gt;             &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;\n 1 toxic            0.552  0.890      0.682     2262\n 2 severe_toxic     0.236  0.917      0.375      240\n 3 obscene          0.550  0.936      0.692     1263\n 4 threat           0.0366 0.493      0.0681      69\n 5 insult           0.471  0.915      0.622     1170\n 6 identity_hate    0.116  0.720      0.200      207\n 7 micro avg        0.416  0.896      0.569     5211\n 8 macro avg        0.327  0.812      0.440     5211\n 9 weighted avg     0.495  0.896      0.629     5211\n10 samples avg      0.0502 0.0848     0.0597    5211"
  },
  {
    "objectID": "posts/toxic_comment_filter/history/f1score/toxic-comment-filter-bilstm.html",
    "href": "posts/toxic_comment_filter/history/f1score/toxic-comment-filter-bilstm.html",
    "title": "Simone Brazzi",
    "section": "",
    "text": "::: {#cell-0 .cell _uuid=‘8f2839f25d086af736a60e9eeb907d3b93b6e0e5’ _cell_guid=‘b1076dfc-b9ad-4769-8c92-a6c4dae69d19’ trusted=‘true’}\n\nCode\n# This Python 3 environment comes with many helpful analytics libraries installed# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python# For example, here's several helpful packages to loadimport numpy as np # linear algebraimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)# Input data files are available in the read-only \"../input/\" directory# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directoryimport osfor dirname, _, filenames in os.walk('/kaggle/input'):    for filename in filenames:        print(os.path.join(dirname, filename))# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n:::\n\n\nCode\nimport pandas as pdimport numpy as npimport matplotlib.pyplot as pltimport tensorflow as tfimport kerasimport keras_nlpfrom keras.backend import clear_sessionfrom keras.models import Model, load_modelfrom keras.layers import TextVectorization, Input, Dense, Embedding, Dropout, GlobalAveragePooling1D, LSTM, Bidirectional, GlobalMaxPool1D, Flatten, Attentionfrom keras.metrics import Precision, Recall, AUC, SensitivityAtSpecificity, SpecificityAtSensitivity, F1Scorefrom sklearn.model_selection import train_test_split, KFoldfrom sklearn.metrics import multilabel_confusion_matrix, classification_report, ConfusionMatrixDisplay, precision_recall_curve, f1_score, recall_score, roc_auc_scoreclass Config():    def __init__(self):        self.url = \"https://s3.eu-west-3.amazonaws.com/profession.ai/datasets/Filter_Toxic_Comments_dataset.csv\"        self.path = \"/Users/simonebrazzi/datasets/toxic_comment/Filter_Toxic_Comments_dataset.csv\"        self.max_tokens = 20000        self.output_sequence_length = 911 # check the analysis done to establish this value        self.embedding_dim = 128        self.batch_size = 32        self.epochs = 100        self.temp_split = 0.3        self.test_split = 0.5        self.random_state = 42        self.total_samples = 159571 # total train samples        self.train_samples = 111699        self.val_samples = 23936        self.features = 'comment_text'        self.labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']        self.new_labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate', \"clean\"]        self.label_mapping = {label: i for i, label in enumerate(self.labels)}        self.new_label_mapping = {label: i for i, label in enumerate(self.labels)}                self.model = \"model_f1.keras\"        self.checkpoint = \"checkpoint.lstm_model_f1.keras\"        self.history = \"lstm_model_f1.xlsx\"        self.matrix_file = \"confusion_matrices.png\"                self.metrics = [            Precision(name='precision'),            Recall(name='recall'),            AUC(name='auc', multi_label=True, num_labels=len(self.labels)),            F1Score(name=\"f1\", average=\"macro\")                    ]    def get_early_stopping(self):        early_stopping = keras.callbacks.EarlyStopping(            monitor=\"val_f1\", # \"val_recall\",            min_delta=0.2,            patience=10,            verbose=0,            mode=\"max\",            restore_best_weights=True,            start_from_epoch=3        )        return early_stopping    def get_model_checkpoint(self, filepath):        model_checkpoint = keras.callbacks.ModelCheckpoint(            filepath=filepath,            monitor=\"val_f1\", # \"val_recall\",            verbose=0,            save_best_only=True,            save_weights_only=False,            mode=\"max\",            save_freq=\"epoch\"        )        return model_checkpoint    def find_optimal_threshold_cv(self, ytrue, yproba, metric, thresholds=np.arange(.05, .35, .05), n_splits=7):      # instantiate KFold      kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)      threshold_scores = []      for threshold in thresholds:        cv_scores = []        for train_index, val_index in kf.split(ytrue):          ytrue_val = ytrue[val_index]          yproba_val = yproba[val_index]          ypred_val = (yproba_val &gt;= threshold).astype(int)          score = metric(ytrue_val, ypred_val, average=\"macro\")          cv_scores.append(score)        mean_score = np.mean(cv_scores)        threshold_scores.append((threshold, mean_score))        # Find the threshold with the highest mean score        best_threshold, best_score = max(threshold_scores, key=lambda x: x[1])      return best_threshold, best_scoreconfig = Config()\n\n\n\n\nCode\nfile = tf.keras.utils.get_file(\"Filter_Toxic_Comments_dataset.csv\", config.url)df = pd.read_csv(file)# df = pd.read_csv(config.path)df.loc[df.sum_injurious == 0, \"clean\"] = 1df.loc[df.sum_injurious != 0, \"clean\"] = 0x = df[config.features].valuesy = df[config.labels].valuesxtrain, xtemp, ytrain, ytemp = train_test_split(  x,  y,  test_size=config.temp_split, # .3  random_state=config.random_state  )xtest, xval, ytest, yval = train_test_split(  xtemp,  ytemp,  test_size=config.test_split, # .5  random_state=config.random_state  )print(    f\"df shape: {df.shape[0]}\\n\",  f\"xtrain shape: {xtrain.shape}\\n\",  f\"ytrain shape: {ytrain.shape}\\n\",  f\"xtest shape: {xtest.shape}\\n\",  f\"ytest shape: {ytest.shape}\\n\",  f\"xval shape: {xval.shape}\\n\",  f\"yval shape: {yval.shape}\\n\"  f\"labels: {config.labels}\\n\"  )text_vectorization = TextVectorization(  max_tokens=config.max_tokens,  standardize=\"lower_and_strip_punctuation\",  split=\"whitespace\",  output_mode=\"int\",  output_sequence_length=config.output_sequence_length,  pad_to_max_tokens=True  )train_ds = (    tf.data.Dataset    .from_tensor_slices((xtrain, ytrain))    .shuffle(xtrain.shape[0])    .batch(config.batch_size)    .prefetch(tf.data.experimental.AUTOTUNE))test_ds = (    tf.data.Dataset    .from_tensor_slices((xtest, ytest))    .batch(config.batch_size)    .prefetch(tf.data.experimental.AUTOTUNE))val_ds = (    tf.data.Dataset    .from_tensor_slices((xval, yval))    .batch(config.batch_size)    .prefetch(tf.data.experimental.AUTOTUNE))# Prepare a dataset that only yields raw text inputs (no labels)text_train_ds = train_ds.map(lambda x, y: x)# Adapt the text vectorization layer to the text data to index the dataset vocabularytext_vectorization.adapt(text_train_ds)processed_train_ds = train_ds.map(    lambda x, y: (text_vectorization(x), y),    num_parallel_calls=tf.data.experimental.AUTOTUNE)processed_val_ds = val_ds.map(    lambda x, y: (text_vectorization(x), y),    num_parallel_calls=tf.data.experimental.AUTOTUNE)processed_test_ds = test_ds.map(    lambda x, y: (text_vectorization(x), y),    num_parallel_calls=tf.data.experimental.AUTOTUNE)from keras.layers import LayerNormalization, MultiHeadAttention, GlobalAveragePooling1Ddef get_deeper_lstm_model():    clear_session()    inputs = Input(shape=(None,), dtype=tf.int64, name=\"inputs\")    embedding = Embedding(        input_dim=config.max_tokens,        output_dim=config.embedding_dim,        mask_zero=True,        name=\"embedding\"    )(inputs)    x = Bidirectional(LSTM(256, return_sequences=True, name=\"bilstm_1\"))(embedding)    x = Bidirectional(LSTM(128, return_sequences=True, name=\"bilstm_2\"))(x)    # Global average pooling    x = GlobalAveragePooling1D()(x)    # Add regularization    x = Dropout(0.3)(x)    x = Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)    x = LayerNormalization()(x)    outputs = Dense(len(config.labels), activation='sigmoid', name=\"outputs\")(x)    model = Model(inputs, outputs)    model.compile(optimizer='adam', loss=\"binary_crossentropy\", metrics=config.metrics, steps_per_execution=32)    return modelmodel = get_deeper_lstm_model()lab = pd.DataFrame(columns=config.labels, data=ytrain)lab.shaper = lab.sum() / len(ytrain)class_weight = dict(zip(range(len(config.labels)), r))class_weightsteps_per_epoch = config.train_samples // config.batch_sizevalidation_steps = config.val_samples // config.batch_size# callbacksmy_es = config.get_early_stopping()my_mc = config.get_model_checkpoint(filepath=\"/kaggle/working/\"  + config.checkpoint)callbacks = [my_es, my_mc]\n\n\nDownloading data from https://s3.eu-west-3.amazonaws.com/profession.ai/datasets/Filter_Toxic_Comments_dataset.csv\n66034407/66034407 ━━━━━━━━━━━━━━━━━━━━ 3s 0us/step\ndf shape: 159571\n xtrain shape: (111699,)\n ytrain shape: (111699, 6)\n xtest shape: (23936,)\n ytest shape: (23936, 6)\n xval shape: (23936,)\n yval shape: (23936, 6)\nlabels: ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n\n\n\n\n\nCode\nhistory_deeper_lstm_model = model.fit(    processed_train_ds.repeat(),    epochs=config.epochs,    validation_data=processed_val_ds.repeat(),    callbacks=callbacks,    class_weight=class_weight,    steps_per_epoch=steps_per_epoch,    validation_steps=validation_steps)\n\n\nEpoch 1/100\n3490/3490 ━━━━━━━━━━━━━━━━━━━━ 390s 98ms/step - auc: 0.8880 - f1: 0.0441 - loss: 0.0575 - precision: 0.7018 - recall: 0.4701 - val_auc: 0.9494 - val_f1: 0.0293 - val_loss: 0.0641 - val_precision: 0.8939 - val_recall: 0.4625\nEpoch 2/100\n3490/3490 ━━━━━━━━━━━━━━━━━━━━ 315s 90ms/step - auc: 0.9557 - f1: 0.0290 - loss: 0.0050 - precision: 0.7994 - recall: 0.6490 - val_auc: 0.9601 - val_f1: 0.0294 - val_loss: 0.0535 - val_precision: 0.7908 - val_recall: 0.6617\nEpoch 3/100\n3490/3490 ━━━━━━━━━━━━━━━━━━━━ 315s 90ms/step - auc: 0.9672 - f1: 0.0291 - loss: 0.0044 - precision: 0.8166 - recall: 0.6815 - val_auc: 0.9633 - val_f1: 0.0295 - val_loss: 0.0544 - val_precision: 0.7733 - val_recall: 0.6961\nEpoch 4/100\n3490/3490 ━━━━━━━━━━━━━━━━━━━━ 315s 90ms/step - auc: 0.9708 - f1: 0.0288 - loss: 0.0040 - precision: 0.8243 - recall: 0.7117 - val_auc: 0.9559 - val_f1: 0.0293 - val_loss: 0.0546 - val_precision: 0.7886 - val_recall: 0.6696\nEpoch 5/100\n3490/3490 ━━━━━━━━━━━━━━━━━━━━ 315s 90ms/step - auc: 0.9718 - f1: 0.0290 - loss: 0.0036 - precision: 0.8409 - recall: 0.7476 - val_auc: 0.9539 - val_f1: 0.0294 - val_loss: 0.0601 - val_precision: 0.7354 - val_recall: 0.6837\nEpoch 6/100\n3490/3490 ━━━━━━━━━━━━━━━━━━━━ 316s 90ms/step - auc: 0.9748 - f1: 0.0290 - loss: 0.0034 - precision: 0.8470 - recall: 0.7679 - val_auc: 0.9406 - val_f1: 0.0295 - val_loss: 0.0710 - val_precision: 0.7332 - val_recall: 0.6863\nEpoch 7/100\n3490/3490 ━━━━━━━━━━━━━━━━━━━━ 315s 90ms/step - auc: 0.9768 - f1: 0.0293 - loss: 0.0031 - precision: 0.8592 - recall: 0.7822 - val_auc: 0.9384 - val_f1: 0.0292 - val_loss: 0.0724 - val_precision: 0.7471 - val_recall: 0.6667\nEpoch 8/100\n3490/3490 ━━━━━━━━━━━━━━━━━━━━ 316s 90ms/step - auc: 0.9756 - f1: 0.0290 - loss: 0.0030 - precision: 0.8650 - recall: 0.7878 - val_auc: 0.9359 - val_f1: 0.0294 - val_loss: 0.0767 - val_precision: 0.7302 - val_recall: 0.6641\nEpoch 9/100\n3490/3490 ━━━━━━━━━━━━━━━━━━━━ 315s 90ms/step - auc: 0.9769 - f1: 0.0290 - loss: 0.0028 - precision: 0.8764 - recall: 0.8020 - val_auc: 0.9415 - val_f1: 0.0295 - val_loss: 0.0750 - val_precision: 0.7159 - val_recall: 0.6955\nEpoch 10/100\n3490/3490 ━━━━━━━━━━━━━━━━━━━━ 316s 90ms/step - auc: 0.9785 - f1: 0.0293 - loss: 0.0028 - precision: 0.8752 - recall: 0.8072 - val_auc: 0.9263 - val_f1: 0.0295 - val_loss: 0.0795 - val_precision: 0.7649 - val_recall: 0.6282\nEpoch 11/100\n3490/3490 ━━━━━━━━━━━━━━━━━━━━ 315s 90ms/step - auc: 0.9775 - f1: 0.0291 - loss: 0.0027 - precision: 0.8837 - recall: 0.8062 - val_auc: 0.9360 - val_f1: 0.0296 - val_loss: 0.0721 - val_precision: 0.7470 - val_recall: 0.6254\nEpoch 12/100\n3490/3490 ━━━━━━━━━━━━━━━━━━━━ 313s 89ms/step - auc: 0.9791 - f1: 0.0297 - loss: 0.0027 - precision: 0.8825 - recall: 0.8113 - val_auc: 0.9368 - val_f1: 0.0296 - val_loss: 0.0776 - val_precision: 0.7307 - val_recall: 0.6644\nEpoch 13/100\n3490/3490 ━━━━━━━━━━━━━━━━━━━━ 316s 90ms/step - auc: 0.9788 - f1: 0.0293 - loss: 0.0026 - precision: 0.8795 - recall: 0.8193 - val_auc: 0.9370 - val_f1: 0.0295 - val_loss: 0.0743 - val_precision: 0.7382 - val_recall: 0.6413\nEpoch 14/100\n3490/3490 ━━━━━━━━━━━━━━━━━━━━ 314s 89ms/step - auc: 0.9781 - f1: 0.0295 - loss: 0.0025 - precision: 0.8892 - recall: 0.8181 - val_auc: 0.9309 - val_f1: 0.0295 - val_loss: 0.0855 - val_precision: 0.7386 - val_recall: 0.6492\n\n\n\n\nCode\nmodel.save(\"/kaggle/working/\" + config.model)pd.DataFrame.from_dict(history_deeper_lstm_model.history).to_excel(\"/kaggle/working/\" + config.history)\n\n\n\n\nCode\nimport kerasfrom keras.models import load_modelmodel = load_model(filepath=\"/kaggle/working/\" + config.model)history = pd.read_excel(\"/kaggle/working/\" + config.history)\n\n\n\n\nCode\nconfig.model, config.history\n\n\n('model_f1.keras', 'lstm_model_f1.xlsx')\n\n\n\n\nCode\nvalidation = model.evaluate(  processed_val_ds.repeat(),  steps=validation_steps, # 748    verbose=1  )\n\n\n748/748 ━━━━━━━━━━━━━━━━━━━━ 87s 38ms/step - auc: 0.9542 - f1: 0.0287 - loss: 0.0529 - precision: 0.7858 - recall: 0.6763\n\n\n\n\nCode\npredictions = model.predict(processed_test_ds, verbose=1)\n\n\n748/748 ━━━━━━━━━━━━━━━━━━━━ 45s 41ms/step\n\n\n\n\nCode\nytrue = ytest.astype(int)y_pred_proba = predictionsoptimal_threshold_recall, best_score_recall = config.find_optimal_threshold_cv(ytrue, y_pred_proba, recall_score)# Use the optimal threshold to make predictionsfinal_predictions_recall = (y_pred_proba &gt;= optimal_threshold_recall).astype(int)\n\n\n\n\nCode\n# convert probability predictions to predictionsypred = predictions &gt;=  optimal_threshold_recall # .05ypred = ypred.astype(int)# create a plot with 3 by 2 subplotsfig, axes = plt.subplots(3, 2, figsize=(15, 15))axes = axes.flatten()mcm = multilabel_confusion_matrix(ytrue, ypred)# plot the confusion matrices for each labelfor i, (cm, label) in enumerate(zip(mcm, config.labels)):    disp = ConfusionMatrixDisplay(confusion_matrix=cm)    disp.plot(ax=axes[i], colorbar=False)    axes[i].set_title(f\"Confusion matrix for label: {label}\")plt.tight_layout()plt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\ncr = classification_report(  ytrue,  ypred,  target_names=config.labels,  digits=4  )print(cr)\n\n\n               precision    recall  f1-score   support\n\n        toxic     0.5524    0.8899    0.6817      2262\n severe_toxic     0.2355    0.9167    0.3748       240\n      obscene     0.5495    0.9359    0.6924      1263\n       threat     0.0366    0.4928    0.0681        69\n       insult     0.4712    0.9145    0.6219      1170\nidentity_hate     0.1164    0.7198    0.2004       207\n\n    micro avg     0.4164    0.8958    0.5685      5211\n    macro avg     0.3269    0.8116    0.4399      5211\n weighted avg     0.4947    0.8958    0.6295      5211\n  samples avg     0.0502    0.0848    0.0597      5211\n\n\n\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n\n\n\n\nCode"
  }
]