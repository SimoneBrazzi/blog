---
title: "Credit Score"
image: "credit_score.jpeg"
description: "ML explainability classification"
author: "Simne Brazzi"
date: "2024-10-26"
draft: false
categories:
  - code
  - ML
  - Python, R
toc: true
toc-title: "Table of Contents"
# other-links:
#   - icon: file-pdf
#     text: LDA Paper
#     href: https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf
toc-depth: 3
number-sections: true
number-depth: 3
embed-resources: true
anchor-sections: true
smooth-scroll: true
highlight-style: monokai
code-line-numbers: true
code-copy: true
code-link: true
code-fold: true
other-links:
  - icon: file-code
    text: Kaggle
    href: https://www.kaggle.com/code/simonebrazzi/credit-score

execute:
  warnings: false
  freeze: auto

editor_options: 
  chunk_output_type: console
---

# Introduction

Create a model capable of estimating the creditworthiness of customers, in order to help the dedicated team understand whether or not to accept a credit card application.

## Features

-   ID: customer identification number
-   CODE_GENDER: gender of the customer
-   FLAGOWNCAR: indicator of car ownership
-   FLAGOWNREALTY: indicator of house ownership
-   CNT_CHILDREN: number of children
-   AMTINCOMETOTAL: annual income
-   NAMEINCOMETYPE: type of income
-   NAMEEDUCATIONTYPE: level of education
-   NAMEFAMILYSTATUS: marital status
-   NAMEHOUSINGTYPE: type of dwelling
-   DAYS_BIRTH: number of days since birth
-   DAYS_EMPLOYED: number of days since employment (if positive, indicates the number of days since being unemployed)
-   FLAG_MOBIL: indicator for the presence of a mobile phone number
-   FLAGWORKPHONE: indicator of the presence of a work phone number
-   FLAG_PHONE: indicator of the presence of a telephone number
-   FLAG_EMAIL: indicator for the presence of an email address
-   OCCUPATION_TYPE: type of occupation
-   CNTFAMMEMBERS: number of family members
-   TARGET: variable which is worth 1 if the customer has a high creditworthiness (constant payment of installments), 0 otherwise.

If a customer is denied a credit card, the team must be able to give a reason. This means that your model must provide indications that are easy to interpret.

It is a binary classification which needs a good explainability.

# Import

## R

```{r import_r}
#| warning: false

library(tidyverse, verbose = FALSE)
library(ggplot2)
library(plotly)
library(reticulate)
library(gt)
library(scales)
library(shapr)
```

## Python

```{python, import_python}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import pickle
import time
import os

from datetime import date
from dateutil.relativedelta import relativedelta
from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler, MinMaxScaler, LabelBinarizer
from sklearn.pipeline import Pipeline
from sklearn.tree import DecisionTreeClassifier
from xgboost import XGBClassifier
from xgboost import DMatrix
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
```

## Config class

::: {#config-class}
```{python, config}
#| lst-label: lst-config
#| lst-cap: Config class


class Config():
    def __init__(self):
        """
        Initialize configuration settings for credit scoring model pipeline.
        Sets up file paths, random state, and feature column classifications.
        """
        # Base directory and file paths
        self.path="/Users/simonebrazzi/R/blog/posts/credit_score/"
        self.file="/Users/simonebrazzi/R/blog/posts/credit_score/credit_scoring.csv"
        self.pkl_path="~/R/blog/posts/credit_score/pkl/"
        
        # Set random state for reproducibility
        self.random_state=42
        
        # Define feature columns by type
        self.col_binary = ['code_gender', 'flag_own_car', 'flag_own_realty']
        self.col_ordinal = ["name_income_type", "name_education_type", "name_family_status", "name_housing_type", "occupation_type"]
        self.col_numeric = ['cnt_children', 'amt_income_total', 'cnt_fam_members']
        self.features = self.col_binary + self.col_ordinal + self.col_numeric
    
    def dump_pkl(self, path, model):
        """
        Serialize model to pickle file.
        Args:
            path: Output file path
            model: Model to serialize
        """
        with open(path, "wb") as f:
            pickle.dump(model, f)
    
    def load_pkl(self, path):
        """
        Load model from pickle file.
        Args:
            path: Input pickle file path
        Returns:
            Deserialized model
        """
        with open(path, "rb") as f:
            model = pickle.load(f)
        return model
    
    def make_dtc(self):
        """
        Create Decision Tree Classifier pipeline with preprocessing and grid search.
        Returns:
            GridSearchCV object configured for DTC
        """
        # Initialize encoders and scaler
        ohe = OneHotEncoder(drop='if_binary', sparse_output=False)
        oe = OrdinalEncoder().set_output(transform='pandas')
        ss = StandardScaler()
        
        # Create balanced Decision Tree
        dtc = DecisionTreeClassifier(class_weight="balanced", random_state=config.random_state)
        
        # Set up preprocessing pipeline
        preprocessor = ColumnTransformer(
            transformers=[
                ("binary_encoder", ohe, config.col_binary),
                ("ordinal_encoder", oe, config.col_ordinal),
                ("standard_scaler", ss, config.col_numeric)
            ],
            remainder='passthrough'
        )
        
        # Create full pipeline
        pipe_dtc = Pipeline(
            steps=[
                ("preprocessor", preprocessor),
                ("dtc", dtc)
            ]
        )
        
        # Define hyperparameter search space
        param_grid_dtc = {
            "dtc__criterion": ["gini", "entropy", "log_loss"],
            "dtc__splitter": ["best", "random"],
            "dtc__max_depth": [1, 2, 5, 10, 15]
        }
        
        # Set up multiple scoring metrics
        scoring = {
            "accuracy": "accuracy",
            "f1": "f1",
            "roc_auc": "roc_auc"
        }
        
        # Configure grid search
        grid_search_dtc = GridSearchCV(
            estimator=pipe_dtc,
            param_grid=param_grid_dtc,
            cv=7,
            scoring=scoring,
            refit="f1",
            n_jobs=-1
        )
        
        return grid_search_dtc
    
    def make_rfc(self):
        """
        Create Random Forest Classifier pipeline with preprocessing and grid search.
        Returns:
            GridSearchCV object configured for RFC
        """
        # Initialize preprocessing components
        ohe = OneHotEncoder(drop='if_binary', sparse_output=False)
        oe = OrdinalEncoder().set_output(transform='pandas')
        ss = StandardScaler()
        
        # Create balanced Random Forest
        rfc = RandomForestClassifier(class_weight="balanced", random_state=config.random_state)
        
        # Set up preprocessing pipeline
        preprocessor = ColumnTransformer(
            transformers=[
                ("binary_encoder", ohe, config.col_binary),
                ("ordinal_encoder", oe, config.col_ordinal),
                ("standard_scaler", ss, config.col_numeric)
            ],
            remainder='passthrough'
        )
        
        # Create full pipeline
        pipe_rfc = Pipeline(
            steps=[
                ("preprocessor", preprocessor),
                ("rfc", rfc)
            ]
        )
        
        # Define parameter search space
        param_dist_rfc = {
            'rfc__n_estimators': [50, 75, 100],
            'rfc__max_depth': [5, 10, 15],
            'rfc__min_samples_split': [5, 10]
        }
        
        # Set up scoring metrics
        scoring = {
            "accuracy": "accuracy",
            "f1": "f1",
            "roc_auc": "roc_auc"
        }
        
        # Configure grid search
        grid_search_rfc = GridSearchCV(
            estimator=pipe_rfc,
            param_grid=param_dist_rfc,
            cv=7,
            scoring=scoring,
            refit="f1",
            n_jobs=-1
        )
        
        return grid_search_rfc
    
    def make_xgb(self):
        """
        Create XGBoost Classifier pipeline with preprocessing and randomized search.
        Returns:
            RandomizedSearchCV object configured for XGBoost
        """
        # Initialize preprocessing components
        ohe = OneHotEncoder(drop='if_binary', sparse_output=False)
        oe = OrdinalEncoder().set_output(transform='pandas')
        ss = StandardScaler()
        
        # Set up preprocessing pipeline
        preprocessor = ColumnTransformer(
            transformers=[
                ("binary_encoder", ohe, config.col_binary),
                ("ordinal_encoder", oe, config.col_ordinal),
                ("standard_scaler", ss, config.col_numeric)
            ],
            remainder='passthrough'
        )
        
        # Calculate class weight
        scale_pos_weight = len(y[y == 0]) / len(y[y == 1])
        
        # Create XGBoost classifier
        xgb = XGBClassifier(
            n_jobs=-1,
            enable_categorical=True,
            scale_pos_weight=scale_pos_weight,
            random_state=config.random_state
        )
        
        # Create full pipeline
        pipe_xgb = Pipeline(
            steps=[
                ("preprocessor", preprocessor),
                ("xgb", xgb)
            ]
        )
        
        # Define comprehensive parameter search space
        param_dist_xgb = {
            "xgb__n_estimators": [100, 150, 200, 300],
            "xgb__max_depth": [3, 5, 7, 10],
            "xgb__learning_rate": [0.1, 0.01, 0.001, 0.0001],
            "xgb__subsample": [0.7, 0.8, 0.9],
            "xgb__colsample_bytree": [0.7, 0.8, 0.9],
            "xgb__gamma": [0, 0.1],
            "xgb__alpha": [0, 0.1],
            "xgb__lambda": [1, 2]
        }
        
        # Set up scoring metrics
        scoring = {
            "accuracy": "accuracy",
            "f1": "f1",
            "roc_auc": "roc_auc"
        }
        
        # Configure randomized search
        random_search_xgb = RandomizedSearchCV(
            estimator=pipe_xgb,
            param_distributions=param_dist_xgb,
            n_iter=30,
            cv=7,
            scoring=scoring,
            refit="f1",
            n_jobs=-1
        )
        
        return random_search_xgb

# Create global config instance
config = Config()
```

:::

# Dataset

Now we can import the dataset. Having all columns UPPERCASE, I will set all of them as lowercase

```{python, import_data}
df = pd.read_csv(config.file)
df.columns = df.columns.str.lower()
```

# EDA

## Target

It is always important to perform an EDA. In this case I am expecting to have less cases of high creditworthiness than low: for my personal knowledge and experience, high creditworthiness clients are not the majority. Also, considering it is a **binary classification**, it is important to be sure if data are balanced to make a better model selection.

```{python, value_count}
value_count = np.unique(df.target, return_counts=True)

var0 = value_count[0][0]
var0_count = value_count[1][0]

var1 = value_count[0][1]
var1_count = value_count[1][1]
```

The variable:

-   `{r} py$var0` has `{r} py$var0_count %>% format(scientific = FALSE, big.mark = ",")`.
-   `{r} py$var1` has `{r} py$var1_count %>% format(scientific = FALSE, big.mark = ",")`.

```{r, tbl-eda}
#| label: tbl-eda
#| tbl-cap: Absolute and relative frequencies

library(reticulate)

df <- py$df

df_g <- df %>% 
  rename(label = target) %>% 
  mutate(label = as.factor(label)) %>% 
  summarize(
    freq_abs = n(),
    freq_rel = n() / nrow(df),
    .by = label
  )
df_g %>% 
  gt() %>% 
  fmt_auto() %>% 
  cols_width(
    label ~ pct(20),
    freq_abs ~ pct(35),
    freq_rel ~ pct(35)
    ) %>% 
  cols_align(
    align = "center",
    columns = c(freq_abs, freq_rel)
  ) %>% 
  tab_header(
    title = "Label frequency",
    subtitle = "Absolute and relative frequencies"
  ) %>% 
  cols_label(
    label = "Label",
    freq_abs = "Absolute frequency",
    freq_rel = "Relative frequency"
  ) %>% 
  tab_options(
    table.width = pct(100)
    )
```

The dataset is **unbalanced** as expected: most of the target are low creditworthiness (0 label). This is important in the training: unbalanced datasets have higher risk of overfitting. If 9 out of 10 observatinos are 0 and you predict 10 times 0, the model has an accuracy of 0.9!

```{r, barchart}
#| label: fig-barchart
#| fig-cap: Absolute frequency barchart

g <- ggplot(df_g) +
  geom_bar(aes(x = label, y = freq_abs, fill = label), stat = "identity") +
  ggtitle("Absolute frequency barchart") +
  xlab("Label") +
  ylab("Absolute frequency") +
  labs(fill = "Label") +
  scale_y_continuous(breaks = scales::breaks_pretty(n = 5)) +
  theme_minimal()
ggplotly(g)
```

::: callout-importand
-   **0**. stands for **low creditworthiness**.
-   **1** stands for **high**.
-   Our dataset is strongly unbalanced towards the 0 label.
:::

## Features

Lets check the **NA** values in each feature.

```{r}
#| label: tbl-na
#| tbl-cap: Number of NA values for each feature

df_na <- df %>%
  summarize(across(everything(), ~ sum(is.na(.)))) %>%
  pivot_longer(
    cols = everything(),
    names_to = "columns",
    values_to = "value"
  )
df_na %>% 
  gt() %>% 
  tab_options(
    table.width = pct(100)
    ) %>% 
  cols_align(align = "center") %>% 
  cols_label(
    columns = "Columns",
    value = "NA Value"
  ) %>% 
  tab_header(
    title = "NA values",
    subtitle = "Number of NA value for each feature"
  ) %>% 
  fmt_number(
    columns = c("value"),
    decimals = 2,
    drop_trailing_zeros = TRUE,
    #drop_trailing_dec_mark = FALSE
  ) 
```

There are 2 interesting things to notice:

#### The column `occupation_type` has lots of NA value. How much in proportion to the total? Should we drop the NAs or keep them?

```{r}
#| label: tbl-occupation-type
#| tbl-cap: NA frequency for occupation_type

df %>%
  summarize(
    freq_abs = sum(is.na(occupation_type)),
    freq_rel = sum(is.na(occupation_type)) / nrow(df)
    ) %>% 
  gt() %>% 
  tab_options(
    table.width = pct(100)
    ) %>% 
  cols_align(align = "center") %>% 
  cols_label(
    freq_abs = "Absolute Frequency",
    freq_rel = "Relative Frequency"
  ) %>% 
  tab_header(
    title = "Absolute and Relative Frequency",
    subtitle = "NA frequency for occupation_type"
  ) %>% 
  fmt_number(
    columns = c("freq_abs", "freq_rel"),
    decimals = 2,
    drop_trailing_zeros = TRUE
  ) 
```

`{r} df %>%summarize(freq_abs = sum(is.na(occupation_type)), freq_rel = sum(is.na(occupation_type)) / nrow(df)) %>% pull(freq_rel) %>% scales::percent(accuracy = 0.01)` of occupation_type are NA. This could be a big concern: how to handle it? Drop this much could mean losing lots of useful information, while keeping it could lead to impute wrong data, which could be worse.

Lets check the occupation_type labels frequency.

```{r}
#| label: tbl-occupation-type-freq-tbl
#| tbl-cap: Occupation type label frequency

df %>% 
  summarise(
    n = n(),
    .by = occupation_type
  ) %>% 
  arrange(desc(n)) %>% 
  gt() %>% 
  tab_options(
    table.width = pct(100)
    ) %>% 
  cols_align(align = "center") %>% 
  cols_label(
    occupation_type = "Occupation Type",
    n = "Frequency"
  ) %>% 
  tab_header(
    title = "Occupation Type Label Frequency"
  ) %>% 
  fmt_number(
    columns = c("n"),
    decimals = 2,
    drop_trailing_zeros = TRUE
  ) 
```

So here is the situation:

1.  we could impute the NA respecting the proportion of the other labels in the dataset or
2.  we can drop the NA values.

I want to drop the NA. Even if we are losing a third of the data, we are not making assumption based on the mode. Occupation_type could be a very important feature and doing inference on it is worst then lose some data. Also, we still have `{r} df %>% filter(!is.na(occupation_type)) %>% nrow()`, which are plenty enough.

```{r}
#| label: fig-barplot-occ-type-freq
#| fig-cap: Barplot Occupation Type Label frequency

df_occ_type <- df %>% 
  filter(!is.na(occupation_type)) %>% 
  summarise(
    n = n(),
    .by = occupation_type
  ) %>% 
  arrange(desc(n))

# to extend the palette to the number of labels
extended_palette <- colorRampPalette(RColorBrewer::brewer.pal(9, "Purples"))(df_occ_type %>% nrow())

g <- ggplot(df_occ_type) +
  geom_col(aes(x = n, y = reorder(occupation_type, n), fill = reorder(occupation_type, n))) +
  theme_minimal() +
  labs(
    x = "Count",
    y = "Labels",
    title = "Occupation Type Label Frequency"
  ) +
  scale_fill_manual(values = extended_palette)

ggplotly(g)
```

#### There is a NA value in at least all the columns: is it possible it is the same observation?

First of all, we can create a list of the columns name which have only 1 NA, using the `df_na` tibble.

```{r}
one_cols <- df_na %>% 
  filter(value == 1) %>% 
  pull(columns)
one_cols
```

Then, to check multiple OR condition, we can use `if_any()` in R. In this case, we are checking if multiple columns are NA, with the previous filter on only one NA in each column.

```{r}
library(reticulate)

df %>% 
  filter(
    if_any(all_of(one_cols), ~ . %>% is.na())
  ) %>% 
  gt()  %>% 
  tab_options(
    table.width = pct(100)
    )
  
py$id_to_remove <- df %>% 
  filter(
    if_any(one_cols, ~ . %>% is.na())
  ) %>% 
  pull(id)
```

#### Interesting features

Lets also look at some interesting variables: click on the different tabs to see the frequency table for each of them.

::: callout-note
Future upgrade will convert this tabset to a Shiny app. I am encountering some difficulties with shinylive, meaning it is not showing the plot.
:::

```{r}
# features to check
cols <- py$config$features
# 
count_results <- map(cols, ~ df %>% filter(!is.na(occupation_type)) %>% count(.data[[.x]]))
names(count_results) <- cols
cols
```

::: panel-tabset
## Count children

```{r}
#| label: tbl-cnt_children
#| tbl-cap: Count of Number of Children

count_results$cnt_children %>% 
  arrange(desc(n)) %>% 
  gt() %>% 
  tab_options(
    table.width = pct(100)
    ) %>% 
  cols_align(align = "center") %>% 
  cols_label(
    cnt_children = "# Children",
    n = "Count"
  ) %>% 
  tab_header(
    title = "Count of Number of Children",
  ) %>% 
  fmt_number(
    columns = c("n"),
    decimals = 2,
    drop_trailing_zeros = TRUE,
    #drop_trailing_dec_mark = FALSE
  ) 
```

## Education type

```{r}
#| label: tbl-name_education_type
#| tbl-cap: Count by Education Type

count_results$name_education_type %>% 
  arrange(desc(n)) %>% 
  gt()  %>% 
  tab_options(
    table.width = pct(100)
    ) %>% 
  cols_align(align = "center") %>% 
  cols_label(
    name_education_type = "Education Type",
    n = "Count"
  ) %>% 
  tab_header(
    title = "Count by Education Type",
  ) %>% 
  fmt_number(
    columns = c("n"),
    decimals = 2,
    drop_trailing_zeros = TRUE,
    #drop_trailing_dec_mark = FALSE
  ) 
```

## Family status

```{r}
#| label: tbl-name_family_status
#| tbl-cap: Count by Family Status

count_results$name_family_status %>% 
  arrange(desc(n)) %>% 
  gt()  %>% 
  tab_options(
    table.width = pct(100)
    ) %>% 
  cols_align(align = "center") %>% 
  cols_label(
    name_family_status = "Family Status",
    n = "Count"
  ) %>% 
  tab_header(
    title = "Count by Family Status",
  ) %>% 
  fmt_number(
    columns = c("n"),
    decimals = 2,
    drop_trailing_zeros = TRUE,
    #drop_trailing_dec_mark = FALSE
  ) 
```

## Housing type

```{r}
#| label: tbl-name_housing_type
#| tbl-cap: Count by House Type

count_results$name_housing_type %>% 
  arrange(desc(n)) %>% 
  gt()  %>% 
  tab_options(
    table.width = pct(100)
    ) %>% 
  cols_align(align = "center") %>% 
  cols_label(
    name_housing_type = "House Type",
    n = "Count"
  ) %>% 
  tab_header(
    title = "Count by House Type",
  ) %>% 
  fmt_number(
    columns = c("n"),
    decimals = 2,
    drop_trailing_zeros = TRUE,
    #drop_trailing_dec_mark = FALSE
  ) 
```

## Count family members

```{r}
#| label: tbl-cnt_fam_members
#| tbl-cap: Count by Number of Family Members

count_results$cnt_fam_members %>% 
  arrange(desc(n)) %>% 
  gt()  %>% 
  tab_options(
    table.width = pct(100)
    ) %>% 
  cols_align(align = "center") %>% 
  cols_label(
    cnt_fam_members = "Number of Family Members",
    n = "Count"
  ) %>% 
  tab_header(
    title = "Count by Number of Family Members",
  ) %>% 
  fmt_number(
    columns = c("n"),
    decimals = 2,
    drop_trailing_zeros = TRUE,
    #drop_trailing_dec_mark = FALSE
  ) 
```

## Gender

```{r}
#| label: tbl-code_gender
#| tbl-cap: Count by Gender

count_results$code_gender %>% 
  arrange(desc(n)) %>% 
  gt() %>% 
  tab_options(
    table.width = pct(100)
    ) %>% 
  cols_align(align = "center") %>% 
  cols_label(
    code_gender = "Gender",
    n = "Count"
  ) %>% 
  tab_header(
    title = "Count by Gender",
  ) %>% 
  fmt_number(
    columns = c("n"),
    decimals = 2,
    drop_trailing_zeros = TRUE,
    #drop_trailing_dec_mark = FALSE
  ) 
```
:::

# Data preparation

From the EDA, we know we can drop

-   NA occupation_type.
-   the id `{r} py$id_to_remove`.

```{python, remove_nas}
df = df[df.id != id_to_remove]
df = df[~df.occupation_type.isna()]
```

Even if it is not useful for the model because these are not features, I want to convert the date columns to birthday and date of employment.

```{python}
df["birthday"] = df.days_birth.map(lambda x : (date.today() + relativedelta(days=x)).strftime("%Y-%m-%d"))
df["employed"] = df["days_employed"].apply(lambda x: (date.today() + relativedelta(days=x)).strftime("%Y-%m-%d") if x < 0 else "0")
```

# Split

To take into consideration the unbalance and have it also in our split, we can use the `stratify` method.

```{python, train_test_split}
x = df[config.features]
y = df.target

xtrain, xtest, ytrain, ytest = train_test_split(
    x,
    y,
    test_size = .2,
    stratify = y,
    random_state=config.random_state
)
```

# Models

To address the model **explainability**, we can choose a CART model. They have 2 interesting point for our task:

1.  **Explainability**, as already mentioned. This will help us explain the reason of rejection.
2.  **Feature selection.** At this [link](https://www.researchgate.net/publication/369355042_A_Comprehensive_Review_of_Feature_Selection_and_Feature_Selection_Stability_in_Machine_Learning) you can find a paper which shows how **embedded method** as the tree models can have the same performance of traditional feature selection method.

::: {callout-important}
It could be also interesting to do a **R**ecursive **F**eature **E**limination. Right now the feature selection is made with personal knowledge to remove some columns before training. The CART model will perform the feature selection, as the paper above explain.
:::

We are going to implement 3 models with an increasingly level of complexity:

1.  DecisionTreeClassifier.
2.  RandomForestClassifier.
3.  XGBoostClassifier.

## DecisionTreeClassifier

### Pipeline

Even if the result of the model are relatively easy to understand, the **hyperparameter tuning** has far too many possibility. This is where `GridSearchCV()` comes handy. Check @lst-config for more details.

```{python, make_dtc}
#| eval: false
dtc = config.make_dtc()
```

One of the many nice `sklearn` features is the [`Pipeline()`](https://scikit-learn.org/1.5/modules/generated/sklearn.pipeline.Pipeline.html). It has lots of important benefits:

-   **Code redundancy and reproducibility** : it is like a recipe you can tune and run without worrying about to put togheter each step in the correct order.
-   **Avoid data leakege**: the preprocessing steps applies only to the training dataset.
-   **Hyperparameter tuning**: the `Pipeline()` integrates with `GridSearchCV()` and `RandomizedSearchCV()`.
-   **Modularity**: it enables tuning the various steps, removing or adding new one.

First, initialize the various preprocessor and the classification model.

Using the `ColumnTransformer()`, we can apply the different preprocessing to the specific columns. The preprocessing is divided between:

-   Encoding the **binary labels** to 0-1 using `OneHotEncoding(drop="if_binary)"`.
-   Encoding the remaining labels using `OneHotEncoding()`. I choose this over other categorical variables encoding because it avoid imputing a hierarchy.
-   **Standardize** the numerical variables using `StandardScaler()`. I also have instantiated the `MinMaxScaler()`, but I am not using it.

The classifier is a `DecisionTreeClassifier()`, which the `GridSearchCV()` will tune.

All of these steps are putted togheter with the `Pipeline()`.

::: {callout-important}
For the binary labels I had to use the `OneHotEncoding(drop="if_binary")`, otherwise other preprocessor would not work with the `ColumnTransformer()`.
:::

### Fit

```{python, dtc_fit}
#| eval: false
dtc_model = dtc.fit(xtrain, ytrain)
```

Check @lst-config for more details.

### Pickle

```{python, dtc_pkl_dump}
#| eval: false
config.dump_pkl(
  path=os.path.expanduser(config.pkl_path) + "dtc_model.pkl",
  model=dtc_model
  )
```

```{python, dtc_pkl_load}
dtc = config.load_pkl(
  path=os.path.expanduser(config.pkl_path) + "dtc_model.pkl"
  )
dtc_model = dtc.best_estimator_
```

### Classification Report

```{python, dtc_cr}
ypred_dtc = dtc_model.predict(xtest)
cr = classification_report(
  ytest,
  ypred_dtc,
  target_names=['0', '1'],
  digits=4,
  output_dict=True
  )
df_cr_dtc= pd.DataFrame.from_dict(cr).reset_index()
```

```{r, dtc_cr}
#| label: tbl-classification-report-dtc
#| tbl-cap: Classification report DTC

library(reticulate)
df_cr <- py$df_cr_dtc %>% dplyr::rename(names = index)
cols <- df_cr %>% colnames()
df_cr %>% 
  pivot_longer(
    cols = -names,
    names_to = "metrics",
    values_to = "values"
  ) %>% 
  pivot_wider(
    names_from = names,
    values_from = values
  ) %>% 
  gt() %>%
  tab_header(
    title = "Confusion Matrix",
    subtitle = "Threshold optimization favoring recall"
  ) %>% 
  fmt_number(
    columns = c("precision", "recall", "f1-score", "support"),
    decimals = 2,
    drop_trailing_zeros = TRUE,
    drop_trailing_dec_mark = FALSE
  ) %>% 
  cols_align(
    align = "center",
    columns = c("precision", "recall", "f1-score", "support")
  ) %>% 
  cols_align(
    align = "left",
    columns = metrics
  ) %>% 
  cols_label(
    metrics = "Metrics",
    precision = "Precision",
    recall = "Recall",
    `f1-score` = "F1-Score",
    support = "Support"
  ) %>% 
  tab_options(
    table.width = pct(100)
    )
```

## RandomForestClassifier

### Pipeline

```{python, make_rfc}
#| eval: false
rfc = config.make_rfc()
```

Check @lst-config for more details.

### Fit

```{python, rfc_fit}
#| eval: false
tic = time.time()
rfc_model = rfc.fit(xtrain, ytrain)
toc = time.time()
elapsed_time= toc - tic
print(f"Elapsed time: {elapsed_time} seconds")
print(f"Elapsed time: {elapsed_time / 60} minutes")
```

### Pickle

```{python, rfc_pkl_dump}
#| eval: false
config.dump_pkl(
  path=os.path.expanduser(config.pkl_path) + "rfc_model.pkl",
  model=rfc_model
  )
```

```{python, rfc_pkl_load}
rfc = config.load_pkl(
  path=os.path.expanduser(config.pkl_path) + "rfc_model.pkl"
  )
rfc_model = rfc.best_estimator_
```

### Classification

```{python, rfc_cr}
ypred_rfc = rfc_model.predict(xtest)
cr_rfc = classification_report(
  ytest,
  ypred_rfc,
  # target_names=config.labels,
  digits=4,
  output_dict=True
  )
df_cr_rfc = pd.DataFrame.from_dict(cr_rfc).reset_index()
```

```{r, rfc_cr}
#| label: tbl-classification-report-rfc
#| tbl-cap: Classification report RFC

library(reticulate)
df_cr <- py$df_cr_rfc %>% dplyr::rename(names = index)
cols <- df_cr %>% colnames()
df_cr %>% 
  pivot_longer(
    cols = -names,
    names_to = "metrics",
    values_to = "values"
  ) %>% 
  pivot_wider(
    names_from = names,
    values_from = values
  ) %>% 
  gt() %>%
  tab_header(
    title = "Confusion Matrix",
    subtitle = "Threshold optimization favoring recall"
  ) %>% 
  fmt_number(
    columns = c("precision", "recall", "f1-score", "support"),
    decimals = 2,
    drop_trailing_zeros = TRUE,
    drop_trailing_dec_mark = FALSE
  ) %>% 
  cols_align(
    align = "center",
    columns = c("precision", "recall", "f1-score", "support")
  ) %>% 
  cols_align(
    align = "left",
    columns = metrics
  ) %>% 
  cols_label(
    metrics = "Metrics",
    precision = "Precision",
    recall = "Recall",
    `f1-score` = "F1-Score",
    support = "Support"
  ) %>% 
  tab_options(
    table.width = pct(100)
    )
```

## XGBoostClassifier

### Pipeline

```{python, make_xgb}
#| eval: false
xgb = config.make_xgb()
```

Check @lst-config for more details.

### Fit

```{python, xgb_fit}
#| eval: false
tic = time.time()
xgb_model = xgb.fit(xtrain, ytrain)
toc = time.time()
elapsed_time= toc - tic
print(f"Elapsed time: {elapsed_time} seconds")
print(f"Elapsed time: {elapsed_time / 60} minutes")
```

### Pickle

```{python, xgb_pkl_dump}
#| eval: false
config.dump_pkl(
  path=os.path.expanduser(config.pkl_path) + "xgb_model.pkl",
  model=xgb_model
  )
```

```{python, xgb_pkl_load}
xgb = config.load_pkl(
  path=os.path.expanduser(config.pkl_path) + "xgb_model.pkl"
  )
xgb_model = xgb.best_estimator_
```

### Classification

```{python}
ypred_xgb = xgb_model.predict(xtest)
cr_xgb = classification_report(
  ytest,
  ypred_xgb,
  # target_names=config.labels,
  digits=4,
  output_dict=True
  )
df_cr_xgb = pd.DataFrame.from_dict(cr_xgb).reset_index()
```

```{r, xgb_cr}
#| label: tbl-classification-report-xgb
#| tbl-cap: Classification report XGB

library(reticulate)
df_cr <- py$df_cr_xgb %>% dplyr::rename(names = index)
cols <- df_cr %>% colnames()
df_cr %>% 
  pivot_longer(
    cols = -names,
    names_to = "metrics",
    values_to = "values"
  ) %>% 
  pivot_wider(
    names_from = names,
    values_from = values
  ) %>% 
  gt() %>%
  tab_header(
    title = "Confusion Matrix",
    subtitle = "Threshold optimization favoring recall"
  ) %>% 
  fmt_number(
    columns = c("precision", "recall", "f1-score", "support"),
    decimals = 2,
    drop_trailing_zeros = TRUE,
    drop_trailing_dec_mark = FALSE
  ) %>% 
  cols_align(
    align = "center",
    columns = c("precision", "recall", "f1-score", "support")
  ) %>% 
  cols_align(
    align = "left",
    columns = metrics
  ) %>% 
  cols_label(
    metrics = "Metrics",
    precision = "Precision",
    recall = "Recall",
    `f1-score` = "F1-Score",
    support = "Support"
  ) %>% 
  tab_options(
    table.width = pct(100)
    )
```

# Models comparison

Lets check with a plot the performance of all the models. The AUC ROC makes it possible. It is a plot useful for classification tasks. It plots False vs True Positive Rate.

```{python, roc_data}
from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt

plt.figure(figsize=(20, 15))

model_names = ["dtc", "rfc", "xgb"]
models_ = [dtc_model, rfc_model, xgb_model]
models = zip(model_names, models_)

for name, model in models:
    fpr, tpr, _ = roc_curve(ytest, model.predict_proba(xtest)[:, 1])
    auc_score = roc_auc_score(ytest, model.predict_proba(xtest)[:, 1])
    plt.plot(fpr, tpr, label=f'{name} (AUC: {auc_score:.2f})')

plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curves Comparison")
plt.tight_layout()
plt.legend(loc="lower right", fontsize=12)
plt.show()
plt.close()
```

From the ROC AUC, the best model is the xgboost, because the **A**rea **U**nder the **C**urve is higher.

# Explainability

What if the model predict a low creditworthiness and the business wants to know why? **Shapley values** make this possible. It comes from cooperative game theory: for more details, check this [link](https://medium.com/the-modern-scientist/what-is-the-shapley-value-8ca624274d5a) and this [link](https://www.sciencedirect.com/science/article/abs/pii/S1574000502030163).

For our purpose, I have found 3 observations we can use. It is also useful to separate the preprocessor and classificator part of the pipeline.

```{python}
model = xgb_model
preprocessor = model.named_steps["preprocessor"]
clf = model.named_steps["xgb"]
idx = [15454, 1284, 30305]
```

```{python, shapley_preparation}
# retain features name
xtrain_processed = preprocessor.transform(xtrain)
xtrain_processed = pd.DataFrame(xtrain_processed, columns=config.features)
# retain features name
xtest_processed = preprocessor.transform(xtest)
xtest_processed = pd.DataFrame(xtest_processed, columns=config.features)
# retain features name
clf.get_booster().feature_names = config.features
# convert to DMatrix
dtrain = DMatrix(xtrain_processed, label=ytrain)
dtest = DMatrix(xtest_processed, label=ytest)
# usefol idxs
idx = [15454, 1284, 30305]

x_processed = preprocessor.transform(x)
```

```{python}
from xgboost import plot_importance

# importance plot
plot_importance(
    booster=clf.get_booster(),
    grid=False,
    importance_type="gain",
    title="Feature Importance by Gain",
    values_format="{v:.2f}"
)
plt.tight_layout()
plt.show()
```

The importance plot tells us the information gain each feature gives to the model prediction. The 3 most important features are also the most logical one:

-   Total income amount.
-   Number of childrens.
-   Type of housing.

Importance provides a **score** that indicates how useful useful each feature is in the construcition of the boosted decision trees. The more an attribute is used to make key decisions with decision trees, the higher its relative importance.

```{python}
#| eval: false
# force plot
plt.figure(figsize=(10, 8))
plot_force = shap.plots.force(
    base_value=explainer.expected_value,
    shap_values=shap_values[idx[0], :],
    # features=None,
    feature_names=config.features,
    # out_names=None,
    # link='identity', # "logit"
    plot_cmap='RdBu',
    matplotlib=True,
    show=True
)
plt.savefig("plot_force.jpeg", dpi=200)

# calculate specific shap explainer for waterfall plot
explainer_ = shap.Explainer(model=xgb_clf)
shap_values_ = explainer(x_processed)
shap_values_.feature_names = config.features
# waterfall plot
shap.plots.waterfall(
    shap_values_[0],
    max_display=len(config.features)
)
plt.savefig("plot_waterfall.jpeg", dpi=200)
# bar plot
shap.plots.bar(
    shap_values_[0],
    max_display=len(config.features)
)
plt.savefig("plot_bar.jpeg", dpi=200)
```

![Force plot](kaggle/force_plot.png){fig-align="center"}

The force plot explains the contribution of features to a specific model prediction.

-   **Base value** is the model expected output or **mean prediction** over the dataset. It is the prediction the model would make without any specific input features. It is the starting point of the plot.

-   **Feature Contributions**. The SHAP values represent how much each feature contributes to the difference between the vase value and the final prediction for a specific observation.

-   **Positive contributions**. Features pushing the prediction higher. Shown in red.

-   **Negative contributions**. Features pulling the prediction lower. Shown in blue. The magnitude of each bar indicates the size of the feature's contribution.

-   **Final Prediction**. Endpoint of the plot. It is the sum of the base value and all the SHAP values for that observation.

![Waterfall plot](kaggle/waterfall_plot.png){fig-align="center"}

Waterfall plots displays explanations for individual predictions. The bottom of a waterfall plot starts as the expected value of the model output, and then each row shows how the positive (red) or negative (blue) contribution of each feature moves the value from the expected model output over the background dataset to the model output for this prediction.

::: {.callout-important}
Due to venv incompatibilities between shap and other packages, the shap evaluation has been made on Kaggle. The plots are the results of the notebook.
:::
